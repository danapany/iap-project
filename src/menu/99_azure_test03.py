"""
Streamlit ê¸°ë°˜ ì‚¬ë‚´ ìš©ì–´ ìš°ì„  í•™ìŠµ RAG ì‹œìŠ¤í…œ
Enterprise Terminology-First RAG System with Streamlit UI

ì‹¤í–‰ ë°©ë²•:
pip install streamlit azure-search-documents openai pandas plotly
streamlit run streamlit_rag_app.py
"""

import streamlit as st
import os
import re
import json
import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import pandas as pd
from collections import Counter
import time

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œë¥¼ ìœ„í•œ python-dotenv import
try:
    from dotenv import load_dotenv
    load_dotenv()  # .env íŒŒì¼ ë¡œë“œ
    DOTENV_AVAILABLE = True
except ImportError:
    DOTENV_AVAILABLE = False
    st.warning("âš ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ ì§ì ‘ ì„¤ì •í•´ì£¼ì„¸ìš”.")

# Plotly ì„ íƒì  import
try:
    import plotly.express as px
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    st.warning("âš ï¸ Plotlyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì°¨íŠ¸ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

# Azure SDK imports
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import *
from azure.core.credentials import AzureKeyCredential

# OpenAI import
import openai

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¢ Enterprise RAG System",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        color: #2E86AB;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #A23B72;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #F18F01;
        padding: 1rem;
        border-radius: 0.5rem;
        color: white;
        margin: 1rem 0;
    }
    .success-box {
        background-color: #C73E1D;
        padding: 1rem;
        border-radius: 0.5rem;
        color: white;
        margin: 1rem 0;
    }
    .term-card {
        border: 2px solid #2E86AB;
        border-radius: 10px;
        padding: 15px;
        margin: 10px 0;
        background-color: #f8f9fa;
    }
    .doc-card {
        border: 1px solid #A23B72;
        border-radius: 8px;
        padding: 12px;
        margin: 8px 0;
        background-color: #fff;
    }
</style>
""", unsafe_allow_html=True)

@dataclass
class TermDefinition:
    """ìš©ì–´ ì •ì˜ ë°ì´í„° í´ë˜ìŠ¤"""
    term: str
    definition: str
    context: str
    synonyms: List[str]
    related_terms: List[str]
    department: str
    examples: List[str]
    confidence_score: float = 0.0

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    content: str
    source_index: str
    score: float
    metadata: Dict

class ConfigManager:
    """ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤ - .env íŒŒì¼ ë° í™˜ê²½ë³€ìˆ˜ í™œìš©"""
    
    def __init__(self):
        # .env íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ (ì´ë¯¸ ìœ„ì—ì„œ load_dotenv() ì‹¤í–‰ë¨)
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
        self.azure_search_endpoint = os.getenv('AZURE_SEARCH_ENDPOINT', 'https://your-search-service.search.windows.net')
        self.azure_search_key = os.getenv('AZURE_SEARCH_KEY', 'your-search-admin-key')
        self.openai_api_key = os.getenv('OPENAI_API_KEY', 'your-openai-api-key')
        
        # ì„ íƒì  ì„¤ì •ë“¤ (ê¸°ë³¸ê°’ í¬í•¨)
        self.azure_search_api_version = os.getenv('AZURE_SEARCH_API_VERSION', '2023-11-01')
        self.openai_model = os.getenv('OPENAI_MODEL', 'gpt-4')
        self.openai_temperature = float(os.getenv('OPENAI_TEMPERATURE', '0.1'))
        self.max_tokens = int(os.getenv('MAX_TOKENS', '2000'))
        
        # ì¸ë±ìŠ¤ ì´ë¦„ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥)
        self.terminology_index = os.getenv('TERMINOLOGY_INDEX', 'company-terminology')
        self.document_indexes = {
            "policies": os.getenv('POLICIES_INDEX', 'company-policies'),
            "procedures": os.getenv('PROCEDURES_INDEX', 'work-procedures'), 
            "manuals": os.getenv('MANUALS_INDEX', 'system-manuals'),
            "announcements": os.getenv('ANNOUNCEMENTS_INDEX', 'company-announcements')
        }
        
        # ê²€ìƒ‰ ì„¤ì •
        self.max_terms_per_query = int(os.getenv('MAX_TERMS_PER_QUERY', '10'))
        self.max_documents_per_index = int(os.getenv('MAX_DOCUMENTS_PER_INDEX', '3'))
        self.min_confidence_score = float(os.getenv('MIN_CONFIDENCE_SCORE', '0.3'))
        
        # ë””ë²„ê¹…/ë¡œê¹… ì„¤ì •
        self.debug_mode = os.getenv('DEBUG_MODE', 'false').lower() == 'true'
        self.log_level = os.getenv('LOG_LEVEL', 'INFO')
    
    def validate_config(self):
        """ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        issues = []
        
        if self.azure_search_endpoint == 'https://your-search-service.search.windows.net':
            issues.append("AZURE_SEARCH_ENDPOINTê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if self.azure_search_key == 'your-search-admin-key':
            issues.append("AZURE_SEARCH_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if self.openai_api_key == 'your-openai-api-key':
            issues.append("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return issues
    
    def get_config_summary(self):
        """ì„¤ì • ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        return {
            "Azure Search Endpoint": self.azure_search_endpoint,
            "Azure Search Key": "***" + self.azure_search_key[-4:] if self.azure_search_key != 'your-search-admin-key' else "ë¯¸ì„¤ì •",
            "OpenAI API Key": "***" + self.openai_api_key[-4:] if self.openai_api_key != 'your-openai-api-key' else "ë¯¸ì„¤ì •",
            "OpenAI Model": self.openai_model,
            "Temperature": self.openai_temperature,
            "Max Tokens": self.max_tokens,
            "Debug Mode": self.debug_mode,
            "ìš©ì–´ ì¸ë±ìŠ¤": self.terminology_index,
            "ë¬¸ì„œ ì¸ë±ìŠ¤ ìˆ˜": len(self.document_indexes)
        }

class TerminologyExtractor:
    """ìš©ì–´ ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.patterns = {
            'acronyms': r'\b[A-Z]{2,}\b',
            'system_names': r'\w+ì‹œìŠ¤í…œ|\w+System|\w+ì„œë¹„ìŠ¤',
            'process_terms': r'\w+ê´€ë¦¬|\w+ì²˜ë¦¬|\w+ë¶„ì„|\w+ìš´ì˜',
            'department_terms': r'\w+íŒ€|\w+ë¶€|\w+ì‹¤|\w+ì„¼í„°',
            'technical_terms': r'\w+í”Œë«í¼|\w+ì†”ë£¨ì…˜|\w+ì—”ì§„',
            'business_terms': r'\w+ì „ëµ|\w+ì •ì±…|\w+ë°©ì¹¨|\w+ê·œì •'
        }
    
    def extract_terms(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì ì¬ì  ìš©ì–´ ì¶”ì¶œ"""
        extracted_terms = set()
        text_upper = text.upper()
        
        for pattern_name, pattern in self.patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            extracted_terms.update(matches)
            
            if pattern_name == 'acronyms':
                upper_matches = re.findall(pattern, text_upper)
                extracted_terms.update(upper_matches)
        
        filtered_terms = [term for term in extracted_terms if 2 <= len(term) <= 20]
        return list(filtered_terms)

def initialize_search_clients():
    """Azure Search í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    try:
        # ConfigManagerë¥¼ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ìƒì„±
        config = ConfigManager()
        credential = AzureKeyCredential(config.azure_search_key)
        
        terminology_client = SearchClient(
            config.azure_search_endpoint,
            config.terminology_index,
            credential
        )
        
        document_clients = {}
        for name, index_name in config.document_indexes.items():
            document_clients[name] = SearchClient(
                config.azure_search_endpoint,
                index_name,
                credential
            )
        
        return terminology_client, document_clients
    except Exception as e:
        st.error(f"Azure Search í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None, None

class StreamlitRAGSystem:
    """Streamlit ê¸°ë°˜ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.config = ConfigManager()
        self.term_extractor = TerminologyExtractor()
        
        # OpenAI ì„¤ì •
        if self.config.openai_api_key != 'your-openai-api-key':
            openai.api_key = self.config.openai_api_key
        
        # Azure Search í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (íŒŒë¼ë¯¸í„° ì—†ì´ í˜¸ì¶œ)
        self.terminology_client, self.document_clients = initialize_search_clients()
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if 'chat_history' not in st.session_state:
            st.session_state.chat_history = []
        if 'query_stats' not in st.session_state:
            st.session_state.query_stats = []
    
    def search_terminology(self, terms: List[str]) -> Dict[str, TermDefinition]:
        """ìš©ì–´ ì‚¬ì „ ê²€ìƒ‰"""
        if not self.terminology_client:
            return {}
        
        found_terms = {}
        
        with st.spinner("ğŸ” ìš©ì–´ ì‚¬ì „ ê²€ìƒ‰ ì¤‘..."):
            for term in terms:
                try:
                    # ëª¨ì˜ ë°ì´í„° (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Azure Search ê²°ê³¼ ì‚¬ìš©)
                    sample_terms = {
                        "ì¸ì‹œë˜íŠ¸": TermDefinition(
                            term="ì¸ì‹œë˜íŠ¸",
                            definition="ì‚¬ê³  ë˜ëŠ” ì‚¬ê±´ì„ ì˜ë¯¸í•˜ë©°, IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤.",
                            context="IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ ì¸ì‹œë˜íŠ¸ëŠ” ì„œë¹„ìŠ¤ ì¤‘ë‹¨ì´ë‚˜ ì„±ëŠ¥ ì €í•˜ì™€ ê°™ì€ ì‚¬ê±´ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
                            synonyms=["ì‚¬ê±´", "ì¥ì• ", "Incident"],
                            related_terms=["ì´ìƒì§•í›„", "VOC", "ë¬¸ì œ"],
                            examples=["DBì¤‘ë‹¨ë˜ì–´ ì¸ì‹œë˜íŠ¸ ì¡°ì¹˜ í•˜ì˜€ìŠµë‹ˆë‹¤."],
                            confidence_score=0.95
                        ),
                        "ì´ë²¤íŠ¸": TermDefinition(
                            term="ì´ë²¤íŠ¸",
                            definition="ê´€ì œ ì•ŒëŒ ì‹œì ì— ë°œìƒí•˜ëŠ” ì‚¬ê±´ì´ë‚˜ ìƒí™©ì„ ì˜ë¯¸í•˜ë©° ì„œë¹„ìŠ¤ ë¹„ì •ìƒë™ì‘ì˜ ê²½ìš°ë‚˜ íŠ¹ì •ì¡°ê±´ ê²½ê³ ì‹œì— ë°œìƒì‹œí‚µë‹ˆë‹¤.",
                            context="IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ ì´ë²¤íŠ¸ëŠ” ì‹œìŠ¤í…œì˜ ì •ìƒì ì¸ ë™ì‘ì„ ëª¨ë‹ˆí„°ë§í•˜ê³ , ì´ìƒ ì§•í›„ë¥¼ ì¡°ê¸°ì— ë°œê²¬í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.ê´€ì œ íˆ´ë¡œëŠ” LAMP, B-Mon, Genio, looksê°€ ìˆìŠµë‹ˆë‹¤",
                            synonyms=["ê´€ì œì´ë²¤íŠ¸", "ê´€ì œì•ŒëŒ", "Event"],
                            related_terms=["ì¸ì‹œë˜íŠ¸", "ì´ìƒì§•í›„", "ë¡œê·¸"],
                            examples=["DBì¤‘ë‹¨ë˜ì–´ ì¸ì‹œë˜íŠ¸ ì¡°ì¹˜ í•˜ì˜€ìŠµë‹ˆë‹¤."],
                            confidence_score=0.95
                        ),                        
                        "ì´ìƒì§•í›„": TermDefinition(
                            term="ì´ìƒì§•í›„",
                            definition="ì¥ì• ë¡œ íŒë‹¨í•˜ê¸° ì´ì „ì— ì„œë¹„ìŠ¤ì˜ ë¹„ì •ìƒ ìƒí™©ì´ ê°ì§€ëœê²ƒìœ¼ë¡œ ì„œë¹„ìŠ¤ ì¥ì• ë¡œ í™•ëŒ€ê°€ ë  ìˆ˜ë„ ìˆëŠ” ì„œë¹„ìŠ¤ì˜ ìƒíƒœì´ë‹¤",
                            context="IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ ì´ìƒì§•í›„ëŠ” ì¥ì• ìƒí™©ì„ ì •í™•íˆ íŒë‹¨í•˜ê¸° ì´ì „ë‹¨ê³„ë¡œ ì¥ì•  ë°œìƒ ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
                            synonyms=["ì§•í›„", "ì‹ í˜¸", "Indicator"],
                            related_terms=["ì¸ì‹œë˜íŠ¸", "ì¥ì• ", "ì´ë²¤íŠ¸"],
                            examples=["MESë¥¼ í†µí•´ ìƒì‚° ì§„í–‰ë¥ ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤"],
                            confidence_score=0.92
                        ),
                        "ì„œë¹„ìŠ¤": TermDefinition(
                            term="ì„œë¹„ìŠ¤",
                            definition="IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ íŠ¹ì • ëª©ì ìœ¼ë¡œ ì œê³µë˜ëŠ” ë…ë¦½ì ì¸ ì„œë¹„ìŠ¤ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.",
                            context="IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ ì„œë¹„ìŠ¤ëŠ” ê³ ê°ì˜ ìš”êµ¬ë¥¼ ì¶©ì¡±ì‹œí‚¤ê¸° ìœ„í•´ ì œê³µë˜ëŠ” ë…ë¦½ì ì¸ ì„œë¹„ìŠ¤ì´ë©° ì—…ë¬´ë¥¼ ìœ„í•œ ì—¬ëŸ¬ ê¸°ëŠ¥ì´ë‚˜ ì†”ë£¨ì…˜ì„ í¬í•¨í•©ë‹ˆë‹¤.",
                            synonyms=["IT ì„œë¹„ìŠ¤", "ì„œë¹„ìŠ¤ ì œê³µ"],
                            related_terms=["ë‹¨ìœ„ì„œë¹„ìŠ¤", "í‘œì¤€ì„œë¹„ìŠ¤", "ë„ë©”ì¸"],
                            examples=["WMSì—ì„œ ì¬ê³  ìœ„ì¹˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"],
                            confidence_score=0.88
                        ),
                        "ë‹¨ìœ„ì„œë¹„ìŠ¤": TermDefinition(
                            term="ë‹¨ìœ„ì„œë¹„ìŠ¤",
                            definition="IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ íŠ¹ì • ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” ë…ë¦½ì ì¸ ì„œë¹„ìŠ¤ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.",
                            context="IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ ë‹¨ìœ„ì„œë¹„ìŠ¤ëŠ” ê³ ê°ì˜ ìš”êµ¬ë¥¼ ì¶©ì¡±ì‹œí‚¤ê¸° ìœ„í•´ ì œê³µë˜ëŠ” ë…ë¦½ì ì¸ ì„œë¹„ìŠ¤ì´ë©° ì—…ë¬´ë¥¼ ìœ„í•œ ì—¬ëŸ¬ ê¸°ëŠ¥ì´ë‚˜ ì†”ë£¨ì…˜ì„ í¬í•¨í•©ë‹ˆë‹¤.",
                            synonyms=["ê¸°ëŠ¥ ì„œë¹„ìŠ¤", "ëª¨ë“ˆ ì„œë¹„ìŠ¤"],
                            related_terms=["ì„œë¹„ìŠ¤", "í‘œì¤€ì„œë¹„ìŠ¤", "ë„ë©”ì¸"],
                            examples=["WMSì—ì„œ ì¬ê³  ìœ„ì¹˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"],
                            confidence_score=0.88
                        ),
                        "í‘œì¤€ì„œë¹„ìŠ¤": TermDefinition(
                            term="í‘œì¤€ì„œë¹„ìŠ¤",
                            definition="ìœ ì‚¬í•œ ê¸°ëŠ¥ì˜ ë‹¨ìœ„ì„œë¹„ìŠ¤ì˜ í†µí•©ê°œë…ìœ¼ë¡œ í‘œì¤€ì„œë¹„ìŠ¤ë¼ê³  ëª…ì¹­í•©ë‹ˆë‹¤.",
                            context="IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ í‘œì¤€ì„œë¹„ìŠ¤ëŠ” ë‹¨ìœ„ ì„œë¹„ìŠ¤ì˜ ì§‘í•©ì²´ì´ë©° ê´€ë¦¬ë¥¼ ìœ„í•˜ì—¬ ëŒ€í‘œì ì¸ ë‹¨ìœ„ì„œë¹„ìŠ¤ëª…ì„ í‘œì¤€ì„œë¹„ìŠ¤ëª…ìœ¼ë¡œ ëª…ëª…í•˜ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤.",
                            synonyms=["IT ì„œë¹„ìŠ¤", "ì„œë¹„ìŠ¤ ì œê³µ"],
                            related_terms=["ë‹¨ìœ„ì„œë¹„ìŠ¤", "í‘œì¤€ì„œë¹„ìŠ¤", "ë„ë©”ì¸"],
                            examples=["WMSì—ì„œ ì¬ê³  ìœ„ì¹˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"],
                            confidence_score=0.88
                        ),
                        "ë„ë©”ì¸": TermDefinition(
                            term="ë„ë©”ì¸",
                            definition="IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ í‘œì¤€ì„œë¹„ìŠ¤ë“¤ì˜ ê´€ë¦¬ì£¼ì²´ì™€ ì—…ë¬´ì˜ì—­ì„ ê³ ë ¤í•œ ëŒ€ë¶„ë¥˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.",
                            context="IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ ë„ë©”ì¸ì€ í‘œì¤€ì„œë¹„ìŠ¤ì˜ ì§‘í•©ì²´ë¡œ ê´€ë¦¬ì£¼ì²´ì™€ ì—…ë¬´ì˜ì—­ì„ ê³ ë ¤í•œ ëŒ€ë¶„ë¥˜ ìœ í˜•ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤.",
                            synonyms=["ì˜ì—­", "ì£¼ì œ"],
                            related_terms=["ì„œë¹„ìŠ¤", "ë‹¨ìœ„ì„œë¹„ìŠ¤", "í‘œì¤€ì„œë¹„ìŠ¤"],
                            examples=["WMSì—ì„œ ì¬ê³  ìœ„ì¹˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"],
                            confidence_score=0.88
                        ),
                        "KOS": TermDefinition(
                            term="KOS",
                            definition="KTì˜ ëŒ€í‘œ ì˜ì—…ê³„ ì‹œìŠ¤í…œì„ ì˜ë¯¸í•˜ëŠ” ë‹¨ì–´ì…ë‹ˆë‹¤.",
                            context="IT ì„œë¹„ìŠ¤ ê´€ë¦¬ì—ì„œ KOSëŠ” KTì˜ ì˜ì—… ê´€ë ¨ ì—…ë¬´ë¥¼ ì§€ì›í•˜ëŠ” ì‹œìŠ¤í…œìœ¼ë¡œ ì‹œìŠ¤í…œì´ ë°©ëŒ€í•˜ì—¬ í•˜ìœ„ë¡œ KOS-ì˜¤ë”, KOS-billing ë“± KOS-ë¡œ ì‹œì‘í•˜ëŠ” í‘œì¤€ì„œë¹„ìŠ¤ë“¤ì´ ë§ì´ í¬í•¨ë˜ì–´ìˆìŠµë‹ˆë‹¤.",
                            synonyms=["KT ì˜ì—… ì‹œìŠ¤í…œ", "KOS ì‹œìŠ¤í…œ"],
                            related_terms=["ì˜ì—…ê³„", "ê°œí†µ", "ì²­ì•½"],
                            examples=["WMSì—ì„œ ì¬ê³  ìœ„ì¹˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"],
                            confidence_score=0.88
                        )                                                                                                          
                    }
                    
                    if term.upper() in sample_terms:
                        found_terms[term] = sample_terms[term.upper()]
                        
                except Exception as e:
                    st.error(f"ìš©ì–´ '{term}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return found_terms
    
    def search_documents(self, query: str, expanded_terms: List[str]) -> List[SearchResult]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.document_clients:
            return []
        
        # ëª¨ì˜ ê²€ìƒ‰ ê²°ê³¼
        sample_results = [
            SearchResult(
                title="ERP ì‹œìŠ¤í…œ ì‚¬ìš© ì •ì±…",
                content="ERP ì‹œìŠ¤í…œì€ íšŒì‚¬ì˜ í•µì‹¬ ì—…ë¬´ ì‹œìŠ¤í…œìœ¼ë¡œ ëª¨ë“  ì§ì›ì´ ìŠ¹ì¸ëœ ì ˆì°¨ì— ë”°ë¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ì¬ê³  ê´€ë¦¬, êµ¬ë§¤ ìŠ¹ì¸, ê¸‰ì—¬ ì²˜ë¦¬ ë“±ì˜ ì—…ë¬´ëŠ” ë°˜ë“œì‹œ ERPë¥¼ í†µí•´ ì§„í–‰ë©ë‹ˆë‹¤.",
                source_index="policies",
                score=4.2,
                metadata={"department": "IT", "category": "ì •ì±…"}
            ),
            SearchResult(
                title="ì¬ê³  ê´€ë¦¬ ì ˆì°¨",
                content="ì¬ê³  ê´€ë¦¬ëŠ” WMSì™€ ERP ì‹œìŠ¤í…œì„ ì—°ë™í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì§„í–‰ë©ë‹ˆë‹¤. ì…ê³ , ì¶œê³ , ì¬ê³ ì¡°ì‚¬ì˜ 3ë‹¨ê³„ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                source_index="procedures",
                score=3.8,
                metadata={"department": "ë¬¼ë¥˜", "category": "ì ˆì°¨"}
            ),
            SearchResult(
                title="MES ì‹œìŠ¤í…œ ë§¤ë‰´ì–¼",
                content="MES ì‹œìŠ¤í…œ ì‚¬ìš©ë²•: ë¡œê·¸ì¸ í›„ ì‘ì—…ì§€ì‹œ í™•ì¸, ìƒì‚° ì‹œì‘, ì‹¤ì  ì…ë ¥, ì™„ë£Œ ì²˜ë¦¬ ìˆœìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.",
                source_index="manuals", 
                score=3.5,
                metadata={"department": "ìƒì‚°", "category": "ë§¤ë‰´ì–¼"}
            )
        ]
        
        # ì¿¼ë¦¬ì™€ ê´€ë ¨ì„±ì´ ë†’ì€ ê²°ê³¼ í•„í„°ë§
        relevant_results = []
        query_lower = query.lower()
        for result in sample_results:
            if any(term.lower() in result.content.lower() or term.lower() in result.title.lower() 
                   for term in [query] + expanded_terms):
                relevant_results.append(result)
        
        return relevant_results[:5]
    
    def create_rag_prompt(self, user_query: str, term_definitions: Dict[str, TermDefinition], 
                         search_results: List[SearchResult]) -> str:
        """RAG í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        terminology_section = ""
        if term_definitions:
            terminology_section = "=== ğŸ¢ ì‚¬ë‚´ ìš©ì–´ ì •ì˜ ===\n"
            for term, definition in term_definitions.items():
                terminology_section += f"""
**{definition.term}**
- ì •ì˜: {definition.definition}
- ìƒì„¸: {definition.context}
- ë¶€ì„œ: {definition.department}
- ì—°ê´€ìš©ì–´: {', '.join(definition.related_terms[:3])}

"""
        
        document_section = ""
        if search_results:
            document_section = "=== ğŸ“‹ ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© ===\n"
            for i, doc in enumerate(search_results, 1):
                document_section += f"""
**[ì°¸ê³ ë¬¸ì„œ {i}]** ({doc.source_index})
ì œëª©: {doc.title}
ë‚´ìš©: {doc.content[:300]}...

"""
        
        prompt = f"""ë‹¹ì‹ ì€ ìš°ë¦¬ íšŒì‚¬ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

{terminology_section}

{document_section}

ì§ˆë¬¸: {user_query}

ìœ„ì˜ ìš©ì–´ ì •ì˜ì™€ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹¤ë¬´ì— ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

        return prompt
    
    def call_llm(self, prompt: str) -> str:
        """LLM í˜¸ì¶œ"""
        try:
            if self.config.openai_api_key == 'your-openai-api-key':
                # API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ëª¨ì˜ ì‘ë‹µ
                return """**ğŸ¤– AI ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€ (ë°ëª¨ ëª¨ë“œ)**

ERP ì‹œìŠ¤í…œì—ì„œì˜ ì¬ê³  ê´€ë¦¬ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

**ğŸ“Š ERP ì¬ê³  ê´€ë¦¬ í”„ë¡œì„¸ìŠ¤:**

1. **ì‹¤ì‹œê°„ ì¬ê³  í˜„í™© í™•ì¸**
   - ERP ë©”ì¸ í™”ë©´ â†’ ì¬ê³ ê´€ë¦¬ â†’ í˜„ì¬ê³  ì¡°íšŒ
   - í’ˆëª©ë³„, ì°½ê³ ë³„ ì¬ê³  ìˆ˜ëŸ‰ ì‹¤ì‹œê°„ í™•ì¸ ê°€ëŠ¥

2. **ì…ì¶œê³  ê´€ë¦¬**
   - ì…ê³ : êµ¬ë§¤ â†’ ì…ê³ ë“±ë¡ â†’ ERP ìë™ ë°˜ì˜
   - ì¶œê³ : íŒë§¤ â†’ ì¶œê³ ì§€ì‹œ â†’ ì¬ê³  ìë™ ì°¨ê°

3. **WMS ì—°ë™**
   - WMSì—ì„œ ë¬¼ë¦¬ì  ì¬ê³  ì´ë™ ì²˜ë¦¬
   - ERPì™€ ì‹¤ì‹œê°„ ì—°ë™ìœ¼ë¡œ ì •í™•ì„± í™•ë³´

**ğŸ’¡ ì‹¤ë¬´ íŒ:**
- ì¬ê³  ë¶€ì¡± ì‹œ ìë™ ì•Œë¦¼ ì„¤ì • í™œìš©
- ì •ê¸° ì¬ê³ ì¡°ì‚¬ë¡œ ì‹œìŠ¤í…œ-ì‹¤ë¬¼ ì¼ì¹˜ì„± í™•ë³´
- ì•ˆì „ì¬ê³  ì„¤ì •ìœ¼ë¡œ í’ˆì ˆ ë°©ì§€

**ğŸ“ ë¬¸ì˜ì²˜:** ITíŒ€ (ë‚´ì„  1234)

ì´ ë‹µë³€ì´ ë„ì›€ì´ ë˜ì…¨ë‚˜ìš”? ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!"""
            
            # ì‹¤ì œ OpenAI API í˜¸ì¶œ
            response = openai.ChatCompletion.create(
                model=self.config.openai_model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ íšŒì‚¬ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.config.max_tokens,
                temperature=self.config.openai_temperature
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return f"âŒ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def process_query(self, user_query: str) -> Tuple[str, Dict]:
        """ë©”ì¸ ì¿¼ë¦¬ ì²˜ë¦¬ ë¡œì§"""
        start_time = time.time()
        
        # 1. ìš©ì–´ ì¶”ì¶œ
        extracted_terms = self.term_extractor.extract_terms(user_query)
        
        # 2. ìš©ì–´ ì •ì˜ ê²€ìƒ‰
        term_definitions = self.search_terminology(extracted_terms)
        
        # 3. í™•ì¥ ìš©ì–´ ìƒì„±
        expanded_terms = []
        for term_def in term_definitions.values():
            expanded_terms.extend(term_def.synonyms)
            expanded_terms.extend(term_def.related_terms)
        
        # 4. ë¬¸ì„œ ê²€ìƒ‰
        search_results = self.search_documents(user_query, expanded_terms)
        
        # 5. í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM í˜¸ì¶œ
        prompt = self.create_rag_prompt(user_query, term_definitions, search_results)
        response = self.call_llm(prompt)
        
        # 6. ì²˜ë¦¬ ì‹œê°„ ë° ë©”íƒ€ë°ì´í„°
        processing_time = time.time() - start_time
        
        metadata = {
            "extracted_terms": extracted_terms,
            "found_terms": len(term_definitions),
            "found_documents": len(search_results),
            "processing_time": processing_time,
            "term_definitions": term_definitions,
            "search_results": search_results
        }
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        st.session_state.query_stats.append({
            "timestamp": datetime.now(),
            "query": user_query,
            "terms_found": len(term_definitions),
            "docs_found": len(search_results),
            "processing_time": processing_time
        })
        
        return response, metadata

def render_sidebar():
    """ì‚¬ì´ë“œë°” ë Œë”ë§"""
    st.sidebar.markdown("## ğŸ› ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    
    # ì„¤ì • ìƒíƒœ í™•ì¸
    config = ConfigManager()
    config_issues = config.validate_config()
    
    # ì—°ê²° ìƒíƒœ í‘œì‹œ
    if not config_issues:
        st.sidebar.success("âœ… ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        st.sidebar.error("âŒ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
        for issue in config_issues:
            st.sidebar.write(f"â€¢ {issue}")
    
    # API ìƒíƒœ í™•ì¸
    api_status = "ğŸŸ¢ ì—°ê²°ë¨" if config.openai_api_key != 'your-openai-api-key' else "ğŸ”´ ë¯¸ì„¤ì •"
    st.sidebar.write(f"**OpenAI API:** {api_status}")
    
    search_status = "ğŸŸ¢ ì—°ê²°ë¨" if config.azure_search_key != 'your-search-admin-key' else "ğŸ”´ ë¯¸ì„¤ì •"
    st.sidebar.write(f"**Azure Search:** {search_status}")
    
    # .env íŒŒì¼ ìƒíƒœ
    env_status = "ğŸŸ¢ ë¡œë“œë¨" if DOTENV_AVAILABLE else "ğŸ”´ python-dotenv ì—†ìŒ"
    st.sidebar.write(f"**.env íŒŒì¼:** {env_status}")
    
    st.sidebar.markdown("---")
    
    # ì˜ˆì‹œ ì§ˆë¬¸ë“¤
    st.sidebar.markdown("## ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
    sample_questions = [
        "ERP ì‹œìŠ¤í…œì—ì„œ ì¬ê³  ê´€ë¦¬ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
        "MESì™€ WMSì˜ ì°¨ì´ì ì´ ë¬´ì—‡ì¸ê°€ìš”?",
        "ìƒì‚° í˜„ì¥ì—ì„œ í’ˆì§ˆ ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ì…ë ¥í•˜ë‚˜ìš”?",
        "ì¬ê³  ì‹¤ì‚¬ëŠ” ì–¸ì œ ì–´ë–»ê²Œ ì§„í–‰í•˜ë‚˜ìš”?",
        "êµ¬ë§¤ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
    ]
    
    for i, question in enumerate(sample_questions):
        if st.sidebar.button(f"Q{i+1}", key=f"sample_q_{i}"):
            st.session_state.current_query = question
    
    st.sidebar.markdown("---")
    
    # í†µê³„ ì •ë³´ (ì„¸ì…˜ ìƒíƒœ í™•ì¸ í›„ í‘œì‹œ)
    if hasattr(st.session_state, 'query_stats') and st.session_state.query_stats:
        st.sidebar.markdown("## ğŸ“Š ì‚¬ìš© í†µê³„")
        stats_df = pd.DataFrame(st.session_state.query_stats)
        
        st.sidebar.metric("ì´ ì§ˆë¬¸ ìˆ˜", len(stats_df))
        avg_time = stats_df['processing_time'].mean()
        st.sidebar.metric("í‰ê·  ì²˜ë¦¬ì‹œê°„", f"{avg_time:.2f}ì´ˆ")
        
        # ìµœê·¼ ì§ˆë¬¸ë“¤
        st.sidebar.markdown("### ìµœê·¼ ì§ˆë¬¸")
        recent_queries = stats_df.tail(3)['query'].tolist()
        for query in recent_queries:
            st.sidebar.write(f"â€¢ {query[:30]}...")

def render_term_definitions(term_definitions):
    """ìš©ì–´ ì •ì˜ í‘œì‹œ"""
    if not term_definitions:
        return
    
    st.markdown("### ğŸ¢ ê´€ë ¨ ìš©ì–´ ì •ì˜")
    
    cols = st.columns(min(len(term_definitions), 3))
    
    for i, (term, definition) in enumerate(term_definitions.items()):
        with cols[i % 3]:
            confidence_color = "ğŸŸ¢" if definition.confidence_score > 0.8 else "ğŸŸ¡"
            
            st.markdown(f"""
            <div class="term-card">
                <h4>{confidence_color} {definition.term}</h4>
                <p><strong>ì •ì˜:</strong> {definition.definition}</p>
                <p><strong>ë¶€ì„œ:</strong> {definition.department}</p>
                <p><strong>ì‹ ë¢°ë„:</strong> {definition.confidence_score:.1%}</p>
                <details>
                    <summary>ìƒì„¸ ì •ë³´</summary>
                    <p><strong>ìƒì„¸ì„¤ëª…:</strong> {definition.context}</p>
                    <p><strong>ì—°ê´€ìš©ì–´:</strong> {', '.join(definition.related_terms[:3])}</p>
                    <p><strong>ì‚¬ìš©ì˜ˆì‹œ:</strong> {definition.examples[0] if definition.examples else 'ì—†ìŒ'}</p>
                </details>
            </div>
            """, unsafe_allow_html=True)

def render_search_results(search_results):
    """ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ"""
    if not search_results:
        return
    
    st.markdown("### ğŸ“‹ ì°¸ê³  ë¬¸ì„œ")
    
    for i, result in enumerate(search_results):
        relevance_icon = "ğŸ”¥" if result.score > 3.5 else "ğŸ“„"
        
        with st.expander(f"{relevance_icon} {result.title} (ì ìˆ˜: {result.score:.1f})"):
            st.write(f"**ì¶œì²˜:** {result.source_index}")
            st.write(f"**ë¶€ì„œ:** {result.metadata.get('department', 'ë¯¸ë¶„ë¥˜')}")
            st.write(f"**ì¹´í…Œê³ ë¦¬:** {result.metadata.get('category', 'ë¯¸ë¶„ë¥˜')}")
            st.write("**ë‚´ìš©:**")
            st.write(result.content[:500] + "..." if len(result.content) > 500 else result.content)

def render_analytics():
    """ë¶„ì„ í˜ì´ì§€ ë Œë”ë§"""
    st.markdown('<div class="main-header">ğŸ“Š ì‚¬ìš© ë¶„ì„</div>', unsafe_allow_html=True)
    
    # ì„¸ì…˜ ìƒíƒœ í™•ì¸
    if not hasattr(st.session_state, 'query_stats') or not st.session_state.query_stats:
        st.info("ì•„ì§ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!")
        return
    
    stats_df = pd.DataFrame(st.session_state.query_stats)
    
    # ë©”íŠ¸ë¦­ í‘œì‹œ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ ì§ˆë¬¸ ìˆ˜", len(stats_df))
    
    with col2:
        avg_time = stats_df['processing_time'].mean()
        st.metric("í‰ê·  ì²˜ë¦¬ì‹œê°„", f"{avg_time:.2f}ì´ˆ")
    
    with col3:
        avg_terms = stats_df['terms_found'].mean()
        st.metric("í‰ê·  ìš©ì–´ ë°œê²¬", f"{avg_terms:.1f}ê°œ")
    
    with col4:
        avg_docs = stats_df['docs_found'].mean()
        st.metric("í‰ê·  ë¬¸ì„œ ë°œê²¬", f"{avg_docs:.1f}ê°œ")
    
    # ì‹œê°„ë³„ ì§ˆë¬¸ ì¶”ì´
    st.markdown("### ğŸ“ˆ ì‹œê°„ë³„ ì§ˆë¬¸ ì¶”ì´")
    if len(stats_df) > 1:
        stats_df['hour'] = stats_df['timestamp'].dt.hour
        hourly_counts = stats_df.groupby('hour').size()
        
        if PLOTLY_AVAILABLE:
            fig = px.bar(x=hourly_counts.index, y=hourly_counts.values,
                        labels={'x': 'ì‹œê°„', 'y': 'ì§ˆë¬¸ ìˆ˜'},
                        title="ì‹œê°„ëŒ€ë³„ ì§ˆë¬¸ ë¶„í¬")
            st.plotly_chart(fig, use_container_width=True)
        else:
            # Plotly ì—†ì´ ê°„ë‹¨í•œ ì°¨íŠ¸ í‘œì‹œ
            chart_data = pd.DataFrame({
                'ì‹œê°„': hourly_counts.index,
                'ì§ˆë¬¸ ìˆ˜': hourly_counts.values
            })
            st.bar_chart(chart_data.set_index('ì‹œê°„'))
    
    # ì²˜ë¦¬ ì‹œê°„ ë¶„í¬
    st.markdown("### â±ï¸ ì²˜ë¦¬ ì‹œê°„ ë¶„í¬")
    if PLOTLY_AVAILABLE:
        fig = px.histogram(stats_df, x='processing_time', nbins=20,
                          labels={'processing_time': 'ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)', 'count': 'ë¹ˆë„'},
                          title="ì§ˆë¬¸ ì²˜ë¦¬ ì‹œê°„ ë¶„í¬")
        st.plotly_chart(fig, use_container_width=True)
    else:
        # Streamlit ê¸°ë³¸ íˆìŠ¤í† ê·¸ë¨ ì‚¬ìš©
        st.histogram(stats_df['processing_time'], bins=20)
    
    # ìµœê·¼ ì§ˆë¬¸ ë‚´ì—­
    st.markdown("### ğŸ“ ìµœê·¼ ì§ˆë¬¸ ë‚´ì—­")
    recent_df = stats_df.tail(10)[['timestamp', 'query', 'terms_found', 'docs_found', 'processing_time']]
    recent_df['timestamp'] = recent_df['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')
    
    st.dataframe(
        recent_df,
        column_config={
            "timestamp": "ì‹œê°„",
            "query": "ì§ˆë¬¸",
            "terms_found": "ìš©ì–´ ìˆ˜",
            "docs_found": "ë¬¸ì„œ ìˆ˜",
            "processing_time": st.column_config.NumberColumn("ì²˜ë¦¬ì‹œê°„(ì´ˆ)", format="%.2f")
        },
        use_container_width=True
    )

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”"""
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'query_stats' not in st.session_state:
        st.session_state.query_stats = []
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = None

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì„¸ì…˜ ìƒíƒœì— ìºì‹œ)
    if st.session_state.rag_system is None:
        with st.spinner("ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
            st.session_state.rag_system = StreamlitRAGSystem()
    
    rag_system = st.session_state.rag_system
    
    # ì‚¬ì´ë“œë°” ë Œë”ë§
    render_sidebar()
    
    # íƒ­ ìƒì„±
    tab1, tab2, tab3 = st.tabs(["ğŸ’¬ ì§ˆì˜ì‘ë‹µ", "ğŸ“Š ë¶„ì„", "âš™ï¸ ê´€ë¦¬"])
    
    with tab1:
        st.markdown('<div class="main-header">ğŸ¤– Enterprise AI Assistant</div>', unsafe_allow_html=True)
        st.markdown("**ì‚¬ë‚´ ìš©ì–´ë¥¼ ì´í•´í•˜ëŠ” ë˜‘ë˜‘í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?**")
        
        # ì§ˆë¬¸ ì…ë ¥
        if 'current_query' in st.session_state:
            user_query = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", value=st.session_state.current_query, height=100)
            del st.session_state.current_query
        else:
            user_query = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ERP ì‹œìŠ¤í…œì—ì„œ ì¬ê³ ëŠ” ì–´ë–»ê²Œ ê´€ë¦¬í•˜ë‚˜ìš”?", height=100)
        
        col1, col2 = st.columns([1, 4])
        with col1:
            submit_button = st.button("ğŸ” ì§ˆë¬¸í•˜ê¸°", type="primary")
        with col2:
            clear_button = st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”")
        
        if clear_button:
            st.session_state.chat_history = []
            st.session_state.query_stats = []
            st.rerun()
        
        # ì§ˆë¬¸ ì²˜ë¦¬
        if submit_button and user_query.strip():
            with st.spinner("ğŸ¤” AIê°€ ë‹µë³€ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                response, metadata = rag_system.process_query(user_query)
                
                # ëŒ€í™” ê¸°ë¡ ì €ì¥
                st.session_state.chat_history.append({
                    "query": user_query,
                    "response": response,
                    "metadata": metadata,
                    "timestamp": datetime.now()
                })
        
        # ìµœì‹  ë‹µë³€ í‘œì‹œ
        if st.session_state.chat_history:
            latest_chat = st.session_state.chat_history[-1]
            
            st.markdown("---")
            
            # ë‹µë³€ í‘œì‹œ
            st.markdown("### ğŸ¤– AI ë‹µë³€")
            st.markdown(latest_chat["response"])
            
            # ë©”íƒ€ë°ì´í„° í‘œì‹œ
            metadata = latest_chat["metadata"]
            
            # ìš©ì–´ ì •ì˜ í‘œì‹œ
            if "term_definitions" in metadata:
                render_term_definitions(metadata["term_definitions"])
            
            # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
            if "search_results" in metadata:
                render_search_results(metadata["search_results"])
            
            # ì²˜ë¦¬ ì •ë³´
            with st.expander("ğŸ” ì²˜ë¦¬ ì •ë³´"):
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ì¶”ì¶œëœ ìš©ì–´", len(metadata.get("extracted_terms", [])))
                with col2:
                    st.metric("ë°œê²¬ëœ ìš©ì–´ ì •ì˜", metadata.get("found_terms", 0))
                with col3:
                    st.metric("ê´€ë ¨ ë¬¸ì„œ", metadata.get("found_documents", 0))
                
                st.write("**ì¶”ì¶œëœ ìš©ì–´:**", ", ".join(metadata.get("extracted_terms", [])))
                st.write(f"**ì²˜ë¦¬ ì‹œê°„:** {metadata.get('processing_time', 0):.2f}ì´ˆ")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
        if len(st.session_state.chat_history) > 1:
            st.markdown("---")
            st.markdown("### ğŸ“š ì´ì „ ëŒ€í™”")
            
            for i, chat in enumerate(reversed(st.session_state.chat_history[:-1])):
                with st.expander(f"ì§ˆë¬¸ {len(st.session_state.chat_history)-i-1}: {chat['query'][:50]}..."):
                    st.write("**ì§ˆë¬¸:**", chat['query'])
                    st.write("**ë‹µë³€:**", chat['response'][:200] + "..." if len(chat['response']) > 200 else chat['response'])
                    st.write("**ì‹œê°„:**", chat['timestamp'].strftime('%Y-%m-%d %H:%M:%S'))
    
    with tab2:
        render_analytics()
    
    with tab3:
        st.markdown('<div class="main-header">âš™ï¸ ì‹œìŠ¤í…œ ê´€ë¦¬</div>', unsafe_allow_html=True)
        
        # ì„¤ì • ì •ë³´
        st.markdown("### ğŸ”§ í˜„ì¬ ì„¤ì •")
        config = ConfigManager()
        config_summary = config.get_config_summary()
        
        # ì„¤ì • ê²€ì¦
        config_issues = config.validate_config()
        if config_issues:
            st.error("âš ï¸ ì„¤ì • ë¬¸ì œ:")
            for issue in config_issues:
                st.write(f"â€¢ {issue}")
            st.info("ğŸ’¡ .env íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        else:
            st.success("âœ… ëª¨ë“  ì„¤ì •ì´ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ì„¤ì • ì •ë³´ í…Œì´ë¸”
        config_data = []
        for key, value in config_summary.items():
            status = "ğŸŸ¢ ì„¤ì •ë¨" if "ë¯¸ì„¤ì •" not in str(value) and "your-" not in str(value) else "ğŸ”´ ë¯¸ì„¤ì •"
            config_data.append({
                "ì„¤ì • í•­ëª©": key,
                "ê°’": str(value),
                "ìƒíƒœ": status
            })
        
        config_df = pd.DataFrame(config_data)
        st.dataframe(config_df, use_container_width=True, hide_index=True)
        
        st.markdown("---")
        
        # ìš©ì–´ ì‚¬ì „ ê´€ë¦¬
        st.markdown("### ğŸ“š ìš©ì–´ ì‚¬ì „ ê´€ë¦¬")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### â• ìƒˆ ìš©ì–´ ì¶”ê°€")
            with st.form("add_term_form"):
                new_term = st.text_input("ìš©ì–´")
                new_definition = st.text_area("ì •ì˜")
                new_context = st.text_area("ìƒì„¸ ì„¤ëª…")
                new_department = st.selectbox("ë‹´ë‹¹ ë¶€ì„œ", ["IT", "ìƒì‚°", "ë¬¼ë¥˜", "ì˜ì—…", "ì¸ì‚¬", "ì¬ë¬´"])
                new_synonyms = st.text_input("ë™ì˜ì–´ (ì‰¼í‘œë¡œ êµ¬ë¶„)")
                new_related = st.text_input("ê´€ë ¨ ìš©ì–´ (ì‰¼í‘œë¡œ êµ¬ë¶„)")
                new_examples = st.text_area("ì‚¬ìš© ì˜ˆì‹œ (í•œ ì¤„ì— í•˜ë‚˜ì”©)")
                
                submit_term = st.form_submit_button("ìš©ì–´ ì¶”ê°€")
                
                if submit_term and new_term and new_definition:
                    st.success(f"âœ… '{new_term}' ìš©ì–´ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.info("ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Azure Search ì¸ë±ìŠ¤ì— ì—…ë¡œë“œë©ë‹ˆë‹¤.")
        
        with col2:
            st.markdown("#### ğŸ“‹ í˜„ì¬ ìš©ì–´ ëª©ë¡")
            
            # ìƒ˜í”Œ ìš©ì–´ ëª©ë¡
            sample_terms = [
                {"ìš©ì–´": "ERP", "ë¶€ì„œ": "IT", "ì •ì˜": "ì „ì‚¬ì  ìì› ê´€ë¦¬ ì‹œìŠ¤í…œ"},
                {"ìš©ì–´": "MES", "ë¶€ì„œ": "ìƒì‚°", "ì •ì˜": "ì œì¡°ì‹¤í–‰ì‹œìŠ¤í…œ"},
                {"ìš©ì–´": "WMS", "ë¶€ì„œ": "ë¬¼ë¥˜", "ì •ì˜": "ì°½ê³ ê´€ë¦¬ì‹œìŠ¤í…œ"},
                {"ìš©ì–´": "CRM", "ë¶€ì„œ": "ì˜ì—…", "ì •ì˜": "ê³ ê°ê´€ê³„ê´€ë¦¬ì‹œìŠ¤í…œ"},
                {"ìš©ì–´": "HRM", "ë¶€ì„œ": "ì¸ì‚¬", "ì •ì˜": "ì¸ì ìì›ê´€ë¦¬ì‹œìŠ¤í…œ"}
            ]
            
            terms_df = pd.DataFrame(sample_terms)
            st.dataframe(terms_df, use_container_width=True, hide_index=True)
            
            # ìš©ì–´ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            st.markdown("#### ğŸ” ìš©ì–´ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
            test_term = st.text_input("í…ŒìŠ¤íŠ¸í•  ìš©ì–´ ì…ë ¥")
            if st.button("ê²€ìƒ‰ í…ŒìŠ¤íŠ¸") and test_term:
                with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                    found_terms = rag_system.search_terminology([test_term])
                    if found_terms:
                        for term, definition in found_terms.items():
                            st.success(f"âœ… ë°œê²¬: {definition.term}")
                            st.write(f"ì •ì˜: {definition.definition}")
                            st.write(f"ì‹ ë¢°ë„: {definition.confidence_score:.1%}")
                    else:
                        st.warning("âŒ í•´ë‹¹ ìš©ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        st.markdown("---")
        
        # ë¬¸ì„œ ì¸ë±ìŠ¤ ìƒíƒœ
        st.markdown("### ğŸ“„ ë¬¸ì„œ ì¸ë±ìŠ¤ ìƒíƒœ")
        
        index_status = []
        for name, index_name in config.document_indexes.items():
            # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Azure Searchì—ì„œ ë¬¸ì„œ ìˆ˜ë¥¼ ê°€ì ¸ì˜´
            doc_count = {"policies": 15, "procedures": 32, "manuals": 28, "announcements": 7}.get(name, 0)
            
            index_status.append({
                "ì¸ë±ìŠ¤ëª…": name,
                "Azure ì¸ë±ìŠ¤": index_name,
                "ë¬¸ì„œ ìˆ˜": doc_count,
                "ìƒíƒœ": "ğŸŸ¢ ì •ìƒ" if doc_count > 0 else "ğŸ”´ ë¹„ì–´ìˆìŒ",
                "ìµœì¢… ì—…ë°ì´íŠ¸": "2024-01-15"
            })
        
        status_df = pd.DataFrame(index_status)
        st.dataframe(status_df, use_container_width=True, hide_index=True)
        
        # ë¬¸ì„œ ì—…ë¡œë“œ ì„¹ì…˜
        st.markdown("### ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ")
        
        uploaded_file = st.file_uploader(
            "ìƒˆ ë¬¸ì„œ ì—…ë¡œë“œ",
            type=['pdf', 'docx', 'txt'],
            help="PDF, Word ë¬¸ì„œ, í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        
        if uploaded_file:
            col1, col2 = st.columns(2)
            with col1:
                doc_title = st.text_input("ë¬¸ì„œ ì œëª©", value=uploaded_file.name)
                doc_category = st.selectbox("ì¹´í…Œê³ ë¦¬", ["ì •ì±…", "ì ˆì°¨", "ë§¤ë‰´ì–¼", "ê³µì§€ì‚¬í•­"])
                doc_department = st.selectbox("ë‹´ë‹¹ ë¶€ì„œ", ["IT", "ìƒì‚°", "ë¬¼ë¥˜", "ì˜ì—…", "ì¸ì‚¬", "ì¬ë¬´", "ì „ì²´"])
            
            with col2:
                doc_tags = st.text_input("íƒœê·¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)")
                doc_description = st.text_area("ë¬¸ì„œ ì„¤ëª…")
            
            if st.button("ë¬¸ì„œ ì²˜ë¦¬ ë° ì—…ë¡œë“œ"):
                with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì¸ë±ìŠ¤ì— ì¶”ê°€í•˜ëŠ” ì¤‘..."):
                    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë¬¸ì„œ íŒŒì‹± ë° Azure Search ì—…ë¡œë“œ ë¡œì§
                    time.sleep(2)  # ì‹œë®¬ë ˆì´ì…˜
                    st.success("âœ… ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ê³  ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.info(f"íŒŒì¼ëª…: {uploaded_file.name}")
                    st.info(f"í¬ê¸°: {uploaded_file.size} bytes")
        
        st.markdown("---")
        
        # ì‹œìŠ¤í…œ ì§„ë‹¨
        st.markdown("### ğŸ”§ ì‹œìŠ¤í…œ ì§„ë‹¨")
        
        if st.button("ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"):
            with st.spinner("ì‹œìŠ¤í…œ ì§„ë‹¨ ì¤‘..."):
                time.sleep(1)
                
                # ì§„ë‹¨ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
                diagnostics = [
                    {"í•­ëª©": "Azure Search ì—°ê²°", "ìƒíƒœ": "ğŸŸ¢ ì •ìƒ", "ì‘ë‹µì‹œê°„": "45ms"},
                    {"í•­ëª©": "OpenAI API ì—°ê²°", "ìƒíƒœ": "ğŸŸ¢ ì •ìƒ", "ì‘ë‹µì‹œê°„": "234ms"},
                    {"í•­ëª©": "ìš©ì–´ ì¸ë±ìŠ¤", "ìƒíƒœ": "ğŸŸ¢ ì •ìƒ", "ë¬¸ì„œìˆ˜": "156ê°œ"},
                    {"í•­ëª©": "ì •ì±… ì¸ë±ìŠ¤", "ìƒíƒœ": "ğŸŸ¢ ì •ìƒ", "ë¬¸ì„œìˆ˜": "15ê°œ"},
                    {"í•­ëª©": "ì ˆì°¨ ì¸ë±ìŠ¤", "ìƒíƒœ": "ğŸŸ¢ ì •ìƒ", "ë¬¸ì„œìˆ˜": "32ê°œ"},
                    {"í•­ëª©": "ë§¤ë‰´ì–¼ ì¸ë±ìŠ¤", "ìƒíƒœ": "ğŸŸ¢ ì •ìƒ", "ë¬¸ì„œìˆ˜": "28ê°œ"}
                ]
                
                diag_df = pd.DataFrame(diagnostics)
                st.dataframe(diag_df, use_container_width=True, hide_index=True)
                
                st.success("âœ… ëª¨ë“  ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!")
        
        # ë°ì´í„° ì´ˆê¸°í™”
        st.markdown("---")
        st.markdown("### âš ï¸ ë°ì´í„° ê´€ë¦¬")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”", type="secondary"):
                st.session_state.chat_history = []
                st.session_state.query_stats = []
                st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.rerun()
        
        with col2:
            if st.button("ğŸ“Š í†µê³„ ë°ì´í„° ë‚´ë³´ë‚´ê¸°", type="secondary"):
                if hasattr(st.session_state, 'query_stats') and st.session_state.query_stats:
                    stats_df = pd.DataFrame(st.session_state.query_stats)
                    csv = stats_df.to_csv(index=False)
                    st.download_button(
                        label="CSV ë‹¤ìš´ë¡œë“œ",
                        data=csv,
                        file_name=f"rag_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                else:
                    st.info("ë‚´ë³´ë‚¼ í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('rag_system.log')
        ]
    )

def load_custom_css():
    """ì»¤ìŠ¤í…€ CSS ë¡œë“œ"""
    st.markdown("""
    <style>
        /* ì¶”ê°€ ìŠ¤íƒ€ì¼ë§ */
        .stButton > button {
            border-radius: 20px;
            border: none;
            padding: 0.5rem 1rem;
            font-weight: bold;
        }
        
        .stButton > button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
        }
        
        .metric-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 1rem;
            border-radius: 10px;
            text-align: center;
        }
        
        .chat-message {
            padding: 1rem;
            border-radius: 10px;
            margin: 0.5rem 0;
            border-left: 4px solid #2E86AB;
            background-color: #f8f9fa;
        }
        
        .footer {
            text-align: center;
            padding: 2rem;
            color: #666;
            font-size: 0.8rem;
        }
    </style>
    """, unsafe_allow_html=True)

def display_footer():
    """í‘¸í„° í‘œì‹œ"""
    st.markdown("---")
    st.markdown("""
    <div class="footer">
        ğŸ¢ Enterprise RAG System v1.0 | 
        Built with Streamlit & Azure AI | 
        Â© 2024 Your Company
    </div>
    """, unsafe_allow_html=True)

# ì•± ì„¤ì • ë° ì‹¤í–‰
if __name__ == "__main__":
    setup_logging()
    load_custom_css()
    
    # ë©”ì¸ ì•± ì‹¤í–‰
    main()
    
    # í‘¸í„° í‘œì‹œ
    display_footer()

# ì‹¤í–‰ ì‹œ í•„ìš”í•œ requirements.txt ë‚´ìš©:
"""
streamlit>=1.28.0
azure-search-documents>=11.4.0
openai>=0.28.0
pandas>=2.0.0
plotly>=5.15.0
python-dotenv>=1.0.0
"""

# .env íŒŒì¼ ì˜ˆì‹œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìƒì„±):
"""
# Azure Search ì„¤ì •
AZURE_SEARCH_ENDPOINT=https://your-search-service.search.windows.net
AZURE_SEARCH_KEY=your-azure-search-admin-key
AZURE_SEARCH_API_VERSION=2023-11-01

# OpenAI ì„¤ì •
OPENAI_API_KEY=sk-your-openai-api-key
OPENAI_MODEL=gpt-4
OPENAI_TEMPERATURE=0.1
MAX_TOKENS=2000

# ì¸ë±ìŠ¤ ì´ë¦„ (ì„ íƒì‚¬í•­ - ê¸°ë³¸ê°’ ì‚¬ìš© ê°€ëŠ¥)
TERMINOLOGY_INDEX=company-terminology
POLICIES_INDEX=company-policies
PROCEDURES_INDEX=work-procedures
MANUALS_INDEX=system-manuals
ANNOUNCEMENTS_INDEX=company-announcements

# ê²€ìƒ‰ ì„¤ì • (ì„ íƒì‚¬í•­)
MAX_TERMS_PER_QUERY=10
MAX_DOCUMENTS_PER_INDEX=3
MIN_CONFIDENCE_SCORE=0.3

# ë””ë²„ê¹… ì„¤ì • (ì„ íƒì‚¬í•­)
DEBUG_MODE=false
LOG_LEVEL=INFO
"""

# ì„¤ì¹˜ ë° ì‹¤í–‰ ê°€ì´ë“œ:
"""
# 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install streamlit azure-search-documents openai pandas plotly python-dotenv

# 2. .env íŒŒì¼ ìƒì„± (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—)
# ìœ„ì˜ .env íŒŒì¼ ì˜ˆì‹œ ë‚´ìš©ì„ ë³µì‚¬í•˜ì—¬ ì‹¤ì œ ê°’ìœ¼ë¡œ ìˆ˜ì •

# 3. .env íŒŒì¼ì„ .gitignoreì— ì¶”ê°€ (ë³´ì•ˆ)
echo ".env" >> .gitignore

# 4. ì•± ì‹¤í–‰
streamlit run streamlit_rag_app.py

# 5. ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8501 ì ‘ì†
"""

# í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì„¤ì • ë°©ë²• (Windows):
"""
# ëª…ë ¹ í”„ë¡¬í”„íŠ¸ì—ì„œ:
set AZURE_SEARCH_ENDPOINT=https://your-search-service.search.windows.net
set AZURE_SEARCH_KEY=your-azure-search-admin-key
set OPENAI_API_KEY=sk-your-openai-api-key
streamlit run streamlit_rag_app.py

# PowerShellì—ì„œ:
$env:AZURE_SEARCH_ENDPOINT="https://your-search-service.search.windows.net"
$env:AZURE_SEARCH_KEY="your-azure-search-admin-key"
$env:OPENAI_API_KEY="sk-your-openai-api-key"
streamlit run streamlit_rag_app.py
"""

# í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì„¤ì • ë°©ë²• (Linux/Mac):
"""
export AZURE_SEARCH_ENDPOINT="https://your-search-service.search.windows.net"
export AZURE_SEARCH_KEY="your-azure-search-admin-key"
export OPENAI_API_KEY="sk-your-openai-api-key"
streamlit run streamlit_rag_app.py
"""