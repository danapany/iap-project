import streamlit as st
import os
from dotenv import load_dotenv
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.models import VectorizableTextQuery
from langchain_openai import AzureChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import ToolMessage, SystemMessage
import sqlite3  # ì¶”ê°€

# === .env íŒŒì¼ ë¡œë“œ ===
load_dotenv()

# === í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ===
OPENAI_ENDPOINT = os.getenv("OPENAI_ENDPOINT")
OPENAI_API_KEY = os.getenv("OPENAI_KEY")
CHAT_MODEL = os.getenv("CHAT_MODEL")
SEARCH_ENDPOINT = os.getenv("SEARCH_ENDPOINT")
SEARCH_API_KEY = os.getenv("SEARCH_API_KEY")
INDEX_NAME = os.getenv("INDEX_GUIDE_NAME")

# === Azure OpenAI ì„¤ì • ===
from openai import AzureOpenAI

client = AzureOpenAI(
    api_version="2024-12-01-preview",
    azure_endpoint=OPENAI_ENDPOINT,
    api_key=OPENAI_API_KEY,
)
deployment = CHAT_MODEL

# === Azure Cognitive Search ì„¤ì • ===
search_client = SearchClient(
    endpoint=SEARCH_ENDPOINT,
    index_name=INDEX_NAME,
    credential=AzureKeyCredential(SEARCH_API_KEY),
)

# === ê²€ìƒ‰ ë„êµ¬ ì •ì˜ ===
from langchain_core.tools import tool


@tool
def search_guide(query, top_k=3):
    """
    íšŒì‚¬ì˜ ì¥ì• ëŒ€ì‘ ì§€ì¹¨ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        query (str): ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë˜ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤. ì˜ˆì‹œ: 'ì¥ì• ëŒ€ì‘ ì ˆì°¨ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”'
        top_k (int, optional): ë°˜í™˜í•  ê²°ê³¼ì˜ ê°œìˆ˜ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ 3ì…ë‹ˆë‹¤. ì˜ˆì‹œ: 5

    Returns:
        list: ê²€ìƒ‰ ê²°ê³¼ (ì ìˆ˜, í…ìŠ¤íŠ¸) ë¦¬ìŠ¤íŠ¸
    """
    vector_query = VectorizableTextQuery(
        text=query, k_nearest_neighbors=top_k, fields="text_vector"
    )
    results = search_client.search(
        search_text=query, vector_queries=[vector_query], filter=None, top=top_k
    )

    return [(doc["@search.score"], doc["chunk"]) for doc in results]


# === LangChain ë° LangGraph ì„¤ì • ===
llm = AzureChatOpenAI(
    azure_deployment=deployment,
    api_version="2024-12-01-preview",
    azure_endpoint=OPENAI_ENDPOINT,
    temperature=0.0,
    api_key=OPENAI_API_KEY,
)

agent_engine = llm.bind_tools(tools=[search_guide])
tool_node = ToolNode([search_guide])


class AgentState(MessagesState): ...


def should_continue(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]

    if last_message.tool_calls:
        return "tools"

    return END


def call_model(state: AgentState):
    sys_prompt = SystemMessage(
        """[ì§€ì¹¨]
        - ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•  ê²ƒ
        - ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ
        - ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° "ì£¼ì–´ì§„ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ì‘ë‹µí•  ê²ƒ
        - ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…í™•íˆ ê·¸ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•  ê²ƒ
        - ë‹µë³€ì€ ë…¼ë¦¬ì ì´ê³  êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì œê³µí•  ê²ƒ
        - ë‹µë³€ì€ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•  ê²ƒ"""
    )

    response = agent_engine.invoke([sys_prompt] + state["messages"])

    return {"messages": [response]}


def save_faq_to_db(content):
    conn = sqlite3.connect("data/db/faq.db")
    c = conn.cursor()
    # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
    c.execute(
        """
        CREATE TABLE IF NOT EXISTS faq (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            content TEXT NOT NULL
        )
    """
    )
    # FAQ ë‚´ìš© ì €ì¥
    c.execute("INSERT INTO faq (content) VALUES (?)", (content,))
    conn.commit()
    conn.close()


# === ì›Œí¬í”Œë¡œìš° ì •ì˜ ===
workflow = StateGraph(AgentState)
workflow.add_node("agent", call_model)
workflow.add_node("tools", tool_node)
workflow.set_entry_point("agent")
workflow.add_conditional_edges("agent", should_continue, ["tools", END])
workflow.add_edge("tools", "agent")

graph = workflow.compile()


# === Streamlit ì•± ===
st.title("ğŸ’¡ ì¥ì• ëŒ€ì‘ì ˆì°¨ FAQ ìƒì„± (ê´€ë¦¬ìš©)")
st.caption("íšŒì‚¬ ì¥ì• ëŒ€ì‘ ì ˆì°¨ì— ëŒ€í•œ FAQë¥¼ 20ê°œ ìƒì„±í•©ë‹ˆë‹¤")

question = "ì¥ì• ëŒ€ì‘ì ˆì°¨ì— ëŒ€í•´ì„œ FAQ 20ê°œë¥¼ ìƒì„±í•´ì¤˜."

if question:
    st.info("FAQ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
    inputs = {"messages": [("user", question)]}
    stream = graph.stream(inputs, stream_mode="values")

    for s in stream:
        message = s["messages"][-1]

    # FAQ ê²°ê³¼ DBì— ì €ì¥
    save_faq_to_db(message.content)

    # Display the generated FAQ messages
    st.write(message.content)

    st.info("FAQ ìƒì„± ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
