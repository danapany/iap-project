import streamlit as st
import re
import time
import os
from datetime import datetime
from config.prompts import SystemPrompts
from config.settings_local import AppConfigLocal
from utils.search_utils_local import SearchManagerLocal
from utils.ui_components_local import UIComponentsLocal
from utils.reprompting_db_manager import RepromptingDBManager
from utils.chart_utils import ChartManager
from utils.statistics_db_manager import StatisticsDBManager
from utils.filter_manager import DocumentFilterManager, QueryType

try:
    from utils.monitoring_manager import MonitoringManager
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False
    class MonitoringManager:
        def __init__(self, *args, **kwargs): pass
        def log_user_activity(self, *args, **kwargs): pass

class DataIntegrityNormalizer:
    """ğŸš¨ RAG ë°ì´í„° ë¬´ê²°ì„± ì ˆëŒ€ ë³´ì¥ ì •ê·œí™” í´ë˜ìŠ¤"""
    
    @staticmethod
    def normalize_error_time(error_time):
        if error_time is None: return 0
        try:
            if isinstance(error_time, str):
                error_time = error_time.strip()
                if error_time == '' or error_time.lower() in ['null', 'none', 'n/a']:
                    return 0
                return int(float(error_time))
            return int(error_time)
        except (ValueError, TypeError):
            return 0
    

    @staticmethod
    def normalize_date_fields(doc):
        normalized_doc = doc.copy()
        error_date = doc.get('error_date', '')
        
        if error_date:
            try:
                error_date_str = str(error_date).strip()
                if '-' in error_date_str and len(error_date_str) >= 7:
                    parts = error_date_str.split('-')
                    if len(parts) >= 2:
                        if parts[0].isdigit() and len(parts[0]) == 4 and not normalized_doc.get('year'):
                            normalized_doc['year'] = parts[0]
                        elif parts[0].isdigit() and len(parts[0]) == 2 and not normalized_doc.get('year'):
                            # ëª¨ë“  2ìë¦¬ ì—°ë„ë¥¼ 2000ë…„ëŒ€ë¡œ ì²˜ë¦¬
                            short_year = int(parts[0])
                            if 0 <= short_year <= 99:
                                normalized_doc['year'] = f"20{short_year:02d}"
                        if parts[1].isdigit():
                            month_num = int(parts[1])
                            if 1 <= month_num <= 12 and not normalized_doc.get('month'):
                                normalized_doc['month'] = str(month_num)
                elif len(error_date_str) >= 8 and error_date_str.isdigit():
                    if not normalized_doc.get('year'):
                        normalized_doc['year'] = error_date_str[:4]
                    try:
                        month_num = int(error_date_str[4:6])
                        if 1 <= month_num <= 12 and not normalized_doc.get('month'):
                            normalized_doc['month'] = str(month_num)
                    except (ValueError, TypeError):
                        pass
            except (ValueError, TypeError):
                pass
        
        return normalized_doc
    
    @staticmethod
    def preserve_original_fields(doc):
        preserved_doc = doc.copy()
        critical_fields = [
            'incident_id', 'service_name', 'symptom', 'root_cause', 
            'incident_repair', 'incident_plan', 'effect', 'error_date',
            'incident_grade', 'owner_depart', 'daynight', 'week',
            'cause_type', 'done_type'
        ]
        
        for field in critical_fields:
            original_value = doc.get(field)
            if original_value is not None:
                preserved_doc[field] = str(original_value).strip() if str(original_value).strip() else original_value
            else:
                preserved_doc[field] = ''
        
        return preserved_doc
    
    @classmethod
    def normalize_document_with_integrity(cls, doc):
        if doc is None: return None
        
        normalized_doc = cls.preserve_original_fields(doc)
        normalized_doc = cls.normalize_date_fields(normalized_doc)
        normalized_doc['error_time'] = cls.normalize_error_time(doc.get('error_time'))
        normalized_doc['_integrity_preserved'] = True
        normalized_doc['_normalized_timestamp'] = datetime.now().isoformat()
        
        return normalized_doc

class StatisticsValidator:
    def __init__(self):
        self.validation_errors = []
        self.validation_warnings = []
    
    def validate_document(self, doc, doc_index):
        errors, warnings = [], []
        for field in ['incident_id', 'service_name', 'error_date']:
            if not doc.get(field):
                errors.append(f"Document {doc_index}: {field} field is empty")
        
        error_time = doc.get('error_time')
        if error_time is not None:
            try:
                error_time_int = int(error_time)
                if error_time_int < 0:
                    warnings.append(f"Document {doc_index}: error_time is negative")
                elif error_time_int > 10080:
                    warnings.append(f"Document {doc_index}: error_time is abnormally large")
            except (ValueError, TypeError):
                errors.append(f"Document {doc_index}: error_time format error")
        return errors, warnings
    
    def validate_statistics_result(self, stats, original_doc_count):
        errors, warnings = [], []
        if stats.get('total_count', 0) != original_doc_count:
            errors.append(f"Total count mismatch: calculated({stats.get('total_count', 0)}) != original({original_doc_count})")
        return errors, warnings

class ImprovedStatisticsCalculator:
    def __init__(self, remove_duplicates=False):
        self.validator = StatisticsValidator()
        self.normalizer = DataIntegrityNormalizer()
        self.remove_duplicates = remove_duplicates
    
    def _extract_filter_conditions(self, query):
        conditions = {'year': None, 'month': None, 'start_month': None, 'end_month': None, 
                    'daynight': None, 'week': None, 'service_name': None, 'department': None, 'grade': None}
        if not query: return conditions
        
        query_lower = query.lower()
        
        # ì—°ë„ ì¶”ì¶œ - 4ìë¦¬ ì—°ë„ ìš°ì„  
        year_match = re.search(r'\b(202[0-9]|201[0-9])ë…„?\b', query_lower)
        if year_match: 
            conditions['year'] = year_match.group(1)
        else:
            # 2ìë¦¬ ì—°ë„ íŒ¨í„´ ê°ì§€ (ì˜ˆ: 22ë…„, 20ë…„)
            short_year_match = re.search(r'\b(\d{2})ë…„\b', query_lower)
            if short_year_match:
                short_year = int(short_year_match.group(1))
                # ëª¨ë“  2ìë¦¬ ì—°ë„ë¥¼ 2000ë…„ëŒ€ë¡œ ë³€í™˜
                if 0 <= short_year <= 99:
                    conditions['year'] = f"20{short_year:02d}"
        
        # ì›” ë²”ìœ„ ì²˜ë¦¬
        month_patterns = [r'\b(\d+)\s*~\s*(\d+)ì›”\b', r'\b(\d+)ì›”\s*~\s*(\d+)ì›”\b', 
                        r'\b(\d+)\s*-\s*(\d+)ì›”\b', r'\b(\d+)ì›”\s*-\s*(\d+)ì›”\b']
        for pattern in month_patterns:
            month_range_match = re.search(pattern, query_lower)
            if month_range_match:
                start_month, end_month = int(month_range_match.group(1)), int(month_range_match.group(2))
                if 1 <= start_month <= 12 and 1 <= end_month <= 12 and start_month <= end_month:
                    conditions['start_month'], conditions['end_month'] = start_month, end_month
                    break
        
        if not conditions['start_month']:
            month_match = re.search(r'\b(\d{1,2})ì›”\b', query_lower)
            if month_match and 1 <= int(month_match.group(1)) <= 12:
                conditions['month'] = str(int(month_match.group(1)))
        
        # ì‹œê°„ëŒ€ ì²˜ë¦¬
        if any(word in query_lower for word in ['ì•¼ê°„', 'ë°¤', 'ìƒˆë²½', 'ì‹¬ì•¼']):
            conditions['daynight'] = 'ì•¼ê°„'
        elif any(word in query_lower for word in ['ì£¼ê°„', 'ë‚®', 'ì˜¤ì „', 'ì˜¤í›„']):
            conditions['daynight'] = 'ì£¼ê°„'
        
        # ìš”ì¼ ì²˜ë¦¬
        week_patterns = {'ì›”': ['ì›”ìš”ì¼', 'ì›”'], 'í™”': ['í™”ìš”ì¼', 'í™”'], 'ìˆ˜': ['ìˆ˜ìš”ì¼', 'ìˆ˜'], 
                        'ëª©': ['ëª©ìš”ì¼', 'ëª©'], 'ê¸ˆ': ['ê¸ˆìš”ì¼', 'ê¸ˆ'], 'í† ': ['í† ìš”ì¼', 'í† '], 'ì¼': ['ì¼ìš”ì¼', 'ì¼']}
        for week_key, patterns in week_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                conditions['week'] = week_key
                break
        
        if 'í‰ì¼' in query_lower: conditions['week'] = 'í‰ì¼'
        elif 'ì£¼ë§' in query_lower: conditions['week'] = 'ì£¼ë§'
        
        grade_match = re.search(r'(\d+)ë“±ê¸‰', query_lower)
        if grade_match: conditions['grade'] = f"{grade_match.group(1)}ë“±ê¸‰"
        
        return conditions
    
    def _validate_document_against_conditions(self, doc, conditions):
        if conditions['year'] and self._extract_year_from_document(doc) != conditions['year']:
            return False, "year mismatch"
        
        if conditions['start_month'] and conditions['end_month']:
            doc_month = self._extract_month_from_document(doc)
            if not doc_month: return False, "no month information"
            try:
                if not (conditions['start_month'] <= int(doc_month) <= conditions['end_month']):
                    return False, "month not in range"
            except (ValueError, TypeError):
                return False, "invalid month format"
        elif conditions['month'] and str(self._extract_month_from_document(doc)) != conditions['month']:
            return False, "month mismatch"
        
        if conditions['daynight']:
            doc_daynight = doc.get('daynight', '').strip()
            if not doc_daynight or doc_daynight != conditions['daynight']:
                return False, "daynight mismatch"
        
        if conditions['week']:
            doc_week = doc.get('week', '').strip()
            required_week = conditions['week']
            if required_week == 'í‰ì¼':
                if doc_week not in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ']:
                    return False, "not weekday"
            elif required_week == 'ì£¼ë§':
                if doc_week not in ['í† ', 'ì¼']:
                    return False, "not weekend"
            elif not doc_week or doc_week != required_week:
                return False, "week mismatch"
        
        if conditions['grade'] and doc.get('incident_grade', '') != conditions['grade']:
            return False, "grade mismatch"
        
        return True, "passed"
    
    def _extract_year_from_document(self, doc):
        # year í•„ë“œì—ì„œ ì§ì ‘ ì¶”ì¶œ (4ìë¦¬ ë˜ëŠ” 2ìë¦¬)
        for key in ['year', 'extracted_year']:
            year = doc.get(key)
            if year:
                year_str = str(year).strip()
                if len(year_str) == 4 and year_str.isdigit():
                    return year_str
                elif len(year_str) == 2 and year_str.isdigit():
                    # ëª¨ë“  2ìë¦¬ ì—°ë„ë¥¼ 2000ë…„ëŒ€ë¡œ ë³€í™˜
                    short_year = int(year_str)
                    if 0 <= short_year <= 99:
                        return f"20{short_year:02d}"
        
        # error_dateì—ì„œ ì¶”ì¶œ
        error_date = str(doc.get('error_date', '')).strip()
        if len(error_date) >= 4:
            if '-' in error_date:
                parts = error_date.split('-')
                if parts and len(parts[0]) == 4 and parts[0].isdigit():
                    return parts[0]
                elif parts and len(parts[0]) == 2 and parts[0].isdigit():
                    # ëª¨ë“  2ìë¦¬ ì—°ë„ë¥¼ 2000ë…„ëŒ€ë¡œ ì²˜ë¦¬
                    short_year = int(parts[0])
                    if 0 <= short_year <= 99:
                        return f"20{short_year:02d}"
            elif error_date[:4].isdigit():
                return error_date[:4]
        return None
    
    def _extract_month_from_document(self, doc):
        for key in ['month', 'extracted_month']:
            month = doc.get(key)
            if month:
                try:
                    month_num = int(month)
                    if 1 <= month_num <= 12:
                        return str(month_num)
                except (ValueError, TypeError):
                    pass
        
        error_date = str(doc.get('error_date', '')).strip()
        if '-' in error_date:
            parts = error_date.split('-')
            if len(parts) >= 2 and parts[1].isdigit():
                try:
                    month_num = int(parts[1])
                    if 1 <= month_num <= 12:
                        return str(month_num)
                except (ValueError, TypeError):
                    pass
        elif len(error_date) >= 6 and error_date.isdigit():
            try:
                month_num = int(error_date[4:6])
                if 1 <= month_num <= 12:
                    return str(month_num)
            except (ValueError, TypeError):
                pass
        return None
    
    def _apply_filters(self, documents, conditions):
        return [doc for doc in documents if self._validate_document_against_conditions(doc, conditions)[0]]
    
    def _empty_statistics(self):
        return {'total_count': 0, 'yearly_stats': {}, 'monthly_stats': {}, 
                'time_stats': {'daynight': {}, 'week': {}}, 'department_stats': {}, 
                'service_stats': {}, 'grade_stats': {}, 'is_error_time_query': False, 
                'validation': {'errors': [], 'warnings': [], 'is_valid': True}, 'primary_stat_type': None}
    
    def _is_error_time_query(self, query):
        return query and any(keyword in query.lower() for keyword in 
                           ['ì¥ì• ì‹œê°„', 'ì¥ì•  ì‹œê°„', 'error_time', 'ì‹œê°„ í†µê³„', 'ì‹œê°„ í•©ê³„', 'ì‹œê°„ í•©ì‚°', 'ë¶„'])
    
    def _determine_primary_stat_type(self, query, yearly_stats, monthly_stats, time_stats, service_stats, department_stats, grade_stats):
        if query:
            query_lower = query.lower()
            keywords = [('yearly', ['ì—°ë„ë³„', 'ë…„ë„ë³„', 'ë…„ë³„', 'ì—°ë³„']), ('monthly', ['ì›”ë³„']), 
                       ('time', ['ì‹œê°„ëŒ€ë³„', 'ì£¼ê°„', 'ì•¼ê°„']), ('weekday', ['ìš”ì¼ë³„']), 
                       ('department', ['ë¶€ì„œë³„', 'íŒ€ë³„']), ('service', ['ì„œë¹„ìŠ¤ë³„']), ('grade', ['ë“±ê¸‰ë³„'])]
            
            for stat_type, kws in keywords:
                if any(kw in query_lower for kw in kws):
                    return stat_type
            
            if re.search(r'\b\d+ì›”\b', query_lower):
                return 'monthly'
        
        stat_counts = {'yearly': len(yearly_stats), 'monthly': len(monthly_stats), 
                      'service': len(service_stats), 'department': len(department_stats), 
                      'grade': len(grade_stats), 'time': len(time_stats.get('daynight', {})) + len(time_stats.get('week', {}))}
        return max(stat_counts.items(), key=lambda x: x[1])[0] if any(stat_counts.values()) else 'yearly'
    
    def _calculate_detailed_statistics(self, documents, conditions, is_error_time_query):
        stats = {'total_count': len(documents), 'yearly_stats': {}, 'monthly_stats': {}, 
                'time_stats': {'daynight': {}, 'week': {}}, 'department_stats': {}, 
                'service_stats': {}, 'grade_stats': {}, 'is_error_time_query': is_error_time_query, 
                'filter_conditions': conditions, 'calculation_details': {}}
        
        yearly_temp, monthly_temp = {}, {}
        for doc in documents:
            year = self._extract_year_from_document(doc)
            month = self._extract_month_from_document(doc)
            error_time = doc.get('error_time', 0) if is_error_time_query else 1
            
            if year:
                yearly_temp[year] = yearly_temp.get(year, 0) + error_time
            if month:
                try:
                    month_num = int(month)
                    if 1 <= month_num <= 12:
                        monthly_temp[month_num] = monthly_temp.get(month_num, 0) + error_time
                except (ValueError, TypeError):
                    pass
        
        for year in sorted(yearly_temp.keys()):
            stats['yearly_stats'][f"{year}ë…„"] = yearly_temp[year]
        for month_num in sorted(monthly_temp.keys()):
            stats['monthly_stats'][f"{month_num}ì›”"] = monthly_temp[month_num]
        
        # ê¸°íƒ€ í†µê³„ ê³„ì‚°
        temp_dicts = {'daynight': {}, 'week': {}, 'department': {}, 'service': {}, 'grade': {}}
        field_mapping = {'daynight': 'daynight', 'week': 'week', 'department': 'owner_depart', 
                        'service': 'service_name', 'grade': 'incident_grade'}
        
        for doc in documents:
            error_time = doc.get('error_time', 0) if is_error_time_query else 1
            for stat_key, field_name in field_mapping.items():
                value = doc.get(field_name, '')
                if value:
                    temp_dicts[stat_key][value] = temp_dicts[stat_key].get(value, 0) + error_time
        
        # ì‹œê°„ëŒ€ í†µê³„
        for time_key in ['ì£¼ê°„', 'ì•¼ê°„']:
            if time_key in temp_dicts['daynight']:
                stats['time_stats']['daynight'][time_key] = temp_dicts['daynight'][time_key]
        
        # ìš”ì¼ í†µê³„
        for week_key in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼', 'í‰ì¼', 'ì£¼ë§']:
            if week_key in temp_dicts['week']:
                week_display = f"{week_key}ìš”ì¼" if week_key in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼'] else week_key
                stats['time_stats']['week'][week_display] = temp_dicts['week'][week_key]
        
        stats['department_stats'] = dict(sorted(temp_dicts['department'].items(), key=lambda x: x[1], reverse=True)[:10])
        stats['service_stats'] = dict(sorted(temp_dicts['service'].items(), key=lambda x: x[1], reverse=True)[:10])
        
        # ë“±ê¸‰ í†µê³„
        for grade_key in ['1ë“±ê¸‰', '2ë“±ê¸‰', '3ë“±ê¸‰', '4ë“±ê¸‰']:
            if grade_key in temp_dicts['grade']:
                stats['grade_stats'][grade_key] = temp_dicts['grade'][grade_key]
        for grade_key, value in sorted(temp_dicts['grade'].items()):
            if grade_key not in stats['grade_stats']:
                stats['grade_stats'][grade_key] = value
        
        total_error_time = sum(doc.get('error_time', 0) for doc in documents)
        stats['calculation_details'] = {
            'total_error_time_minutes': total_error_time,
            'total_error_time_hours': round(total_error_time / 60, 2),
            'average_error_time': round(total_error_time / len(documents), 2) if documents else 0,
            'max_error_time': max((doc.get('error_time', 0) for doc in documents), default=0),
            'min_error_time': min((doc.get('error_time', 0) for doc in documents), default=0),
            'documents_with_error_time': len([doc for doc in documents if doc.get('error_time', 0) > 0])
        }
        stats['primary_stat_type'] = None
        return stats
    
    def calculate_comprehensive_statistics(self, query, documents, query_type="default"):
        if not documents:
            return self._empty_statistics()
        
        normalized_docs, validation_errors, validation_warnings = [], [], []
        for i, doc in enumerate(documents):
            if doc is None: continue
            errors, warnings = self.validator.validate_document(doc, i)
            validation_errors.extend(errors)
            validation_warnings.extend(warnings)
            normalized_doc = self.normalizer.normalize_document_with_integrity(doc)
            normalized_docs.append(normalized_doc)
        
        if self.remove_duplicates:
            unique_docs = {}
            for doc in normalized_docs:
                incident_id = doc.get('incident_id', '')
                if incident_id and incident_id not in unique_docs:
                    unique_docs[incident_id] = doc
            clean_documents = list(unique_docs.values())
        else:
            clean_documents = normalized_docs
        
        filter_conditions = self._extract_filter_conditions(query)
        is_stats_query = any(keyword in query.lower() for keyword in 
                           ['ê±´ìˆ˜', 'í†µê³„', 'ì—°ë„ë³„', 'ì›”ë³„', 'í˜„í™©', 'ë¶„í¬', 'ì•Œë ¤ì¤˜', 'ëª‡ê±´', 'ê°œìˆ˜'])
        filtered_docs = clean_documents if is_stats_query else self._apply_filters(clean_documents, filter_conditions)
        
        is_error_time_query = self._is_error_time_query(query)
        stats = self._calculate_detailed_statistics(filtered_docs, filter_conditions, is_error_time_query)
        stats['primary_stat_type'] = self._determine_primary_stat_type(
            query, stats['yearly_stats'], stats['monthly_stats'], stats['time_stats'], 
            stats['service_stats'], stats['department_stats'], stats['grade_stats'])
        
        result_errors, result_warnings = self.validator.validate_statistics_result(stats, len(filtered_docs))
        validation_errors.extend(result_errors)
        validation_warnings.extend(result_warnings)
        stats['validation'] = {'errors': validation_errors, 'warnings': validation_warnings, 'is_valid': len(validation_errors) == 0}
        return stats

class QueryProcessorLocal:
    def __init__(self, azure_openai_client, search_client, model_name, config=None, embedding_client=None):
        self.azure_openai_client = azure_openai_client
        self.search_client = search_client
        self.model_name = model_name
        self.config = config or AppConfigLocal()
        self.embedding_client = embedding_client
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.search_manager = SearchManagerLocal(search_client, embedding_client, self.config)
        self.ui_components = UIComponentsLocal()
        self.reprompting_db_manager = RepromptingDBManager()
        self.chart_manager = ChartManager()
        self.normalizer = DataIntegrityNormalizer()
        self.statistics_calculator = ImprovedStatisticsCalculator(remove_duplicates=False)
        
        # âœ… StatisticsDBManagerëŠ” lazy initializationìœ¼ë¡œ ë³€ê²½
        self._statistics_db_manager = None  # ì´ˆê¸°ì—ëŠ” None
        
        self.filter_manager = DocumentFilterManager(debug_mode=True)
        
        self.debug_mode = True
        self._manual_logging_enabled = True

        # í†µê³„ ê´€ë ¨ í‚¤ì›Œë“œ ëŒ€í­ í™•ì¥
        self.statistics_keywords = {
            'basic_stats': [
                'ê±´ìˆ˜', 'ê°œìˆ˜', 'ìˆ˜ëŸ‰', 'ìˆ«ì', 'ëª‡ê±´', 'ëª‡ê°œ', 'ì–¼ë§ˆë‚˜', 'ì–´ëŠì •ë„', 
                'ì–¼ë§ˆ', 'ì–´ë–»ê²Œ', 'ì–´ëŠ', 'ì–´ë–¤', 'ëª‡ë²ˆ', 'ëª‡ì°¨ë¡€', 'ëª‡íšŒ'
            ],
            'stats_verbs': [
                'ì•Œë ¤ì¤˜', 'ë³´ì—¬ì¤˜', 'ë§í•´ì¤˜', 'í™•ì¸í•´ì¤˜', 'ì²´í¬í•´ì¤˜', 'ì¡°íšŒí•´ì¤˜',
                'ê²€ìƒ‰í•´ì¤˜', 'ì°¾ì•„ì¤˜', 'ê°€ì ¸ì™€ì¤˜', 'ì¶”ì¶œí•´ì¤˜', 'ë¶„ì„í•´ì¤˜'
            ],
            'stats_nouns': [
                'í†µê³„', 'í˜„í™©', 'ë¶„í¬', 'ì§‘ê³„', 'í•©ê³„', 'ì´í•©', 'ëˆ„ì ', 'ì „ì²´',
                'ìš”ì•½', 'ê°œìš”', 'ìƒí™©', 'ì •ë„', 'ìˆ˜ì¤€', 'ë²”ìœ„', 'ê·œëª¨', 'ì‹¤ì '
            ],
            'time_keywords': [
                'ì—°ë„ë³„', 'ë…„ë„ë³„', 'ë…„ë³„', 'ì—°ë³„', 'í•´ë³„', 'ì›”ë³„', 'ë§¤ì›”', 'ì›”ê°„',
                'ìš”ì¼ë³„', 'ì£¼ê°„ë³„', 'ì¼ë³„', 'ì‹œê°„ëŒ€ë³„', 'ì£¼ì•¼ë³„', 'ê¸°ê°„ë³„'
            ],
            'category_keywords': [
                'ë“±ê¸‰ë³„', 'ì¥ì• ë“±ê¸‰ë³„', 'gradeë³„', 'ë¶€ì„œë³„', 'íŒ€ë³„', 'ì¡°ì§ë³„',
                'ì„œë¹„ìŠ¤ë³„', 'ì‹œìŠ¤í…œë³„', 'ì›ì¸ë³„', 'ì›ì¸ìœ í˜•ë³„', 'ìœ í˜•ë³„', 'íƒ€ì…ë³„'
            ],
            'service_patterns': [
                r'\b([A-Z]{2,10})\s+(?:ì—°ë„ë³„|ì›”ë³„|ì¥ì• |ê±´ìˆ˜|í†µê³„|í˜„í™©)',
                r'([ê°€-í£]{2,20}(?:í”Œë«í¼|ì‹œìŠ¤í…œ|ì„œë¹„ìŠ¤|í¬í„¸|ì•±|ê´€ë¦¬|ì„¼í„°))\s+(?:ì—°ë„ë³„|ì›”ë³„|ì¥ì• |ê±´ìˆ˜|í†µê³„)',
                r'(?:ì•Œë ¤|ë³´ì—¬|í™•ì¸).*?([A-Zê°€-í£][A-Zê°€-í£0-9\s]{1,20}).*?(?:ì—°ë„ë³„|ì›”ë³„|ì¥ì• |ê±´ìˆ˜|í†µê³„)'
            ]
        }

        # ëª¨ë‹ˆí„°ë§ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        if MONITORING_AVAILABLE:
            try:
                self.monitoring_manager = MonitoringManager()
                self.monitoring_enabled = True
            except Exception:
                self.monitoring_manager = MonitoringManager()
                self.monitoring_enabled = False
        else:
            self.monitoring_manager = MonitoringManager()
            self.monitoring_enabled = False
    
    @property
    def statistics_db_manager(self):
        """âœ… Lazy initialization property for StatisticsDBManager"""
        if self._statistics_db_manager is None:
            if self.debug_mode:
                print("ğŸ“„ Initializing StatisticsDBManager (lazy loading)...")
            self._statistics_db_manager = StatisticsDBManager()
        return self._statistics_db_manager

    def _validate_documents_against_query_conditions(self, query, documents):
        """ğŸš¨ í•µì‹¬ ê°œì„ : ì¿¼ë¦¬ ì¡°ê±´ê³¼ ë¬¸ì„œ ì¼ì¹˜ì„± ê²€ì¦"""
        if not query or not documents:
            return []
        
        # ì¿¼ë¦¬ì—ì„œ ì¡°ê±´ ì¶”ì¶œ
        extracted_service = self.search_manager.extract_service_name_from_query(query)
        extracted_year = self._extract_year_from_query(query)
        extracted_months = self._extract_months_from_query(query)
        time_conditions = self.extract_time_conditions(query)  # âœ… ì¶”ê°€
        
        if self.debug_mode:
            print(f"DEBUG: Query conditions - Service: {extracted_service}, Year: {extracted_year}, Months: {extracted_months}, TimeConditions: {time_conditions}")
        
        validated_documents = []
        
        for doc in documents:
            is_valid = True
            validation_reasons = []
            
            # ì„œë¹„ìŠ¤ëª… ê²€ì¦ (ê¸°ì¡´)
            if extracted_service:
                doc_service = doc.get('service_name', '').strip()
                if not self._is_service_match(extracted_service, doc_service):
                    is_valid = False
                    validation_reasons.append(f"Service mismatch: expected '{extracted_service}', got '{doc_service}'")
            
            # ì—°ë„ ê²€ì¦ (ê¸°ì¡´)
            if extracted_year:
                doc_year = self._extract_year_from_document(doc)
                if not doc_year or doc_year != extracted_year:
                    is_valid = False
                    validation_reasons.append(f"Year mismatch: expected '{extracted_year}', got '{doc_year}'")
            
            # ì›” ê²€ì¦ (ê¸°ì¡´)
            if extracted_months:
                doc_month = self._extract_month_from_document(doc)
                if doc_month and int(doc_month) not in extracted_months:
                    is_valid = False
                    validation_reasons.append(f"Month mismatch: expected {extracted_months}, got '{doc_month}'")
            
            # âœ… ì‹œê°„ëŒ€ ê²€ì¦ ì¶”ê°€
            if time_conditions.get('is_time_query') and time_conditions.get('daynight'):
                doc_daynight = doc.get('daynight', '').strip()
                if not doc_daynight or doc_daynight != time_conditions['daynight']:
                    is_valid = False
                    validation_reasons.append(f"Daynight mismatch: expected '{time_conditions['daynight']}', got '{doc_daynight}'")
            
            # âœ… ìš”ì¼ ê²€ì¦ ì¶”ê°€
            if time_conditions.get('is_time_query') and time_conditions.get('week'):
                doc_week = doc.get('week', '').strip()
                expected_week = time_conditions['week']
                
                # í‰ì¼/ì£¼ë§ íŠ¹ë³„ ì²˜ë¦¬
                if expected_week == 'í‰ì¼':
                    if doc_week not in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ']:
                        is_valid = False
                        validation_reasons.append(f"Week mismatch: expected weekday, got '{doc_week}'")
                elif expected_week == 'ì£¼ë§':
                    if doc_week not in ['í† ', 'ì¼']:
                        is_valid = False
                        validation_reasons.append(f"Week mismatch: expected weekend, got '{doc_week}'")
                else:
                    if not doc_week or doc_week != expected_week:
                        is_valid = False
                        validation_reasons.append(f"Week mismatch: expected '{expected_week}', got '{doc_week}'")
            
            if is_valid:
                validated_documents.append(doc)
                if self.debug_mode:
                    print(f"DEBUG: Document {doc.get('incident_id', 'Unknown')} validated successfully")
            else:
                if self.debug_mode:
                    print(f"DEBUG: Document {doc.get('incident_id', 'Unknown')} rejected: {', '.join(validation_reasons)}")
        
        return validated_documents
    
    def _is_service_match(self, query_service, doc_service):
        """ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ ë¡œì§ - ì •í™•í•œ ë§¤ì¹­ê³¼ ìœ ì—°í•œ ë§¤ì¹­ ëª¨ë‘ ê³ ë ¤"""
        if not query_service or not doc_service:
            return False
        
        query_service_lower = query_service.lower().strip()
        doc_service_lower = doc_service.lower().strip()
        
        # 1. ì •í™•í•œ ë§¤ì¹­
        if query_service_lower == doc_service_lower:
            return True
        
        # 2. í¬í•¨ ê´€ê³„ ë§¤ì¹­
        if query_service_lower in doc_service_lower or doc_service_lower in query_service_lower:
            return True
        
        # 3. ê³µë°± ì œê±° í›„ ë§¤ì¹­
        query_no_space = re.sub(r'\s+', '', query_service_lower)
        doc_no_space = re.sub(r'\s+', '', doc_service_lower)
        if query_no_space == doc_no_space:
            return True
        
        # 4. í† í° ê¸°ë°˜ ë§¤ì¹­ (ë¶€ë¶„ ì¼ì¹˜)
        query_tokens = set(re.findall(r'[a-zê°€-í£0-9]+', query_service_lower))
        doc_tokens = set(re.findall(r'[a-zê°€-í£0-9]+', doc_service_lower))
        
        if query_tokens and doc_tokens:
            intersection = len(query_tokens.intersection(doc_tokens))
            union = len(query_tokens.union(doc_tokens))
            similarity = intersection / union if union > 0 else 0
            
            # ìœ ì‚¬ë„ 70% ì´ìƒì´ë©´ ë§¤ì¹­ìœ¼ë¡œ ê°„ì£¼
            if similarity >= 0.7:
                return True
        
        return False
    
    def _extract_year_from_query(self, query):
        """ì¿¼ë¦¬ì—ì„œ ì—°ë„ ì¶”ì¶œ"""
        if not query:
            return None
        
        # 4ìë¦¬ ì—°ë„ íŒ¨í„´
        year_patterns = [r'\b(\d{4})ë…„\b', r'\b(\d{4})\s*ë…„ë„\b', r'\b(\d{4})ë…„ë„\b', r'\b(202[0-9]|201[0-9])\b']
        
        for pattern in year_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                year = matches[-1]  # ê°€ì¥ ë§ˆì§€ë§‰ ë§¤ì¹­ëœ ì—°ë„ ì‚¬ìš©
                if year.isdigit() and len(year) == 4:
                    return year
        
        # 2ìë¦¬ ì—°ë„ íŒ¨í„´
        short_year_match = re.search(r'\b(\d{2})ë…„\b', query)
        if short_year_match:
            short_year = int(short_year_match.group(1))
            # ëª¨ë“  2ìë¦¬ ì—°ë„ë¥¼ 2000ë…„ëŒ€ë¡œ ë³€í™˜
            if 0 <= short_year <= 99:
                return f"20{short_year:02d}"
        
        return None
    
    def _extract_months_from_query(self, query):
        """ì¿¼ë¦¬ì—ì„œ ì›” ì¶”ì¶œ"""
        if not query:
            return []
        
        months = []
        
        # ì›” ë²”ìœ„ íŒ¨í„´ (ì˜ˆ: 1~6ì›”, 1ì›”~6ì›”)
        range_patterns = [r'\b(\d+)\s*~\s*(\d+)ì›”\b', r'\b(\d+)ì›”\s*~\s*(\d+)ì›”\b']
        for pattern in range_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for match in matches:
                start_month, end_month = int(match[0]), int(match[1])
                if 1 <= start_month <= 12 and 1 <= end_month <= 12 and start_month <= end_month:
                    months.extend(range(start_month, end_month + 1))
        
        # ê°œë³„ ì›” íŒ¨í„´ (ì˜ˆ: 1ì›”, 2ì›”)
        if not months:  # ë²”ìœ„ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ
            month_matches = re.findall(r'\b(\d{1,2})ì›”\b', query)
            for match in month_matches:
                month_num = int(match)
                if 1 <= month_num <= 12:
                    months.append(month_num)
        
        return list(set(months))  # ì¤‘ë³µ ì œê±°
    
    def _extract_year_from_document(self, doc):
        """ë¬¸ì„œì—ì„œ ì—°ë„ ì¶”ì¶œ"""
        # year í•„ë“œì—ì„œ ì§ì ‘ ì¶”ì¶œ
        for key in ['year', 'extracted_year']:
            year = doc.get(key)
            if year:
                year_str = str(year).strip()
                if len(year_str) == 4 and year_str.isdigit():
                    return year_str
        
        # error_dateì—ì„œ ì¶”ì¶œ
        error_date = str(doc.get('error_date', '')).strip()
        if len(error_date) >= 4:
            if '-' in error_date:
                parts = error_date.split('-')
                if parts and len(parts[0]) == 4 and parts[0].isdigit():
                    return parts[0]
            elif error_date[:4].isdigit():
                return error_date[:4]
        
        return None
    
    def _extract_month_from_document(self, doc):
        """ë¬¸ì„œì—ì„œ ì›” ì¶”ì¶œ"""
        for key in ['month', 'extracted_month']:
            month = doc.get(key)
            if month:
                try:
                    month_num = int(month)
                    if 1 <= month_num <= 12:
                        return str(month_num)
                except (ValueError, TypeError):
                    pass
        
        error_date = str(doc.get('error_date', '')).strip()
        if '-' in error_date:
            parts = error_date.split('-')
            if len(parts) >= 2 and parts[1].isdigit():
                try:
                    month_num = int(parts[1])
                    if 1 <= month_num <= 12:
                        return str(month_num)
                except (ValueError, TypeError):
                    pass
        elif len(error_date) >= 6 and error_date.isdigit():
            try:
                month_num = int(error_date[4:6])
                if 1 <= month_num <= 12:
                    return str(month_num)
            except (ValueError, TypeError):
                pass
        return None

    def generate_rag_response_with_data_integrity(self, query, documents, query_type="default", time_conditions=None, department_conditions=None, reprompting_info=None):
        """ğŸš¨ RAG ë°ì´í„° ë¬´ê²°ì„±ì„ ì ˆëŒ€ ë³´ì¥í•˜ëŠ” ì‘ë‹µ ìƒì„± - ì¡°ê±´ ê²€ì¦ ê°•í™”"""
        if not documents:
            return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì–´ì„œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        try:
            # ğŸš¨ ì›ë³¸ ë°ì´í„° ë³´ì¡´ì„ ìœ„í•œ ì „ì²˜ë¦¬
            integrity_documents = [self.normalizer.normalize_document_with_integrity(doc) for doc in documents]
            
            if self.debug_mode:
                print(f"DEBUG: Data integrity preserved for {len(integrity_documents)} documents")
            
            # âš ï¸ í•µì‹¬ ê°œì„ : ì¡°ê±´ ë§¤ì¹­ ì¬ê²€ì¦ - ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë¬¸ì„œ ì¼ì¹˜ì„± í™•ì¸
            validated_documents = self._validate_documents_against_query_conditions(query, integrity_documents)
            
            if not validated_documents:
                # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ìˆì§€ë§Œ ì¡°ê±´ì— ë§ì§€ ì•ŠëŠ” ê²½ìš°
                extracted_service = self.search_manager.extract_service_name_from_query(query)
                extracted_year = self._extract_year_from_query(query)
                
                if extracted_service and extracted_year:
                    return f"ì œê³µëœ ë°ì´í„°ì—ì„œ '{extracted_service}'ì™€ ê´€ë ¨ëœ {extracted_year}ë…„ë„ ì¥ì•  ë‚´ì—­ì€ ì—†ìŠµë‹ˆë‹¤."
                elif extracted_service:
                    return f"ì œê³µëœ ë°ì´í„°ì—ì„œ '{extracted_service}' ì„œë¹„ìŠ¤ì™€ ê´€ë ¨ëœ ì¥ì•  ë‚´ì—­ì€ ì—†ìŠµë‹ˆë‹¤."
                elif extracted_year:
                    return f"ì œê³µëœ ë°ì´í„°ì—ì„œ {extracted_year}ë…„ë„ ì¥ì•  ë‚´ì—­ì€ ì—†ìŠµë‹ˆë‹¤."
                else:
                    return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ìš”ì²­í•˜ì‹  ì¡°ê±´ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            
            if self.debug_mode:
                print(f"DEBUG: Validated {len(validated_documents)}/{len(integrity_documents)} documents match query conditions")
            
            # ê²€ì¦ëœ ë¬¸ì„œë¡œ ì‘ë‹µ ìƒì„± ì§„í–‰
            final_documents = validated_documents
            
            # í†µê³„ ê³„ì‚° - statistics ì¿¼ë¦¬íƒ€ì…ì—ì„œë§Œ ì°¨íŠ¸ ìƒì„±
            if query_type == "statistics":
                return self._generate_statistics_response_with_integrity(query, final_documents)
            
            # ì •ë ¬ ì ìš©
            sort_info = self.detect_sorting_requirements(query)
            processing_documents = self.apply_custom_sorting(final_documents, sort_info)
            
            final_query = reprompting_info.get('transformed_query', query) if reprompting_info and reprompting_info.get('transformed') else query
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - ì›ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš©
            context_parts = [f"""ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(processing_documents)}ê±´
    âš ï¸ ì¤‘ìš”: ì•„ë˜ ëª¨ë“  í•„ë“œê°’ì€ ì›ë³¸ RAG ë°ì´í„°ì´ë¯€ë¡œ ì ˆëŒ€ ë³€ê²½í•˜ê±°ë‚˜ ìš”ì•½í•˜ì§€ ë§ˆì„¸ìš”."""]
            
            for i, doc in enumerate(processing_documents[:30]):
                context_parts.append(f"""ë¬¸ì„œ {i+1}:
    ì¥ì•  ID: {doc.get('incident_id', '')}
    ì„œë¹„ìŠ¤ëª…: {doc.get('service_name', '')}
    ì¥ì• ì‹œê°„: {doc.get('error_time', 0)}ë¶„
    ì¥ì• í˜„ìƒ: {doc.get('symptom', '')}
    ì¥ì• ì›ì¸: {doc.get('root_cause', '')}
    ë³µêµ¬ë°©ë²•: {doc.get('incident_repair', '')}
    ê°œì„ ê³„íš: {doc.get('incident_plan', '')}
    ì²˜ë¦¬ìœ í˜•: {doc.get('done_type', '')}
    ë°œìƒì¼ì: {doc.get('error_date', '')}
    ì¥ì• ë“±ê¸‰: {doc.get('incident_grade', '')}
    ë‹´ë‹¹ë¶€ì„œ: {doc.get('owner_depart', '')}
    ì‹œê°„ëŒ€: {doc.get('daynight', '')}
    ìš”ì¼: {doc.get('week', '')}
    """)
            
            # ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            integrity_prompt = self._get_data_integrity_prompt(query_type)
            
            user_prompt = f"""{integrity_prompt}

    **ì›ë³¸ RAG ë°ì´í„° (ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€):**
    {chr(10).join(context_parts)}

    **ì‚¬ìš©ì ì§ˆë¬¸:** {final_query}

    **ì‘ë‹µ ì§€ì¹¨:**
    1. ìœ„ ì›ë³¸ ë°ì´í„°ì˜ ëª¨ë“  í•„ë“œê°’ì„ ì •í™•íˆ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”
    2. ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”
    3. 'í•´ë‹¹ ì •ë³´ì—†ìŒ' ê°™ì€ ì„ì˜ì˜ ê°’ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”
    4. ë¹ˆ í•„ë“œëŠ” ë¹ˆ ìƒíƒœë¡œ ë‘ê±°ë‚˜ ì›ë³¸ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”

    ë‹µë³€:"""

            max_tokens = 2500 if query_type == 'inquiry' else 3000 if query_type == 'repair' else 1500
            response = self.azure_openai_client.chat.completions.create(
                model=self.model_name, 
                messages=[
                    {"role": "system", "content": integrity_prompt}, 
                    {"role": "user", "content": user_prompt}
                ], 
                temperature=0.0, 
                max_tokens=max_tokens
            )
            
            final_answer = response.choices[0].message.content
            return final_answer
            
        except Exception as e:
            st.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _get_data_integrity_prompt(self, query_type):
        """ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        base_prompt = f"""ë‹¹ì‹ ì€ IT ì‹œìŠ¤í…œ ì¥ì•  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸš¨ ì ˆëŒ€ ìµœìš°ì„  ê·œì¹™ - ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥ ğŸš¨
**ì œê³µëœ RAG ë°ì´í„°ì˜ ì–´ë–¤ ì •ë³´ë„ ì ˆëŒ€ ë³€ê²½í•˜ê±°ë‚˜ ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”**

### 1. ì›ë³¸ ë°ì´í„° ë³´ì¡´ ì›ì¹™
- **ëª¨ë“  í•„ë“œê°’ì„ ì›ë³¸ RAG ë°ì´í„° ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”**
- **ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ì˜ì—­í•˜ì§€ ë§ˆì„¸ìš”**
- **"í•´ë‹¹ ì •ë³´ì—†ìŒ", "N/A", "ì •ë³´ ì—†ìŒ" ë“±ì˜ ì„ì˜ ê°’ ìƒì„± ê¸ˆì§€**
- **ë¹ˆ í•„ë“œëŠ” ë¹ˆ ìƒíƒœë¡œ ë‘ê±°ë‚˜ ì›ë³¸ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”**

### 2. í•„ë“œë³„ ì¶œë ¥ ì›ì¹™
- **incident_id**: ì›ë³¸ ID ê·¸ëŒ€ë¡œ (ì˜ˆ: INM25011031275)
- **service_name**: ì›ë³¸ ì„œë¹„ìŠ¤ëª… ê·¸ëŒ€ë¡œ
- **symptom**: ì›ë³¸ ì¥ì• í˜„ìƒ ì „ì²´ ë‚´ìš© ê·¸ëŒ€ë¡œ
- **root_cause**: ì›ë³¸ ì¥ì• ì›ì¸ ì „ì²´ ë‚´ìš© ê·¸ëŒ€ë¡œ  
- **incident_repair**: ì›ë³¸ ë³µêµ¬ë°©ë²• ì „ì²´ ë‚´ìš© ê·¸ëŒ€ë¡œ
- **error_date**: ì›ë³¸ ë‚ ì§œ ê·¸ëŒ€ë¡œ (ì˜ˆ: 2025-01-10)
- **error_time**: ì›ë³¸ ì‹œê°„ ê·¸ëŒ€ë¡œ (ì˜ˆ: 94ë¶„)
- **incident_grade**: ì›ë³¸ ë“±ê¸‰ ê·¸ëŒ€ë¡œ (ì˜ˆ: 3ë“±ê¸‰)
- **owner_depart**: ì›ë³¸ ë¶€ì„œëª… ê·¸ëŒ€ë¡œ
- **daynight**: ì›ë³¸ ì‹œê°„ëŒ€ ê·¸ëŒ€ë¡œ (ì£¼ê°„/ì•¼ê°„)
- **week**: ì›ë³¸ ìš”ì¼ ê·¸ëŒ€ë¡œ

### 3. ê¸ˆì§€ ì‚¬í•­
- âŒ ë‚´ìš© ìš”ì•½ ê¸ˆì§€
- âŒ ì˜ì—­ ê¸ˆì§€  
- âŒ ìƒëµ ê¸ˆì§€
- âŒ ì„ì˜ ê°’ ìƒì„± ê¸ˆì§€
- âŒ "ì•½ XXë¶„", "ëŒ€ëµ XX" ë“±ì˜ í‘œí˜„ ê¸ˆì§€
- âŒ "ì£¼ìš” ë‚´ìš©:", "í•µì‹¬:", "ìš”ì•½:" ë“±ì˜ ì ‘ë‘ì‚¬ ê¸ˆì§€

### 4. í—ˆìš© ì‚¬í•­
- âœ… ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ë³µì‚¬
- âœ… êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì •ë¦¬ (ë‚´ìš© ë³€ê²½ ì—†ì´)
- âœ… í•„ë“œëª… ëª…ì‹œ (ê°’ì€ ì›ë³¸ ê·¸ëŒ€ë¡œ)

{SystemPrompts.get_prompt(query_type)}"""
        
        return base_prompt
    
    def _generate_statistics_response_with_integrity(self, query, documents):
        """ë°ì´í„° ë¬´ê²°ì„±ì„ ë³´ì¥í•˜ëŠ” í†µê³„ ì‘ë‹µ ìƒì„± - ì›ì¸ìœ í˜• ì²˜ë¦¬ ê°•í™”"""
        try:
            # âœ… 1. DB ìš°ì„  ì¡°íšŒ ì‹œë„ (lazy initialization)
            db_statistics = self.statistics_db_manager.get_statistics(query)
            
            if self.debug_mode and db_statistics.get('debug_info'):
                debug_info = db_statistics['debug_info']
                
                with st.expander("ğŸ” SQL ì¿¼ë¦¬ ë””ë²„ê·¸ ì •ë³´", expanded=False):
                    st.markdown("### ğŸ” íŒŒì‹±ëœ ì¡°ê±´")
                    st.json(debug_info['parsed_conditions'])
                    
                    st.markdown("### ğŸ’¾ ì‹¤í–‰ëœ SQL ì¿¼ë¦¬")
                    st.code(debug_info['sql_query'], language='sql')
                    
                    st.markdown("### ğŸ“¢ SQL íŒŒë¼ë¯¸í„°")
                    st.json(list(debug_info['sql_params']))
                    
                    st.markdown("### ğŸ“Š ì¿¼ë¦¬ ê²°ê³¼")
                    st.info(f"ì´ {debug_info['result_count']}ê°œì˜ ê²°ê³¼ ë°˜í™˜")
                    
                    if db_statistics.get('results'):
                        st.markdown("#### ê²°ê³¼ ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ)")
                        st.json(db_statistics['results'][:5])
            
            # 2. DBì—ì„œ í†µê³„ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            if (db_statistics and 
                db_statistics.get('results') and 
                (db_statistics.get('total_value', 0) > 0 or 
                db_statistics.get('cause_type_stats', {}) or
                db_statistics.get('yearly_stats', {}) or
                db_statistics.get('monthly_stats', {}))):
                
                return self._format_db_statistics_with_chart_support(db_statistics, query)
            else:
                # 3. DBì—ì„œ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¬¸ì„œ ê¸°ë°˜ í†µê³„ë¡œ fallback
                if self.debug_mode:
                    print("DB statistics returned no results, falling back to document-based statistics")
                return self._calculate_statistics_with_chart_support(documents, query)
                
        except Exception as e:
            print(f"ERROR: í†µê³„ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _calculate_statistics_with_chart_support(self, documents, query):
        """ë¬¸ì„œ ê¸°ë°˜ í†µê³„ ê³„ì‚° - ì°¨íŠ¸ í¬í•¨"""
        try:
            stats = self.statistics_calculator.calculate_comprehensive_statistics(query, documents, "statistics")
            
            if not stats or stats.get('total_count', 0) == 0:
                return "ì¡°ê±´ì— ë§ëŠ” ì¥ì•  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì°¨íŠ¸ ìƒì„± ë¡œì§
            chart_fig, chart_info = None, None
            requested_chart_type = self._extract_chart_type_from_query(query)
            
            chart_keywords = ['ì°¨íŠ¸', 'ê·¸ë˜í”„', 'ì‹œê°í™”', 'ê·¸ë ¤', 'ê·¸ë ¤ì¤˜', 'ë³´ì—¬ì¤˜', 'ì‹œê°ì ìœ¼ë¡œ', 'ë„í‘œ', 'ë„ì‹í™”']
            has_chart_request = any(keyword in query.lower() for keyword in chart_keywords)
            
            if has_chart_request or requested_chart_type:
                chart_data, chart_type = self._get_chart_data_from_stats(stats, requested_chart_type)
                
                if chart_data and len(chart_data) > 0:
                    try:
                        chart_title = self._generate_chart_title(query, stats)
                        chart_fig = self.chart_manager.create_chart(chart_type, chart_data, chart_title)
                        
                        if chart_fig:
                            chart_info = {
                                'chart': chart_fig,
                                'chart_type': chart_type,
                                'chart_data': chart_data,
                                'chart_title': chart_title,
                                'query': query,
                                'is_error_time_query': stats.get('is_error_time_query', False)
                            }
                            print(f"DEBUG: Chart created successfully - type: {chart_type}")
                    except Exception as e:
                        print(f"DEBUG: Chart creation failed: {e}")
                        chart_info = None
            
            # í†µê³„ ì‘ë‹µ ìƒì„±
            response_lines = []
            total_count = stats.get('total_count', 0)
            is_error_time = stats.get('is_error_time_query', False)
            value_type = "ì¥ì• ì‹œê°„(ë¶„)" if is_error_time else "ë°œìƒê±´ìˆ˜"
            
            response_lines.append(f"## ğŸ“Š í†µê³„ ìš”ì•½")
            response_lines.append(f"**ì´ {value_type}: {total_count}**")
            
            # ê°ì¢… í†µê³„ ì¶”ê°€ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            if stats.get('yearly_stats'):
                response_lines.append(f"\n## ğŸ“ˆ ì—°ë„ë³„ í†µê³„")
                for year, count in sorted(stats['yearly_stats'].items()):
                    response_lines.append(f"* **{year}: {count}ê±´**")
                response_lines.append(f"\n**ğŸ’¡ ì´ í•©ê³„: {sum(stats['yearly_stats'].values())}ê±´**")
            
            if stats.get('monthly_stats'):
                response_lines.append(f"\n## ğŸ“ˆ ì›”ë³„ í†µê³„")
                sorted_months = sorted(stats['monthly_stats'].items(), key=lambda x: int(x[0].replace('ì›”', '')))
                for month, count in sorted_months:
                    response_lines.append(f"* **{month}: {count}ê±´**")
                response_lines.append(f"\n**ğŸ’¡ ì´ í•©ê³„: {sum(stats['monthly_stats'].values())}ê±´**")
            
            # ê¸°íƒ€ í†µê³„ë“¤ë„ ë™ì¼í•˜ê²Œ ì¶”ê°€...
            
            final_answer = '\n'.join(response_lines)
            
            # ì°¨íŠ¸ì™€ í•¨ê»˜ ë°˜í™˜
            if chart_info:
                return (final_answer, chart_info)
            return final_answer
            
        except Exception as e:
            print(f"ERROR: ë¬¸ì„œ ê¸°ë°˜ í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return f"í†µê³„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _format_db_statistics_with_chart_support(self, db_stats, query):
        """DB í†µê³„ ê²°ê³¼ë¥¼ ì°¨íŠ¸ì™€ í•¨ê»˜ í¬ë§·íŒ… - ì›ì¸ìœ í˜• ì²˜ë¦¬ ê°•í™”"""
        try:
            conditions = db_stats['query_conditions']
            
            # ì°¨íŠ¸ ìƒì„± ë¡œì§
            chart_fig, chart_info = None, None
            requested_chart_type = self._extract_chart_type_from_query(query)
            
            chart_keywords = ['ì°¨íŠ¸', 'ê·¸ë˜í”„', 'ì‹œê°í™”', 'ê·¸ë ¤', 'ê·¸ë ¤ì¤˜', 'ë³´ì—¬ì¤˜', 'ì‹œê°ì ìœ¼ë¡œ', 'ë„í‘œ', 'ë„ì‹í™”']
            has_chart_request = any(keyword in query.lower() for keyword in chart_keywords)
            
            if has_chart_request or requested_chart_type:
                chart_data, chart_type = self._get_chart_data_from_db_stats(db_stats, requested_chart_type)
                
                if chart_data and len(chart_data) > 0:
                    try:
                        chart_title = self._generate_chart_title_from_db_stats(query, db_stats)
                        chart_fig = self.chart_manager.create_chart(chart_type, chart_data, chart_title)
                        
                        if chart_fig:
                            chart_info = {
                                'chart': chart_fig,
                                'chart_type': chart_type,
                                'chart_data': chart_data,
                                'chart_title': chart_title,
                                'query': query,
                                'is_error_time_query': db_stats['is_error_time_query']
                            }
                            print(f"DEBUG: Chart created successfully - type: {chart_type}")
                    except Exception as e:
                        print(f"DEBUG: Chart creation failed: {e}")
                        chart_info = None
            
            # í†µê³„ ì‘ë‹µ ìƒì„± (ì›ì¸ìœ í˜• íŠ¹ë³„ ì²˜ë¦¬)
            response_lines = []
            total_value = db_stats.get('total_value', 0)
            is_error_time = db_stats.get('is_error_time_query', False)
            is_cause_type_query = db_stats.get('is_cause_type_query', False)
            value_type = "ì¥ì• ì‹œê°„(ë¶„)" if is_error_time else "ë°œìƒê±´ìˆ˜"
            
            # ê¸°ë³¸ ìš”ì•½
            response_lines.append(f"## ğŸ“Š í†µê³„ ìš”ì•½")
            response_lines.append(f"**ì´ {value_type}: {total_value}**")
            
            # ì›ì¸ìœ í˜•ë³„ í†µê³„ (ìš°ì„  í‘œì‹œ)
            if is_cause_type_query and db_stats.get('cause_type_stats'):
                response_lines.append(f"\n## ğŸ” ì›ì¸ìœ í˜•ë³„ {value_type}")
                cause_stats = db_stats['cause_type_stats']
                
                for cause_type, count in cause_stats.items():
                    response_lines.append(f"* **{cause_type}: {count}ê±´**")
                
                response_lines.append(f"\n**ğŸ’¡ ì´ ì›ì¸ìœ í˜• ìˆ˜: {len(cause_stats)}ê°œ**")
                response_lines.append(f"**ğŸ’¡ ì´ í•©ê³„: {sum(cause_stats.values())}ê±´**")
            
            # ì—°ë„ë³„ í†µê³„
            if db_stats.get('yearly_stats'):
                response_lines.append(f"\n## ğŸ“ˆ ì—°ë„ë³„ í†µê³„")
                for year, count in sorted(db_stats['yearly_stats'].items()):
                    response_lines.append(f"* **{year}: {count}ê±´**")
                response_lines.append(f"\n**ğŸ’¡ ì´ í•©ê³„: {sum(db_stats['yearly_stats'].values())}ê±´**")
            
            # ì›”ë³„ í†µê³„
            if db_stats.get('monthly_stats'):
                response_lines.append(f"\n## ğŸ“ˆ ì›”ë³„ í†µê³„")
                sorted_months = sorted(db_stats['monthly_stats'].items(), key=lambda x: int(x[0].replace('ì›”', '')))
                for month, count in sorted_months:
                    response_lines.append(f"* **{month}: {count}ê±´**")
                response_lines.append(f"\n**ğŸ’¡ ì´ í•©ê³„: {sum(db_stats['monthly_stats'].values())}ê±´**")
            
            # ë“±ê¸‰ë³„ í†µê³„
            if db_stats.get('grade_stats'):
                response_lines.append(f"\n## âš ï¸ ì¥ì• ë“±ê¸‰ë³„ í†µê³„")
                grade_order = ['1ë“±ê¸‰', '2ë“±ê¸‰', '3ë“±ê¸‰', '4ë“±ê¸‰']
                grade_stats = db_stats['grade_stats']
                
                for grade in grade_order:
                    if grade in grade_stats:
                        response_lines.append(f"* **{grade}: {grade_stats[grade]}ê±´**")
                
                response_lines.append(f"\n**ğŸ’¡ ì´ í•©ê³„: {sum(grade_stats.values())}ê±´**")
            
            # ì„œë¹„ìŠ¤ë³„ í†µê³„ (ìƒìœ„ 10ê°œ)
            if db_stats.get('service_stats'):
                response_lines.append(f"\n## ğŸ’» ì„œë¹„ìŠ¤ë³„ í†µê³„ (ìƒìœ„ 10ê°œ)")
                sorted_services = sorted(db_stats['service_stats'].items(), key=lambda x: x[1], reverse=True)[:10]
                for service, count in sorted_services:
                    response_lines.append(f"* **{service}: {count}ê±´**")
                response_lines.append(f"\n**ğŸ’¡ ìƒìœ„ 10ê°œ í•©ê³„: {sum(count for _, count in sorted_services)}ê±´**")
            
            # ë¶€ì„œë³„ í†µê³„ (ìƒìœ„ 10ê°œ)
            if db_stats.get('department_stats'):
                response_lines.append(f"\n## ğŸ¢ ë¶€ì„œë³„ í†µê³„ (ìƒìœ„ 10ê°œ)")
                sorted_departments = sorted(db_stats['department_stats'].items(), key=lambda x: x[1], reverse=True)[:10]
                for dept, count in sorted_departments:
                    response_lines.append(f"* **{dept}: {count}ê±´**")
                response_lines.append(f"\n**ğŸ’¡ ìƒìœ„ 10ê°œ í•©ê³„: {sum(count for _, count in sorted_departments)}ê±´**")
            
            # ì‹œê°„ëŒ€ë³„ í†µê³„
            if db_stats.get('time_stats', {}).get('daynight'):
                response_lines.append(f"\n## ğŸ•˜ ì‹œê°„ëŒ€ë³„ í†µê³„")
                for time, count in db_stats['time_stats']['daynight'].items():
                    response_lines.append(f"* **{time}: {count}ê±´**")
                response_lines.append(f"\n**ğŸ’¡ ì´ í•©ê³„: {sum(db_stats['time_stats']['daynight'].values())}ê±´**")
            
            # ìš”ì¼ë³„ í†µê³„
            if db_stats.get('time_stats', {}).get('week'):
                response_lines.append(f"\n## ğŸ“… ìš”ì¼ë³„ í†µê³„")
                for day, count in db_stats['time_stats']['week'].items():
                    response_lines.append(f"* **{day}: {count}ê±´**")
                response_lines.append(f"\n**ğŸ’¡ ì´ í•©ê³„: {sum(db_stats['time_stats']['week'].values())}ê±´**")
            
            final_answer = '\n'.join(response_lines)
            
            # ì°¨íŠ¸ì™€ í•¨ê»˜ ë°˜í™˜
            if chart_info:
                return (final_answer, chart_info)
            return final_answer
            
        except Exception as e:
            return f"í†µê³„ í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜: {str(e)}"

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (ë‹¤ë¥¸ ê°œì„ ëœ ë²„ì „ì—ì„œ ê°€ì ¸ì˜¨ ë©”ì„œë“œë“¤ ì¶”ê°€)
    def check_and_transform_query_with_reprompting(self, user_query):
        """ê°œì„ ëœ ë¦¬í”„ë¡¬í”„íŒ… - ê°•ì œ ì¹˜í™˜ ì¶”ê°€"""
        if not user_query:
            return {'transformed': False, 'original_query': user_query, 'transformed_query': user_query, 'match_type': 'none'}
        
        force_replaced_query = self.force_replace_problematic_queries(user_query)
        
        try:
            if force_replaced_query != user_query:
                if not self.debug_mode:
                    st.success("âœ… ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
                return {
                    'transformed': True, 
                    'original_query': user_query, 
                    'transformed_query': force_replaced_query, 
                    'question_type': 'statistics',
                    'wrong_answer_summary': 'ë™ì˜ì–´ í‘œí˜„ ìµœì í™”',
                    'match_type': 'force_replacement'
                }
            
            exact_result = self.reprompting_db_manager.check_reprompting_question(user_query)
            if exact_result['exists']:
                if not self.debug_mode:
                    st.success("âœ… ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
                return {
                    'transformed': True, 
                    'original_query': user_query, 
                    'transformed_query': exact_result['custom_prompt'], 
                    'question_type': exact_result['question_type'], 
                    'wrong_answer_summary': exact_result['wrong_answer_summary'], 
                    'match_type': 'exact'
                }
            
            similar_questions = self.reprompting_db_manager.find_similar_questions(user_query, similarity_threshold=0.7, limit=3)
            if similar_questions:
                best_match = similar_questions[0]
                try:
                    transformed_query = re.sub(re.escape(best_match['question']), best_match['custom_prompt'], user_query, flags=re.IGNORECASE)
                except:
                    transformed_query = user_query.replace(best_match['question'], best_match['custom_prompt'])
                
                is_transformed = transformed_query != user_query
                if is_transformed and not self.debug_mode:
                    st.info("ğŸ“‹ ìœ ì‚¬ ì§ˆë¬¸ íŒ¨í„´ì„ ê°ì§€í•˜ì—¬ ì§ˆë¬¸ì„ ìµœì í™”í–ˆìŠµë‹ˆë‹¤.")
                return {
                    'transformed': is_transformed, 
                    'original_query': user_query, 
                    'transformed_query': transformed_query, 
                    'question_type': best_match['question_type'], 
                    'wrong_answer_summary': best_match['wrong_answer_summary'], 
                    'similarity': best_match['similarity'], 
                    'similar_question': best_match['question'], 
                    'match_type': 'similar'
                }
            
            return {'transformed': False, 'original_query': user_query, 'transformed_query': user_query, 'match_type': 'none'}
            
        except Exception as e:
            return {'transformed': False, 'original_query': user_query, 'transformed_query': user_query, 'match_type': 'error', 'error': str(e)}
    
    def extract_time_conditions(self, query):
        if not query:
            return {'daynight': None, 'week': None, 'is_time_query': False}
        
        time_conditions = {'daynight': None, 'week': None, 'is_time_query': False}
        query_lower = query.lower()
        
        if any(keyword in query_lower for keyword in ['ì•¼ê°„', 'ë°¤', 'ìƒˆë²½', 'ì‹¬ì•¼']):
            time_conditions.update({'is_time_query': True, 'daynight': 'ì•¼ê°„'})
        elif any(keyword in query_lower for keyword in ['ì£¼ê°„', 'ë‚®', 'ì˜¤ì „', 'ì˜¤í›„']):
            time_conditions.update({'is_time_query': True, 'daynight': 'ì£¼ê°„'})
        
        week_map = {'ì›”ìš”ì¼': 'ì›”', 'í™”ìš”ì¼': 'í™”', 'ìˆ˜ìš”ì¼': 'ìˆ˜', 'ëª©ìš”ì¼': 'ëª©', 'ê¸ˆìš”ì¼': 'ê¸ˆ', 'í† ìš”ì¼': 'í† ', 'ì¼ìš”ì¼': 'ì¼', 'í‰ì¼': 'í‰ì¼', 'ì£¼ë§': 'ì£¼ë§'}
        for keyword, value in week_map.items():
            if keyword in query_lower:
                time_conditions.update({'is_time_query': True, 'week': value})
                break
        
        return time_conditions
    
    def extract_department_conditions(self, query):
        if not query:
            return {'owner_depart': None, 'is_department_query': False}
        return {'owner_depart': None, 'is_department_query': any(keyword in query for keyword in ['ë‹´ë‹¹ë¶€ì„œ', 'ì¡°ì¹˜ë¶€ì„œ', 'ì²˜ë¦¬ë¶€ì„œ', 'ì±…ì„ë¶€ì„œ', 'ê´€ë¦¬ë¶€ì„œ', 'ë¶€ì„œ', 'íŒ€', 'ì¡°ì§'])}

    def classify_query_type_with_llm(self, query):
        """ê°œì„ ëœ LLM ê¸°ë°˜ ì˜ë¯¸ì  ì¿¼ë¦¬ ë¶„ë¥˜ - í†µê³„ í‚¤ì›Œë“œ ì¸ì‹ ê°•í™”"""
        if not query:
            return 'default'
        
        print(f"DEBUG: Starting enhanced semantic query classification for: '{query}'")
        
        # 1ë‹¨ê³„: ì‚¬ì „ í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ (ê°•í™”)
        pre_classification = self._pre_classify_by_keywords(query)
        if pre_classification != 'unknown':
            print(f"DEBUG: Pre-classified as '{pre_classification}' by keyword matching")
            return pre_classification
        
        try:
            # ê°œì„ ëœ ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸
            classification_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì˜ë¯¸ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì •í™•íˆ ë¶„ë¥˜í•˜ì„¸ìš”.

**ğŸš¨ í•µì‹¬ ì›ì¹™: ì¦ìƒ/ì¡°ê±´ì€ ë¬´ì‹œí•˜ê³  ì‚¬ìš©ìì˜ ì˜ë„ë§Œ íŒŒì•…í•˜ì„¸ìš”**

**ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬:**

1. **inquiry**: ì¥ì•  ë‚´ì—­/ëª©ë¡/ë¦¬ìŠ¤íŠ¸ë¥¼ ë³´ê³  ì‹¶ì€ ì˜ë„
   - ì˜ë„ í‚¤ì›Œë“œ: "ë‚´ì—­", "ëª©ë¡", "ë¦¬ìŠ¤íŠ¸", "ì´ë ¥", "ì¡°íšŒ" + "ì•Œë ¤ì¤˜/ë³´ì—¬ì¤˜/ë§í•´ì¤˜"
   - âœ… ì˜ˆì‹œ: "ì ‘ì†ë¶ˆê°€ ì¥ì• ë‚´ì—­ ì•Œë ¤ì¤˜" â†’ inquiry (ì ‘ì†ë¶ˆê°€ëŠ” ì¡°ê±´ì¼ë¿)
   - âœ… ì˜ˆì‹œ: "ë¡œê·¸ì¸ ì‹¤íŒ¨ ëª©ë¡ ë³´ì—¬ì¤˜" â†’ inquiry (ë¡œê·¸ì¸ ì‹¤íŒ¨ëŠ” ì¡°ê±´ì¼ë¿)
   - âœ… ì˜ˆì‹œ: "ERP ì¥ì• ë‚´ì—­" â†’ inquiry
   - âœ… ì˜ˆì‹œ: "ì•¼ê°„ ë°œìƒí•œ ë‚´ì—­" â†’ inquiry

2. **repair**: ë³µêµ¬ë°©ë²•, í•´ê²°ë°©ë²•, ì›ì¸ íŒŒì•…ì„ ì›í•˜ëŠ” ì˜ë„
   - ì˜ë„ í‚¤ì›Œë“œ: "ë³µêµ¬ë°©ë²•", "í•´ê²°ë°©ë²•", "ì¡°ì¹˜ë°©ë²•", "ì›ì¸", "ì™œ", "ì–´ë–»ê²Œ"
   - âœ… ì˜ˆì‹œ: "ì ‘ì†ë¶ˆê°€ ë³µêµ¬ë°©ë²•" â†’ repair
   - âœ… ì˜ˆì‹œ: "ë¡œê·¸ì¸ ì‹¤íŒ¨ ì–´ë–»ê²Œ í•´ê²°" â†’ repair
   - âœ… ì˜ˆì‹œ: "ì¥ì•  ì›ì¸ ë¶„ì„" â†’ repair

3. **statistics**: ê±´ìˆ˜, í†µê³„, ìˆ˜ì¹˜ ì •ë³´ë¥¼ ì›í•˜ëŠ” ì˜ë„
   - ì˜ë„ í‚¤ì›Œë“œ: "ê±´ìˆ˜", "ëª‡ê±´", "ëª‡ê°œ", "ì–¼ë§ˆë‚˜", "í†µê³„", "ì—°ë„ë³„", "ì›”ë³„"
   - âœ… ì˜ˆì‹œ: "ì ‘ì†ë¶ˆê°€ ëª‡ê±´" â†’ statistics (ì ‘ì†ë¶ˆê°€ëŠ” ì¡°ê±´ì¼ë¿)
   - âœ… ì˜ˆì‹œ: "ERP ì¥ì• ê±´ìˆ˜" â†’ statistics
   - âœ… ì˜ˆì‹œ: "ì—°ë„ë³„ í†µê³„" â†’ statistics

4. **default**: ìœ„ ì„¸ ê°€ì§€ ì˜ë„ê°€ ì—†ëŠ” ì¼ë°˜ ì§ˆë¬¸

**ğŸš¨ ë¶„ë¥˜ ê·œì¹™:**
- "ì ‘ì†ë¶ˆê°€", "ë¡œê·¸ì¸ ì‹¤íŒ¨", "ì˜¤ë¥˜ ë°œìƒ" ë“±ì€ **ì¡°ê±´/ì¦ìƒ**ì´ë¯€ë¡œ ë¶„ë¥˜ì— ì˜í–¥ ì—†ìŒ
- ì˜¤ì§ **ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²°ê³¼ë¬¼ì˜ í˜•íƒœ**ë¡œë§Œ íŒë‹¨:
  - ë‚´ì—­/ëª©ë¡ì„ ì›í•¨ â†’ inquiry
  - í•´ê²°ì±…/ì›ì¸ì„ ì›í•¨ â†’ repair
  - ìˆ«ì/í†µê³„ë¥¼ ì›í•¨ â†’ statistics

**ì‚¬ìš©ì ì§ˆë¬¸:** {query}

**ì‘ë‹µ í˜•ì‹:** repair, inquiry, statistics, default ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

            response = self.azure_openai_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ IT ì§ˆë¬¸ì„ ì˜ë¯¸ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì •í™•íˆ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŠ¹íˆ í†µê³„ ê´€ë ¨ ì§ˆë¬¸ì„ ë†“ì¹˜ì§€ ì•Šê³  ì •í™•íˆ ì¸ì‹í•´ì•¼ í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": classification_prompt}
                ],
                temperature=0.0,
                max_tokens=50
            )
            
            query_type = response.choices[0].message.content.strip().lower()
            
            if query_type not in ['repair', 'inquiry', 'statistics', 'default']:
                print(f"WARNING: Invalid query type '{query_type}', applying fallback classification")
                query_type = self._enhanced_fallback_classification(query)
            
            print(f"DEBUG: LLM semantic classification result: {query_type}")
            
            confidence_score = self._calculate_enhanced_classification_confidence(query, query_type)
            print(f"DEBUG: Classification confidence: {confidence_score:.2f}")
            
            # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ fallback ì‚¬ìš©
            if confidence_score < 0.6:
                fallback_type = self._enhanced_fallback_classification(query)
                print(f"DEBUG: Low confidence, using fallback: {fallback_type}")
                return fallback_type
            
            return query_type
                
        except Exception as e:
            print(f"ERROR: LLM semantic classification failed: {e}")
            return self._enhanced_fallback_classification(query)

    def _pre_classify_by_keywords(self, query):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì‚¬ì „ ë¶„ë¥˜ - ì˜ë„ë§Œ íŒŒì•…, ì¦ìƒì€ ë¬´ì‹œ"""
        if not query:
            return 'unknown'
        
        query_lower = query.lower()
        
        # ğŸ”¥ 1ìˆœìœ„: inquiry ëª…ì‹œì  ì˜ë„ ì²´í¬
        explicit_inquiry_patterns = [
            r'(?:ë‚´ì—­|ëª©ë¡|ë¦¬ìŠ¤íŠ¸|ì´ë ¥).*(?:ì•Œë ¤ì¤˜|ë³´ì—¬ì¤˜|ë§í•´ì¤˜|í™•ì¸|ì²´í¬|ì¡°íšŒ)',
            r'(?:ì•Œë ¤ì¤˜|ë³´ì—¬ì¤˜|ë§í•´ì¤˜).*(?:ë‚´ì—­|ëª©ë¡|ë¦¬ìŠ¤íŠ¸|ì´ë ¥)',
        ]
        
        for pattern in explicit_inquiry_patterns:
            if re.search(pattern, query_lower):
                print(f"DEBUG: âœ… Explicit inquiry intent detected: {pattern}")
                return 'inquiry'
        
        # ğŸ”¥ 2ìˆœìœ„: ê°•ë ¥í•œ í†µê³„ ì‹ í˜¸ ì²´í¬
        strong_stats_patterns = [
            r'\b[A-Zê°€-í£][A-Z0-9ê°€-í£\s]{1,20}\s+(?:ì—°ë„ë³„|ì›”ë³„|ë…„ë„ë³„|ë…„ë³„)\s*(?:ì¥ì• )?\s*ê±´ìˆ˜',
            r'\b(?:ëª‡ê±´|ëª‡ê°œ|ì–¼ë§ˆë‚˜|ì–´ëŠì •ë„|ëª‡ë²ˆ|ëª‡ì°¨ë¡€)\b',
            r'(?:ê±´ìˆ˜|ê°œìˆ˜|ìˆ˜ëŸ‰|í†µê³„|í˜„í™©|ë¶„í¬)\s*(?:ì•Œë ¤|ë³´ì—¬|ë§í•´|í™•ì¸|ì²´í¬|ì¡°íšŒ)',
            r'\b(?:ì—°ë„ë³„|ë…„ë„ë³„|ì›”ë³„|ë“±ê¸‰ë³„|ë¶€ì„œë³„|ì„œë¹„ìŠ¤ë³„)\s+\w+',
            r'\b[A-Z]{2,10}\s+(?:ì—°ë„ë³„|ì›”ë³„|ë…„ë„ë³„|í†µê³„|í˜„í™©|ê±´ìˆ˜)'
        ]
        
        for pattern in strong_stats_patterns:
            if re.search(pattern, query_lower):
                print(f"DEBUG: Strong statistics pattern detected: {pattern}")
                return 'statistics'
        
        # ğŸ”¥ 3ìˆœìœ„: ë³µí•© í‚¤ì›Œë“œ ì ìˆ˜ - ì¦ìƒì€ ì œì™¸, ì˜ë„ë§Œ í‰ê°€
        stats_score = 0
        repair_score = 0
        inquiry_score = 0
        
        # í†µê³„ ì˜ë„ í‚¤ì›Œë“œ
        for category, keywords in self.statistics_keywords.items():
            for keyword in keywords:
                if keyword in query_lower:
                    if category == 'basic_stats':
                        stats_score += 3
                    elif category == 'time_keywords':
                        stats_score += 2
                    else:
                        stats_score += 1
        
        # ğŸ”¥ FIXED: repairëŠ” í–‰ë™ ìš”ì²­ë§Œ (ì¦ìƒ í‚¤ì›Œë“œ ì œê±°)
        repair_intent_keywords = ['ë³µêµ¬ë°©ë²•', 'í•´ê²°ë°©ë²•', 'ì¡°ì¹˜ë°©ë²•', 'ì›ì¸', 'ì™œ', 'ì–´ë–»ê²Œ', 'í•´ê²°', 'ì¡°ì¹˜', 'ëŒ€ì‘']
        for keyword in repair_intent_keywords:
            if keyword in query_lower:
                repair_score += 3  # ëª…í™•í•œ ì˜ë„ì´ë¯€ë¡œ ë†’ì€ ì ìˆ˜
        
        # ğŸ”¥ inquiry ì˜ë„ í‚¤ì›Œë“œ
        inquiry_intent_keywords = ['ë‚´ì—­', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'ì¡°íšŒ', 'ì´ë ¥', 'ê±´ë“¤', 'ì‚¬ë¡€ë“¤', 'ë°œìƒí•œê²ƒ', 'ìˆëŠ”ì§€']
        for keyword in inquiry_intent_keywords:
            if keyword in query_lower:
                inquiry_score += 3
        
        print(f"DEBUG: Intent scores - inquiry: {inquiry_score}, stats: {stats_score}, repair: {repair_score}")
        
        # ğŸ”¥ CHANGED: ìš°ì„ ìˆœìœ„ ëª…í™•í™”
        if inquiry_score >= 3:
            return 'inquiry'
        elif stats_score >= 3:
            return 'statistics'
        elif repair_score >= 3:
            return 'repair'
        
        return 'unknown'

    def _enhanced_fallback_classification(self, query):
        """ê°•í™”ëœ fallback ë¶„ë¥˜ ë¡œì§ - ì˜ë„ ì¤‘ì‹¬"""
        if not query:
            return 'default'
        
        query_lower = query.lower()
        
        # 1ìˆœìœ„: inquiry ì˜ë„
        inquiry_intent = ['ë‚´ì—­', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'ì´ë ¥', 'ì¡°íšŒ']
        inquiry_verbs = ['ì•Œë ¤ì¤˜', 'ë³´ì—¬ì¤˜', 'ë§í•´ì¤˜', 'í™•ì¸', 'ì²´í¬']
        
        has_inquiry_noun = any(word in query_lower for word in inquiry_intent)
        has_inquiry_verb = any(word in query_lower for word in inquiry_verbs)
        
        if has_inquiry_noun and has_inquiry_verb:
            return 'inquiry'
        
        # 2ìˆœìœ„: í†µê³„ ì˜ë„
        if any(keyword in query_lower for keyword in ['ì—°ë„ë³„', 'ë…„ë„ë³„', 'ì›”ë³„', 'ë“±ê¸‰ë³„']):
            return 'statistics'
        
        if any(keyword in query_lower for keyword in ['ëª‡ê±´', 'ëª‡ê°œ', 'ì–¼ë§ˆë‚˜', 'ê±´ìˆ˜', 'ê°œìˆ˜']):
            return 'statistics'
        
        # 3ìˆœìœ„: repair ì˜ë„ (í–‰ë™ ìš”ì²­ë§Œ)
        if any(word in query_lower for word in ['ë³µêµ¬ë°©ë²•', 'í•´ê²°ë°©ë²•', 'ì¡°ì¹˜ë°©ë²•', 'ì›ì¸', 'ì™œ', 'ì–´ë–»ê²Œ']):
            return 'repair'
        
        # 4ìˆœìœ„: ê¸°ë³¸ê°’
        return 'default'

    def _calculate_enhanced_classification_confidence(self, query, predicted_type):
        """ê°•í™”ëœ ë¶„ë¥˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            query_lower = query.lower()
            
            # ê° íƒ€ì…ë³„ ê°•ë ¥í•œ ì‹ í˜¸
            strong_signals = {
                'statistics': [
                    'ëª‡ê±´', 'ëª‡ê°œ', 'ì–¼ë§ˆë‚˜', 'ê±´ìˆ˜', 'ê°œìˆ˜', 'í†µê³„', 'í˜„í™©', 'ë¶„í¬',
                    'ì—°ë„ë³„', 'ì›”ë³„', 'ë“±ê¸‰ë³„', 'ë¶€ì„œë³„', 'ì„œë¹„ìŠ¤ë³„'
                ],
                'repair': ['ë³µêµ¬ë°©ë²•', 'í•´ê²°ë°©ë²•', 'ì¡°ì¹˜ë°©ë²•', 'ë¶ˆê°€', 'ì‹¤íŒ¨', 'ì›ì¸', 'ì™œ'],
                'inquiry': ['ë‚´ì—­', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 'ì¡°íšŒ', 'ë³´ì—¬ì¤˜'],
                'default': []
            }
            
            predicted_signals = strong_signals.get(predicted_type, [])
            signal_count = sum(1 for signal in predicted_signals if signal in query_lower)
            
            # ì¶©ëŒí•˜ëŠ” ì‹ í˜¸ ì²´í¬
            conflicting_signals = 0
            for other_type, signals in strong_signals.items():
                if other_type != predicted_type:
                    conflicting_signals += sum(1 for signal in signals if signal in query_lower)
            
            # ê¸°ë³¸ ì‹ ë¢°ë„
            confidence = 0.5
            
            # ì‹ í˜¸ ê°•ë„ì— ë”°ë¥¸ ê°€ì 
            if signal_count > 0:
                confidence += 0.4 * min(signal_count, 3) / 3
            
            # ì¶©ëŒ ì‹ í˜¸ì— ë”°ë¥¸ ê°ì 
            if conflicting_signals > 0:
                confidence -= 0.3 * min(conflicting_signals, 2) / 2
            
            # í†µê³„ ì¿¼ë¦¬ì˜ íŠ¹ë³„ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ ë¶€ì—¬)
            if predicted_type == 'statistics':
                # í†µê³„ ê´€ë ¨ ê°•ë ¥í•œ íŒ¨í„´ì´ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ì¦ê°€
                stats_patterns = [
                    r'\b[A-Z]{2,10}\s+(?:ì—°ë„ë³„|ì›”ë³„|í†µê³„|í˜„í™©|ê±´ìˆ˜)',
                    r'(?:ëª‡ê±´|ëª‡ê°œ|ì–¼ë§ˆë‚˜|ê±´ìˆ˜)\b',
                    r'(?:ì—°ë„ë³„|ì›”ë³„|ë“±ê¸‰ë³„)\s+\w+'
                ]
                
                for pattern in stats_patterns:
                    if re.search(pattern, query_lower):
                        confidence += 0.2
                        break
            
            return max(0.0, min(1.0, confidence))
            
        except Exception:
            return 0.5

    def _extract_chart_type_from_query(self, query):
        """ì¿¼ë¦¬ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­ëœ ì°¨íŠ¸ íƒ€ì… ì¶”ì¶œ"""
        if not query:
            return None
        
        query_lower = query.lower()
        
        chart_type_keywords = {
            'horizontal_bar': ['ê°€ë¡œë§‰ëŒ€', 'ê°€ë¡œ ë§‰ëŒ€', 'ê°€ë¡œë§‰ëŒ€ì°¨íŠ¸', 'ê°€ë¡œ ë§‰ëŒ€ ì°¨íŠ¸', 'horizontal bar', 'barh', 'ê°€ë¡œë°”', 'ê°€ë¡œ ë°”', 'ê°€ë¡œí˜• ë§‰ëŒ€', 'ê°€ë¡œí˜•'],
            'bar': ['ì„¸ë¡œë§‰ëŒ€', 'ì„¸ë¡œ ë§‰ëŒ€', 'ì„¸ë¡œë§‰ëŒ€ì°¨íŠ¸', 'ì„¸ë¡œ ë§‰ëŒ€ ì°¨íŠ¸', 'ë§‰ëŒ€ì°¨íŠ¸', 'ë§‰ëŒ€ ì°¨íŠ¸', 'bar chart', 'vertical bar', 'ë°”ì°¨íŠ¸', 'ë°” ì°¨íŠ¸', 'ì„¸ë¡œí˜•'],
            'line': ['ì„ ì°¨íŠ¸', 'ì„  ì°¨íŠ¸', 'ì„ ê·¸ë˜í”„', 'ì„  ê·¸ë˜í”„', 'ë¼ì¸ì°¨íŠ¸', 'ë¼ì¸ ì°¨íŠ¸', 'line chart', 'line graph', 'êº¾ì€ì„ ', 'êº¾ì€ì„ ê·¸ë˜í”„', 'ì¶”ì´', 'íŠ¸ë Œë“œ'],
            'pie': ['íŒŒì´ì°¨íŠ¸', 'íŒŒì´ ì°¨íŠ¸', 'ì›í˜•ì°¨íŠ¸', 'ì›í˜• ì°¨íŠ¸', 'ì›ê·¸ë˜í”„', 'pie chart', 'íŒŒì´ê·¸ë˜í”„', 'ë¹„ìœ¨ì°¨íŠ¸', 'ë¹„ìœ¨ ì°¨íŠ¸', 'ì›í˜•']
        }
        
        for chart_type, keywords in chart_type_keywords.items():
            for keyword in keywords:
                if keyword in query_lower:
                    print(f"DEBUG: Detected chart type '{chart_type}' from keyword '{keyword}'")
                    return chart_type
        
        return None

    def _generate_chart_title(self, query, stats):
        primary_type = stats.get('primary_stat_type', 'general')
        title_map = {'yearly': 'ì—°ë„ë³„ ì¥ì•  ë°œìƒ í˜„í™©', 'monthly': 'ì›”ë³„ ì¥ì•  ë°œìƒ í˜„í™©', 'time': 'ì‹œê°„ëŒ€ë³„ ì¥ì•  ë°œìƒ ë¶„í¬', 'weekday': 'ìš”ì¼ë³„ ì¥ì•  ë°œìƒ ë¶„í¬', 'department': 'ë¶€ì„œë³„ ì¥ì•  ì²˜ë¦¬ í˜„í™©', 'service': 'ì„œë¹„ìŠ¤ë³„ ì¥ì•  ë°œìƒ í˜„í™©', 'grade': 'ì¥ì• ë“±ê¸‰ë³„ ë°œìƒ ë¹„ìœ¨', 'general': 'ì¥ì•  ë°œìƒ í†µê³„'}
        base_title = title_map.get(primary_type, 'ì¥ì•  í†µê³„')
        
        if stats.get('is_error_time_query'):
            base_title = base_title.replace('ë°œìƒ', 'ì‹œê°„').replace('ê±´ìˆ˜', 'ì‹œê°„')
        
        if query:
            year_match = re.search(r'\b(202[0-9])\b', query)
            if year_match:
                base_title = f"{year_match.group(1)}ë…„ {base_title}"
        return base_title

    def _get_chart_data_from_stats(self, stats, requested_chart_type=None):
        """í†µê³„ì—ì„œ ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ"""
        primary_type = stats.get('primary_stat_type')
        if not primary_type:
            return None, None
        
        data_map = {
            'yearly': (stats.get('yearly_stats', {}), 'line'),
            'monthly': (stats.get('monthly_stats', {}), 'line'),
            'time': (stats.get('time_stats', {}).get('daynight', {}) or stats.get('time_stats', {}).get('week', {}), 'bar'),
            'weekday': (stats.get('time_stats', {}).get('week', {}), 'bar'),
            'department': (dict(sorted(stats.get('department_stats', {}).items(), key=lambda x: x[1], reverse=True)[:10]), 'horizontal_bar'),
            'service': (dict(sorted(stats.get('service_stats', {}).items(), key=lambda x: x[1], reverse=True)[:10]), 'horizontal_bar'),
            'grade': (stats.get('grade_stats', {}), 'pie')
        }
        
        data, default_chart_type = data_map.get(primary_type, (stats.get('yearly_stats', {}), 'line'))
        
        chart_type = requested_chart_type or default_chart_type
        if default_chart_type == 'line' and len(data) == 1:
            chart_type = 'bar'
        
        print(f"DEBUG: Using {'user-requested' if requested_chart_type else 'default'} chart type: {chart_type}")
        
        return data, chart_type

    def _extract_incident_id_sort_key(self, incident_id):
        """Incident ID ì •ë ¬ í‚¤ ì¶”ì¶œ"""
        if not incident_id:
            return 99999999999999
        try:
            return int(incident_id[3:]) if incident_id.startswith('INM') and len(incident_id) > 3 else hash(incident_id) % 999999999999999
        except (ValueError, TypeError):
            return hash(str(incident_id)) % 99999999999999

    def _apply_default_sorting(self, documents):
        """ê¸°ë³¸ ì •ë ¬ ì ìš©"""
        if not documents:
            return documents
        try:
            def default_sort_key(doc):
                error_date = doc.get('error_date', '1900-01-01') or '1900-01-01'
                try:
                    error_time_val = int(doc.get('error_time', 0) or 0)
                except (ValueError, TypeError):
                    error_time_val = 0
                return (error_date, error_time_val, -self._extract_incident_id_sort_key(doc.get('incident_id', 'INM99999999999')))
            documents.sort(key=default_sort_key, reverse=True)
        except Exception:
            pass
        return documents

    def detect_sorting_requirements(self, query):
        """ì •ë ¬ ìš”êµ¬ì‚¬í•­ ê°ì§€"""
        sort_info = {'requires_custom_sort': False, 'sort_field': None, 'sort_direction': 'desc', 'sort_type': None, 'limit': None, 'secondary_sort': 'default'}
        if not query:
            return sort_info
        
        query_lower = query.lower()
        
        # ì¥ì• ì‹œê°„ ê´€ë ¨ ì •ë ¬ íŒ¨í„´
        error_time_patterns = [r'ì¥ì• ì‹œê°„.*(?:ê°€ì¥.*?ê¸´|ê¸´.*?ìˆœ|ì˜¤ë˜.*?ê±¸ë¦°|ìµœëŒ€|í°.*?ìˆœ)', r'(?:ìµœì¥|ìµœëŒ€|ê°€ì¥.*?ì˜¤ë˜).*ì¥ì• ', r'top.*\d+.*ì¥ì• ì‹œê°„']
        for pattern in error_time_patterns:
            if re.search(pattern, query_lower):
                sort_info.update({'requires_custom_sort': True, 'sort_field': 'error_time', 'sort_type': 'error_time', 'sort_direction': 'desc'})
                break
        
        top_match = re.search(r'top\s*(\d+)|ìƒìœ„\s*(\d+)', query_lower)
        if top_match:
            sort_info['limit'] = min(int(top_match.group(1) or top_match.group(2)), 50)
            if not sort_info['requires_custom_sort']:
                sort_info.update({'requires_custom_sort': True, 'sort_field': 'error_time', 'sort_type': 'error_time'})
        return sort_info

    def apply_custom_sorting(self, documents, sort_info):
        """ì»¤ìŠ¤í…€ ì •ë ¬ ì ìš©"""
        if not documents:
            return documents
        try:
            if sort_info['requires_custom_sort'] and sort_info['sort_type'] == 'error_time':
                def error_time_sort_key(doc):
                    try:
                        error_time_val = int(doc.get('error_time', 0) or 0)
                    except (ValueError, TypeError):
                        error_time_val = 0
                    error_date = doc.get('error_date', '1900-01-01')
                    incident_sort_key = self._extract_incident_id_sort_key(doc.get('incident_id', 'INM99999999999'))
                    return (-error_time_val, error_date, incident_sort_key) if sort_info['sort_direction'] == 'desc' else (error_time_val, error_date, incident_sort_key)
                documents.sort(key=error_time_sort_key)
                if sort_info['limit']:
                    documents = documents[:sort_info['limit']]
            else:
                self._apply_default_sorting(documents)
            return documents
        except Exception:
            return self._apply_default_sorting(documents)

    def _display_response_with_marker_conversion(self, response, chart_info=None, query_type="default"):
        """UI ì»´í¬ë„ŒíŠ¸ì— ëª¨ë“  ì²˜ë¦¬ë¥¼ ìœ„ì„í•˜ëŠ” ë‹¨ìˆœí™”ëœ ë²„ì „"""
        if not response:
            st.write("ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        response_text, chart_info = response if isinstance(response, tuple) else (response, chart_info)
        
        print(f"PROCESSOR_DEBUG: Query type ì „ë‹¬: {query_type}")
        print(f"PROCESSOR_DEBUG: Response ê¸¸ì´: {len(response_text)}")
        print(f"PROCESSOR_DEBUG: REPAIR_BOX í¬í•¨ ì—¬ë¶€: {'[REPAIR_BOX_START]' in response_text}")
        
        self.ui_components.display_response_with_query_type_awareness(
            response, 
            query_type=query_type, 
            chart_info=chart_info
        )

    def process_query(self, query, query_type=None):
        """ğŸš¨ ë©”ì¸ ì¿¼ë¦¬ ì²˜ë¦¬ - RAG ë°ì´í„° ë¬´ê²°ì„± ì ˆëŒ€ ë³´ì¥"""
        if not query:
            st.error("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        if not hasattr(st.session_state, 'current_query_logged'):
            st.session_state.current_query_logged = False
        st.session_state.current_query_logged = False
        
        start_time = time.time()
        response_text = None
        document_count = 0
        error_message = None
        success = False
        
        with st.chat_message("assistant"):
            try:
                original_query = query
                force_replaced_query = self.force_replace_problematic_queries(query)
                
                if force_replaced_query != original_query:
                    if self.debug_mode:
                        st.info(f"ğŸ”„ ì¿¼ë¦¬ ê°•ì œ ì¹˜í™˜: '{original_query}' â†’ '{force_replaced_query}'")
                    query = force_replaced_query
                
                reprompting_info = self.check_and_transform_query_with_reprompting(query)
                processing_query = reprompting_info.get('transformed_query', query)
                
                time_conditions = self.extract_time_conditions(processing_query)
                department_conditions = self.extract_department_conditions(processing_query)
                
                if query_type is None:
                    with st.spinner("ğŸ” ì§ˆë¬¸ ë¶„ì„ ì¤‘..."):
                        query_type = self.classify_query_type_with_llm(processing_query)
                
                if self.debug_mode and query_type.lower() == 'inquiry':
                    st.info("ğŸ“‹ ì¥ì•  ë‚´ì—­ ì¡°íšŒ ëª¨ë“œë¡œ ë¶„ê¸°ë˜ì—ˆìŠµë‹ˆë‹¤. ë³µêµ¬ë°©ë²• ë°•ìŠ¤ ì—†ì´ ê¹”ë”í•œ ëª©ë¡ì„ ì œê³µí•©ë‹ˆë‹¤.")
                
                target_service_name = self.search_manager.extract_service_name_from_query(processing_query)
                
                with st.spinner("ğŸ“„ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
                    documents = self.search_manager.semantic_search_with_adaptive_filtering(processing_query, target_service_name, query_type) or []
                    document_count = len(documents)
                    
                    if documents:
                        with st.expander("ğŸ“„ ë§¤ì¹­ëœ ë¬¸ì„œ ìƒì„¸ ë³´ê¸°"):
                            self.ui_components.display_documents_with_quality_info(documents)
                        
                        with st.spinner("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘..."):
                            # ğŸš¨ ë¬´ê²°ì„± ë³´ì¥ ì‘ë‹µ ìƒì„± ë©”ì„œë“œë§Œ ì‚¬ìš©
                            response = self.generate_rag_response_with_data_integrity(
                                query, documents, query_type, time_conditions, department_conditions, reprompting_info
                            )
                            
                            if response:
                                response_text = response[0] if isinstance(response, tuple) else response
                                
                                success = self._is_successful_response(response_text, document_count)
                                if not success:
                                    error_message = self._get_failure_reason(response_text, document_count)
                                
                                self._display_response_with_marker_conversion(response, query_type=query_type)
                                st.session_state.messages.append({"role": "assistant", "content": response_text})
                            else:
                                response_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                success = False
                                error_message = "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                                st.write(response_text)
                                st.session_state.messages.append({"role": "assistant", "content": response_text})
                    else:
                        with st.spinner("ğŸ“„ ì¶”ê°€ ê²€ìƒ‰ ì¤‘..."):
                            fallback_documents = self.search_manager.search_documents_fallback(processing_query, target_service_name)
                            document_count = len(fallback_documents)
                            
                            if fallback_documents:
                                # ğŸš¨ ë¬´ê²°ì„± ë³´ì¥ ì‘ë‹µ ìƒì„± ë©”ì„œë“œë§Œ ì‚¬ìš©
                                response = self.generate_rag_response_with_data_integrity(
                                    query, fallback_documents, query_type, time_conditions, department_conditions, reprompting_info
                                )
                                response_text = response[0] if isinstance(response, tuple) else response
                                
                                success = self._is_successful_response(response_text, document_count)
                                if not success:
                                    error_message = self._get_failure_reason(response_text, document_count)
                                
                                self._display_response_with_marker_conversion(response, query_type=query_type)
                                st.session_state.messages.append({"role": "assistant", "content": response_text})
                            else:
                                response_text = f"'{target_service_name or 'í•´ë‹¹ ì¡°ê±´'}'ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                success = False
                                error_message = "ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨"
                                st.write(response_text)
                                st.session_state.messages.append({"role": "assistant", "content": response_text})
                                
            except Exception as e:
                response_time = time.time() - start_time
                error_message = str(e)[:50] + ("..." if len(str(e)) > 50 else "")
                success = False
                response_text = f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(response_text)
                st.session_state.messages.append({"role": "assistant", "content": response_text})
                
                if not st.session_state.current_query_logged and self.monitoring_enabled and self._manual_logging_enabled:
                    self._log_query_activity(
                        query=query,
                        query_type=query_type,
                        response_time=response_time,
                        document_count=document_count,
                        success=success,
                        error_message=error_message,
                        response_content=response_text
                    )
                    st.session_state.current_query_logged = True
                return
            
            response_time = time.time() - start_time
            if not st.session_state.current_query_logged and self.monitoring_enabled and self._manual_logging_enabled:
                self._log_query_activity(
                    query=query,
                    query_type=query_type,
                    response_time=response_time,
                    document_count=document_count,
                    success=success,
                    error_message=error_message,
                    response_content=response_text
                )
                st.session_state.current_query_logged = True

    # ê¸°íƒ€ í•„ìˆ˜ ë©”ì„œë“œë“¤
    def _is_successful_response(self, response_text: str, document_count: int) -> bool:
        """ì‘ë‹µì´ ì„±ê³µì ì¸ì§€ íŒë‹¨"""
        if not response_text or response_text.strip() == "":
            return False
        
        failure_patterns = [
            r"í•´ë‹¹.*ì¡°ê±´.*ë¬¸ì„œ.*ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            r"ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì–´ì„œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            r"ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            r"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            r"ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            r"ì£„ì†¡í•©ë‹ˆë‹¤.*ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            r"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
            r"ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
            r"ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            r"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"
        ]
        
        for pattern in failure_patterns:
            if re.search(pattern, response_text, re.IGNORECASE):
                return False
        
        if len(response_text.strip()) < 10:
            return False
        
        if document_count == 0:
            return False
        
        if not self._is_rag_based_response(response_text, document_count):
            return False
        
        return True

    def _is_rag_based_response(self, response_text: str, document_count: int = None) -> bool:
        """RAG ì›ì²œ ë°ì´í„° ê¸°ë°˜ ë‹µë³€ì¸ì§€ íŒë‹¨"""
        
        if not response_text:
            return False
        
        response_lower = response_text.lower()
        
        if document_count is not None and document_count < 2:
            return False
        
        rag_markers = ['[repair_box_start]', '[cause_box_start]', 'case1', 'case2', 'case3', 'ì¥ì•  id', 'incident_id', 'service_name', 'ë³µêµ¬ë°©ë²•:', 'ì¥ì• ì›ì¸:', 'ì„œë¹„ìŠ¤ëª…:', 'ë°œìƒì¼ì‹œ:', 'ì¥ì• ì‹œê°„:', 'ë‹´ë‹¹ë¶€ì„œ:', 'ì°¸ì¡°ì¥ì• ì •ë³´', 'ì¥ì• ë“±ê¸‰:', 'inm2']
        rag_marker_count = sum(1 for marker in rag_markers if marker in response_lower)
        
        rag_patterns = [r'ì¥ì• \s*id\s*:\s*inm\d+', r'ì„œë¹„ìŠ¤ëª…\s*:\s*\w+', r'ë°œìƒì¼[ì‹œì]\s*:\s*\d{4}', r'ì¥ì• ì‹œê°„\s*:\s*\d+ë¶„', r'ë³µêµ¬ë°©ë²•\s*:\s*', r'ì¥ì• ì›ì¸\s*:\s*', r'\d+ë“±ê¸‰', r'incident_repair', r'error_date', r'case\d+\.']
        rag_pattern_count = sum(1 for pattern in rag_patterns if re.search(pattern, response_lower))
        
        general_patterns = [r'ì¼ë°˜ì ìœ¼ë¡œ\s+', r'ë³´í†µ\s+', r'ëŒ€ë¶€ë¶„\s+', r'í”íˆ\s+', r'ì£¼ë¡œ\s+', r'ë‹¤ìŒê³¼\s+ê°™ì€\s+ë°©ë²•', r'ë‹¤ìŒ\s+ë‹¨ê³„', r'ê¸°ë³¸ì ì¸\s+', r'í‘œì¤€ì ì¸\s+', r'ê¶Œì¥ì‚¬í•­', r'best\s+practice', r'ëª¨ë²”\s+ì‚¬ë¡€', r'ë‹¤ìŒê³¼\s+ê°™ì´\s+ì ‘ê·¼', r'ì‹œìŠ¤í…œ\s+ê´€ë¦¬ì', r'ë„¤íŠ¸ì›Œí¬\s+ê´€ë¦¬', r'ì„œë²„\s+ê´€ë¦¬']
        general_pattern_count = sum(1 for pattern in general_patterns if re.search(pattern, response_lower))
        
        non_rag_keywords = ['ì¼ë°˜ì ìœ¼ë¡œ', 'ë³´í†µ', 'ëŒ€ë¶€ë¶„', 'í”íˆ', 'ì£¼ë¡œ', 'ê¸°ë³¸ì ìœ¼ë¡œ', 'í‘œì¤€ì ìœ¼ë¡œ', 'ê¶Œì¥ì‚¬í•­', 'ëª¨ë²”ì‚¬ë¡€', 'ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•', 'ë‹¤ìŒ ë‹¨ê³„', 'ê¸°ë³¸ì ì¸ ì ê²€', 'ì‹œìŠ¤í…œ ê´€ë¦¬', 'ë„¤íŠ¸ì›Œí¬ ê´€ë¦¬', 'ì„œë²„ ê´€ë¦¬', 'ì¼ë°˜ì ì¸ í•´ê²°ì±…', 'í‘œì¤€ ì ˆì°¨', 'ê¸°ë³¸ ì›ì¹™', 'ë‹¤ìŒê³¼ ê°™ì€ ì¡°ì¹˜', 'ê¸°ë³¸ì ì¸ ìˆœì„œ']
        non_rag_keyword_count = sum(1 for keyword in non_rag_keywords if keyword in response_lower)
        
        statistics_indicators = ['ê±´ìˆ˜', 'í†µê³„', 'í˜„í™©', 'ë¶„í¬', 'ì—°ë„ë³„', 'ì›”ë³„', 'ì°¨íŠ¸']
        statistics_count = sum(1 for indicator in statistics_indicators if indicator in response_lower)
        
        print(f"DEBUG RAG íŒë‹¨: rag_markers={rag_marker_count}, rag_patterns={rag_pattern_count}, general_patterns={general_pattern_count}, non_rag_keywords={non_rag_keyword_count}")
        
        if rag_marker_count >= 3 or rag_pattern_count >= 2:
            return True
        
        if statistics_count >= 2 and any(word in response_lower for word in ['ì°¨íŠ¸', 'í‘œ', 'ì´', 'í•©ê³„']):
            return True
        
        if general_pattern_count >= 2 or non_rag_keyword_count >= 3:
            if rag_marker_count == 0 and rag_pattern_count == 0:
                print(f"DEBUG: ì¼ë°˜ì  ë‹µë³€ìœ¼ë¡œ íŒë‹¨ë¨ (general_pattern_count={general_pattern_count}, non_rag_keyword_count={non_rag_keyword_count})")
                return False
        
        if rag_marker_count > 0 or rag_pattern_count > 0:
            return True
        
        if len(response_text) > 200 and document_count and document_count >= 3:
            if non_rag_keyword_count < 2:
                return True
        
        print(f"DEBUG: ê¸°ë³¸ì ìœ¼ë¡œ ì¼ë°˜ ë‹µë³€ìœ¼ë¡œ íŒë‹¨ë¨")
        return False

    def _get_failure_reason(self, response_text: str, document_count: int) -> str:
        """ì‹¤íŒ¨ ì›ì¸ ë¶„ì„"""
        if not response_text or response_text.strip() == "":
            return "ì‘ë‹µ ë‚´ìš© ì—†ìŒ"
        
        if document_count == 0:
            return "ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨"
        
        if len(response_text.strip()) < 10:
            return "ì‘ë‹µ ê¸¸ì´ ë¶€ì¡±"
        
        failure_reasons = {
            r"í•´ë‹¹.*ì¡°ê±´.*ë¬¸ì„œ.*ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤": "ì¡°ê±´ ë§ëŠ” ë¬¸ì„œ ì—†ìŒ",
            r"ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì–´ì„œ": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ",
            r"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ",
            r"ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤": "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"
        }
        
        for pattern, reason in failure_reasons.items():
            if re.search(pattern, response_text, re.IGNORECASE):
                return reason
        
        if not self._is_rag_based_response(response_text, document_count):
            return "RAG ê¸°ë°˜ ë‹µë³€ ì•„ë‹˜"
        
        return "ì ì ˆí•œ ë‹µë³€ ìƒì„± ì‹¤íŒ¨"
    
    def _log_query_activity(self, query: str, query_type: str = None, response_time: float = None,
                        document_count: int = None, success: bool = None, 
                        error_message: str = None, response_content: str = None):
        """ì¿¼ë¦¬ í™œë™ ë¡œê¹…"""
        try:
            if hasattr(self, '_manual_logging_enabled') and not self._manual_logging_enabled:
                print(f"DEBUG: ìˆ˜ë™ ë¡œê¹…ì´ ë¹„í™œì„±í™”ë˜ì–´ ë¡œê¹…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            if hasattr(st.session_state, 'current_query_logged') and st.session_state.current_query_logged:
                print(f"DEBUG: í˜„ì¬ ì¿¼ë¦¬ê°€ ì´ë¯¸ ë¡œê¹…ë˜ì–´ ì¤‘ë³µ ë¡œê¹…ì„ ë°©ì§€í•©ë‹ˆë‹¤.")
                return
                
            if self.monitoring_manager:
                ip_address = getattr(st.session_state, 'client_ip', '127.0.0.1')
                
                self.monitoring_manager.log_user_activity(
                    ip_address=ip_address,
                    question=query,
                    query_type=query_type,
                    user_agent="Streamlit/ChatBot",
                    response_time=response_time,
                    document_count=document_count,
                    success=success,
                    error_message=error_message,
                    response_content=response_content
                )
                
                if hasattr(st.session_state, 'current_query_logged'):
                    st.session_state.current_query_logged = True
                    
                print(f"DEBUG: ì¿¼ë¦¬ ë¡œê¹… ì™„ë£Œ - Query: {query[:50]}..., Success: {success}")
                
        except Exception as e:
            print(f"ëª¨ë‹ˆí„°ë§ ë¡œê·¸ ì‹¤íŒ¨: {str(e)}")

    def force_replace_problematic_queries(self, query):
        """ë¬¸ì œ ì¿¼ë¦¬ ì¹˜í™˜ ë¡œì§ ë‹¨ìˆœí™”"""
        if not query:
            return query
        
        query_lower = query.lower()
        
        simple_replacements = {
            'ëª‡ê±´ì´ì•¼': 'ëª‡ê±´',
            'ëª‡ê±´ì´ë‹ˆ': 'ëª‡ê±´', 
            'ëª‡ê±´ì¸ê°€': 'ëª‡ê±´',
            'ì•Œë ¤ì¤˜': '',
            'ë³´ì—¬ì¤˜': '',
            'ë§í•´ì¤˜': ''
        }
        
        normalized_query = query
        for old, new in simple_replacements.items():
            normalized_query = normalized_query.replace(old, new)
        
        return normalized_query.strip()

    # ê¸°íƒ€ í•„ìˆ˜ ë©”ì„œë“œë“¤ ìœ ì§€
    def _convert_to_query_type_enum(self, query_type_str):
        mapping = {
            'repair': QueryType.REPAIR,
            'inquiry': QueryType.INQUIRY, 
            'statistics': QueryType.STATISTICS,
            'default': QueryType.DEFAULT
        }
        return mapping.get(query_type_str, QueryType.DEFAULT)

    def _determine_query_scope(self, conditions):
        scope_parts = []
        
        if conditions.get('year'):
            scope_parts.append(conditions['year'])
        
        if conditions.get('months'):
            months = [m.replace('ì›”', '') for m in conditions['months']]
            if len(months) == 1:
                scope_parts.append(f"{months[0]}ì›”")
            elif len(months) > 1:
                scope_parts.append(f"{months[0]}~{months[-1]}ì›”")
        
        if conditions.get('daynight'):
            scope_parts.append(conditions['daynight'])
        
        if conditions.get('week'):
            week_val = conditions['week']
            if week_val not in ['í‰ì¼', 'ì£¼ë§']:
                scope_parts.append(f"{week_val}ìš”ì¼")
            else:
                scope_parts.append(week_val)
        
        if conditions.get('incident_grade'):
            scope_parts.append(conditions['incident_grade'])
        
        if conditions.get('service_name'):
            scope_parts.append(f"'{conditions['service_name']}' ì„œë¹„ìŠ¤")
        
        if conditions.get('owner_depart'):
            scope_parts.append(f"'{conditions['owner_depart']}' ë¶€ì„œ")
        
        return ' '.join(scope_parts) if scope_parts else "ì „ì²´ ê¸°ê°„"
    
    def _get_chart_data_from_db_stats(self, db_stats, requested_chart_type=None):
        """DB í†µê³„ì—ì„œ ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ - ì›ì¸ìœ í˜• ìš°ì„  ì²˜ë¦¬"""
        conditions = db_stats['query_conditions']
        
        # ì›ì¸ìœ í˜• ì¿¼ë¦¬ì¸ ê²½ìš° ì›ì¸ìœ í˜• ì°¨íŠ¸ ë°ì´í„° ìš°ì„  ë°˜í™˜
        if db_stats.get('is_cause_type_query', False) and db_stats.get('cause_type_stats'):
            cause_stats = db_stats['cause_type_stats']
            # ìƒìœ„ 10ê°œë§Œ ì°¨íŠ¸ë¡œ í‘œì‹œ
            top_causes = dict(list(cause_stats.items())[:10])
            chart_type = requested_chart_type or 'horizontal_bar'
            return top_causes, chart_type
        
        # ê¸°ì¡´ ë¡œì§
        group_by = conditions.get('group_by', [])
        
        data_map = {
            'year': ('yearly_stats', 'line'),
            'month': ('monthly_stats', 'line'),
            'daynight': ('time_stats', 'bar', 'daynight'),
            'week': ('time_stats', 'bar', 'week'),
            'owner_depart': ('department_stats', 'horizontal_bar'),
            'service_name': ('service_stats', 'horizontal_bar'),
            'incident_grade': ('grade_stats', 'pie'),
            'cause_type': ('cause_type_stats', 'horizontal_bar')
        }
        
        for group_type in group_by:
            if group_type in data_map:
                mapping = data_map[group_type]
                if len(mapping) == 3:  # time_stats case
                    data = db_stats[mapping[0]].get(mapping[2], {})
                    default_chart_type = mapping[1]
                else:
                    data = db_stats.get(mapping[0], {})
                    default_chart_type = mapping[1]
                
                if group_type in ['owner_depart', 'service_name', 'cause_type']:
                    data = dict(sorted(data.items(), key=lambda x: x[1], reverse=True)[:10])
                
                break
        else:
            data = db_stats.get('yearly_stats', {}) or db_stats.get('monthly_stats', {})
            default_chart_type = 'line'
        
        chart_type = requested_chart_type or default_chart_type
        
        if default_chart_type == 'line' and len(data) == 1:
            chart_type = 'bar'
        
        return data, chart_type
    
    def _generate_chart_title_from_db_stats(self, query, db_stats):
        """DB í†µê³„ ê¸°ë°˜ ì°¨íŠ¸ ì œëª© ìƒì„± - ì›ì¸ìœ í˜• ì²˜ë¦¬"""
        conditions = db_stats['query_conditions']
        
        # ì›ì¸ìœ í˜• ì¿¼ë¦¬ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if db_stats.get('is_cause_type_query', False):
            title_parts = ["ì›ì¸ìœ í˜•ë³„"]
            
            if conditions.get('year'):
                title_parts.insert(0, f"{conditions['year']}ë…„")
            
            if db_stats['is_error_time_query']:
                title_parts.append("ì¥ì• ì‹œê°„ ë¶„í¬")
            else:
                title_parts.append("ì¥ì•  ë°œìƒ í˜„í™©")
            
            return ' '.join(title_parts)
        
        # ê¸°ì¡´ ë¡œì§
        group_by = conditions.get('group_by', [])
        title_parts = []
        
        if conditions.get('year'):
            title_parts.append(conditions['year'])
        
        group_titles = {
            'year': "ì—°ë„ë³„",
            'month': "ì›”ë³„",
            'daynight': "ì‹œê°„ëŒ€ë³„",
            'week': "ìš”ì¼ë³„",
            'owner_depart': "ë¶€ì„œë³„",
            'service_name': "ì„œë¹„ìŠ¤ë³„",
            'incident_grade': "ë“±ê¸‰ë³„",
            'cause_type': "ì›ì¸ìœ í˜•ë³„"
        }
        
        for group_type in group_by:
            if group_type in group_titles:
                title_parts.append(group_titles[group_type])
                break
        
        if db_stats['is_error_time_query']:
            title_parts.append("ì¥ì• ì‹œê°„")
        else:
            title_parts.append("ì¥ì•  ë°œìƒ í˜„í™©")
        
        return ' '.join(title_parts)

    def calculate_unified_statistics(self, documents, query, query_type="default"):
        """í†µí•© í†µê³„ ê³„ì‚° - ë¬´ê²°ì„± ë³´ì¥ ê³„ì‚°ê¸° ì‚¬ìš©"""
        return self.statistics_calculator._empty_statistics() if not documents else self.statistics_calculator.calculate_comprehensive_statistics(query, documents, query_type)
