import streamlit as st
import re
from config.prompts import SystemPrompts
from config.settings_local import AppConfigLocal
from utils.search_utils_local import SearchManagerLocal
from utils.ui_components_local import UIComponentsLocal

class QueryProcessorLocal:
    """ì¿¼ë¦¬ ì²˜ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤ - ì‹œê°„ëŒ€/ìš”ì¼ ê¸°ë°˜ í•„í„°ë§ ì§€ì› ì¶”ê°€ + ì •í™•í•œ ì„œë¹„ìŠ¤ëª… í•„í„°ë§ ê°•í™”"""
    
    def __init__(self, azure_openai_client, search_client, model_name, config=None):
        self.azure_openai_client = azure_openai_client
        self.search_client = search_client
        self.model_name = model_name
        self.config = config if config else AppConfigLocal()
        self.search_manager = SearchManagerLocal(search_client, self.config)
        self.ui_components = UIComponentsLocal()
        # ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì • (ê°œë°œ ì‹œì—ë§Œ Trueë¡œ ì„¤ì •)
        self.debug_mode = False
    
    def extract_time_conditions(self, query):
        """ì¿¼ë¦¬ì—ì„œ ì‹œê°„ëŒ€/ìš”ì¼ ì¡°ê±´ ì¶”ì¶œ"""
        time_conditions = {
            'daynight': None,  # 'ì£¼ê°„' ë˜ëŠ” 'ì•¼ê°„'
            'week': None,      # ìš”ì¼
            'is_time_query': False
        }
        
        # ì£¼ê°„/ì•¼ê°„ íŒ¨í„´ ê²€ìƒ‰
        daynight_patterns = [
            r'\b(ì•¼ê°„|ë°¤|ìƒˆë²½|ì‹¬ì•¼|ì•¼ì‹œê°„)\b',
            r'\b(ì£¼ê°„|ë‚®|ì˜¤ì „|ì˜¤í›„|ì£¼ì‹œê°„|ì¼ê³¼ì‹œê°„)\b'
        ]
        
        for pattern in daynight_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                time_conditions['is_time_query'] = True
                for match in matches:
                    if match in ['ì•¼ê°„', 'ë°¤', 'ìƒˆë²½', 'ì‹¬ì•¼', 'ì•¼ì‹œê°„']:
                        time_conditions['daynight'] = 'ì•¼ê°„'
                    elif match in ['ì£¼ê°„', 'ë‚®', 'ì˜¤ì „', 'ì˜¤í›„', 'ì£¼ì‹œê°„', 'ì¼ê³¼ì‹œê°„']:
                        time_conditions['daynight'] = 'ì£¼ê°„'
        
        # ìš”ì¼ íŒ¨í„´ ê²€ìƒ‰
        week_patterns = [
            r'\b(ì›”ìš”ì¼|ì›”)\b',
            r'\b(í™”ìš”ì¼|í™”)\b', 
            r'\b(ìˆ˜ìš”ì¼|ìˆ˜)\b',
            r'\b(ëª©ìš”ì¼|ëª©)\b',
            r'\b(ê¸ˆìš”ì¼|ê¸ˆ)\b',
            r'\b(í† ìš”ì¼|í† )\b',
            r'\b(ì¼ìš”ì¼|ì¼)\b',
            r'\b(í‰ì¼|ì£¼ì¤‘)\b',
            r'\b(ì£¼ë§|í† ì¼)\b'
        ]
        
        for pattern in week_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                time_conditions['is_time_query'] = True
                for match in matches:
                    if match in ['ì›”ìš”ì¼', 'ì›”']:
                        time_conditions['week'] = 'ì›”'
                    elif match in ['í™”ìš”ì¼', 'í™”']:
                        time_conditions['week'] = 'í™”'
                    elif match in ['ìˆ˜ìš”ì¼', 'ìˆ˜']:
                        time_conditions['week'] = 'ìˆ˜'
                    elif match in ['ëª©ìš”ì¼', 'ëª©']:
                        time_conditions['week'] = 'ëª©'
                    elif match in ['ê¸ˆìš”ì¼', 'ê¸ˆ']:
                        time_conditions['week'] = 'ê¸ˆ'
                    elif match in ['í† ìš”ì¼', 'í† ']:
                        time_conditions['week'] = 'í† '
                    elif match in ['ì¼ìš”ì¼', 'ì¼']:
                        time_conditions['week'] = 'ì¼'
                    elif match in ['í‰ì¼', 'ì£¼ì¤‘']:
                        time_conditions['week'] = 'í‰ì¼'
                    elif match in ['ì£¼ë§', 'í† ì¼']:
                        time_conditions['week'] = 'ì£¼ë§'
        
        return time_conditions
    
    def extract_department_conditions(self, query):
        """ì¿¼ë¦¬ì—ì„œ ë¶€ì„œ ê´€ë ¨ ì¡°ê±´ ì¶”ì¶œ"""
        department_conditions = {
            'owner_depart': None,  # íŠ¹ì • ë¶€ì„œëª…
            'is_department_query': False
        }
        
        # ë¶€ì„œ ê´€ë ¨ í‚¤ì›Œë“œ ê°ì§€
        department_keywords = [
            'ë‹´ë‹¹ë¶€ì„œ', 'ì¡°ì¹˜ë¶€ì„œ', 'ì²˜ë¦¬ë¶€ì„œ', 'ì±…ì„ë¶€ì„œ', 'ê´€ë¦¬ë¶€ì„œ',
            'ë¶€ì„œ', 'íŒ€', 'ì¡°ì§', 'ë‹´ë‹¹', 'ì²˜ë¦¬', 'ì¡°ì¹˜', 'ê´€ë¦¬'
        ]
        
        # ë¶€ì„œ ì§ˆë¬¸ì¸ì§€ í™•ì¸
        if any(keyword in query for keyword in department_keywords):
            department_conditions['is_department_query'] = True
        
        # íŠ¹ì • ë¶€ì„œëª… ì¶”ì¶œ (ì¼ë°˜ì ì¸ ë¶€ì„œëª… íŒ¨í„´)
        department_patterns = [
            r'\b(ê°œë°œ|ìš´ì˜|ê¸°ìˆ |ì‹œìŠ¤í…œ|ë„¤íŠ¸ì›Œí¬|ë³´ì•ˆ|DB|ë°ì´í„°ë² ì´ìŠ¤|ì¸í”„ë¼|í´ë¼ìš°ë“œ)(?:ë¶€ì„œ|íŒ€|íŒŒíŠ¸)?\b',
            r'\b(ê³ ê°|ì„œë¹„ìŠ¤|ìƒë‹´|ì§€ì›|í—¬í”„ë°ìŠ¤í¬)(?:ë¶€ì„œ|íŒ€|íŒŒíŠ¸)?\b',
            r'\b(IT|ì •ë³´ì‹œìŠ¤í…œ|ì •ë³´ê¸°ìˆ |ì „ì‚°)(?:ë¶€ì„œ|íŒ€|íŒŒíŠ¸)?\b',
            r'\b([ê°€-í£]+)(?:ë¶€ì„œ|íŒ€|íŒŒíŠ¸)\b'
        ]
        
        for pattern in department_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                # ì²« ë²ˆì§¸ ë§¤ì¹­ëœ ë¶€ì„œëª… ì‚¬ìš©
                department_conditions['owner_depart'] = matches[0]
                break
        
        return department_conditions
    
    def classify_query_type_with_llm(self, query):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ íƒ€ì…ì„ ìë™ìœ¼ë¡œ ë¶„ë¥˜ - ì‹œê°„ ê´€ë ¨ ì¿¼ë¦¬ ì§€ì›"""
        try:
            classification_prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.

**ë¶„ë¥˜ ê¸°ì¤€:**
1. **repair**: ì„œë¹„ìŠ¤ëª…ê³¼ ì¥ì• í˜„ìƒì´ ëª¨ë‘ í¬í•¨ëœ ë³µêµ¬ë°©ë²• ë¬¸ì˜
   - ì˜ˆ: "ERP ì ‘ì†ë¶ˆê°€ ë³µêµ¬ë°©ë²•", "API_Link ì‘ë‹µì§€ì—° í•´ê²°ë°©ë²•"
   
2. **cause**: ì¥ì• ì›ì¸ ë¶„ì„ì´ë‚˜ ì›ì¸ íŒŒì•…ì„ ìš”ì²­í•˜ëŠ” ë¬¸ì˜
   - ì˜ˆ: "ERP ì ‘ì†ë¶ˆê°€ ì›ì¸ì´ ë­ì•¼?", "API ì‘ë‹µì§€ì—° ì¥ì• ì›ì¸", "ì™œ ì¥ì• ê°€ ë°œìƒí–ˆì–´?"
   
3. **similar**: ì„œë¹„ìŠ¤ëª… ì—†ì´ ì¥ì• í˜„ìƒë§Œìœ¼ë¡œ ìœ ì‚¬ì‚¬ë¡€ ë¬¸ì˜
   - ì˜ˆ: "ì ‘ì†ë¶ˆê°€ í˜„ìƒ ìœ ì‚¬ì‚¬ë¡€", "ì‘ë‹µì§€ì—° ë™ì¼í˜„ìƒ ë³µêµ¬ë°©ë²•"
   
4. **default**: ê·¸ ì™¸ì˜ ëª¨ë“  ê²½ìš° (í†µê³„, ê±´ìˆ˜, ì¼ë°˜ ë¬¸ì˜, ì‹œê°„ëŒ€ë³„ ì¡°íšŒ ë“±)
   - ì˜ˆ: "ë…„ë„ë³„ ê±´ìˆ˜", "ì¥ì•  í†µê³„", "ì„œë¹„ìŠ¤ í˜„í™©", "ì•¼ê°„ì— ë°œìƒí•œ ì¥ì• ", "ì£¼ë§ ì¥ì•  í˜„í™©"

**ì‚¬ìš©ì ì§ˆë¬¸:** {query}

**ì‘ë‹µ í˜•ì‹:** repair, cause, similar, default ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

            response = self.azure_openai_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ IT ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ì •í™•íˆ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": classification_prompt}
                ],
                temperature=0.1,
                max_tokens=50
            )
            
            query_type = response.choices[0].message.content.strip().lower()
            
            if query_type not in ['repair', 'cause', 'similar', 'default']:
                query_type = 'default'
                
            return query_type
            
        except Exception as e:
            if self.debug_mode:
                st.warning(f"ì¿¼ë¦¬ ë¶„ë¥˜ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {str(e)}")
            return 'default'

    def validate_document_relevance_with_llm(self, query, documents):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„±ì„ ì¬ê²€ì¦ - repair/cause ì „ìš©"""
        try:
            if not documents:
                return []
            
            validation_prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ë‹¤ìŒ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì‹¤ì œë¡œ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë§Œ ì„ ë³„í•´ì£¼ì„¸ìš”.
ê° ë¬¸ì„œì— ëŒ€í•´ 0-100ì  ì‚¬ì´ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , 70ì  ì´ìƒì¸ ë¬¸ì„œë§Œ ì„ íƒí•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
1. ì„œë¹„ìŠ¤ëª… ì¼ì¹˜ë„ (ì‚¬ìš©ìê°€ íŠ¹ì • ì„œë¹„ìŠ¤ë¥¼ ì–¸ê¸‰í•œ ê²½ìš°)
2. ì¥ì• í˜„ìƒ/ì¦ìƒ ì¼ì¹˜ë„  
3. ì‚¬ìš©ìê°€ ìš”êµ¬í•œ ì •ë³´ ìœ í˜•ê³¼ì˜ ì¼ì¹˜ë„
4. ì „ì²´ì ì¸ ë§¥ë½ ì¼ì¹˜ë„

"""

            for i, doc in enumerate(documents):
                doc_info = f"""
ë¬¸ì„œ {i+1}:
- ì„œë¹„ìŠ¤ëª…: {doc.get('service_name', '')}
- ì¥ì• í˜„ìƒ: {doc.get('symptom', '')}
- ì˜í–¥ë„: {doc.get('effect', '')}
- ì¥ì• ì›ì¸: {doc.get('root_cause', '')[:100]}...
- ë³µêµ¬ë°©ë²•: {doc.get('incident_repair', '')[:100]}...
"""
                validation_prompt += doc_info

            validation_prompt += """

ì‘ë‹µ í˜•ì‹ (JSON):
{
    "validated_documents": [
        {
            "document_index": 1,
            "relevance_score": 85,
            "reason": "ì„œë¹„ìŠ¤ëª…ê³¼ ì¥ì• í˜„ìƒì´ ì •í™•íˆ ì¼ì¹˜í•¨"
        },
        {
            "document_index": 3,
            "relevance_score": 72,
            "reason": "ì¥ì• í˜„ìƒì€ ìœ ì‚¬í•˜ì§€ë§Œ ì„œë¹„ìŠ¤ëª…ì´ ë‹¤ë¦„"
        }
    ]
}

70ì  ì´ìƒì¸ ë¬¸ì„œë§Œ í¬í•¨í•˜ì„¸ìš”.
"""

            response = self.azure_openai_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ ì •í™•í•˜ê²Œ í‰ê°€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": validation_prompt}
                ],
                temperature=0.1,
                max_tokens=800
            )
            
            response_content = response.choices[0].message.content.strip()
            
            try:
                import json
                json_start = response_content.find('{')
                json_end = response_content.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_content = response_content[json_start:json_end]
                    validation_result = json.loads(json_content)
                    
                    validated_docs = []
                    for validated_doc in validation_result.get('validated_documents', []):
                        doc_index = validated_doc.get('document_index', 1) - 1
                        if 0 <= doc_index < len(documents):
                            original_doc = documents[doc_index].copy()
                            original_doc['relevance_score'] = validated_doc.get('relevance_score', 0)
                            original_doc['validation_reason'] = validated_doc.get('reason', 'ê²€ì¦ë¨')
                            validated_docs.append(original_doc)
                    
                    validated_docs.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
                    return validated_docs
                    
            except (json.JSONDecodeError, KeyError) as e:
                if self.debug_mode:
                    st.warning(f"ë¬¸ì„œ ê²€ì¦ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                return documents[:5]
                
        except Exception as e:
            if self.debug_mode:
                st.warning(f"ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return documents[:5]
        
        return documents

    def validate_service_specific_documents(self, documents, target_service_name):
        """ì§€ì •ëœ ì„œë¹„ìŠ¤ëª…ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë§Œ í•„í„°ë§ - ì •í™•í•œ ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ ê°•í™”"""
        if not target_service_name or not documents:
            return documents
        
        # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª…ì¸ì§€ í™•ì¸
        is_common, _ = self.search_manager.is_common_term_service(target_service_name)
        
        validated_docs = []
        filter_stats = {
            'total': len(documents),
            'exact_matches': 0,
            'partial_matches': 0,
            'excluded': 0
        }
        
        for doc in documents:
            doc_service_name = doc.get('service_name', '').strip()
            
            if is_common:
                # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª…: ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ í—ˆìš©
                if doc_service_name.lower() == target_service_name.lower():
                    filter_stats['exact_matches'] += 1
                    validated_docs.append(doc)
                else:
                    filter_stats['excluded'] += 1
                    if self.debug_mode:
                        st.info(f"ì œì™¸ëœ ë¬¸ì„œ: {doc_service_name} (ìš”ì²­: {target_service_name})")
            else:
                # ì¼ë°˜ì ì¸ ì„œë¹„ìŠ¤ëª…: ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ ê´€ê³„ì¸ ê²½ìš° í—ˆìš©
                if doc_service_name.lower() == target_service_name.lower():
                    filter_stats['exact_matches'] += 1
                    validated_docs.append(doc)
                elif target_service_name.lower() in doc_service_name.lower() or doc_service_name.lower() in target_service_name.lower():
                    filter_stats['partial_matches'] += 1
                    validated_docs.append(doc)
                else:
                    filter_stats['excluded'] += 1
        
        # ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ í•„í„°ë§ ê²°ê³¼ í‘œì‹œ
        if self.debug_mode:
            service_type = "ì¼ë°˜ìš©ì–´" if is_common else "ì¼ë°˜"
            st.info(f"""
            ğŸ¯ ì„œë¹„ìŠ¤ëª… í•„í„°ë§ ê²°ê³¼ ({service_type} ì„œë¹„ìŠ¤: {target_service_name})
            - ì „ì²´ ë¬¸ì„œ: {filter_stats['total']}ê°œ
            - ì •í™•íˆ ì¼ì¹˜: {filter_stats['exact_matches']}ê°œ
            - ë¶€ë¶„ ì¼ì¹˜: {filter_stats['partial_matches']}ê°œ
            - ì œì™¸ëœ ë¬¸ì„œ: {filter_stats['excluded']}ê°œ
            - ìµœì¢… ì„ ë³„: {len(validated_docs)}ê°œ
            """)
        
        return validated_docs

    def generate_rag_response_with_adaptive_processing(self, query, documents, query_type="default", time_conditions=None, department_conditions=None):
        """ì¿¼ë¦¬ íƒ€ì…ë³„ ì ì‘í˜• RAG ì‘ë‹µ ìƒì„± - ì‹œê°„ ì¡°ê±´ ë° ë¶€ì„œ ì¡°ê±´ ì§€ì› + ì •í™•í•œ ì„œë¹„ìŠ¤ëª… í•„í„°ë§ ê°•í™”"""
        try:
            # ì‹œê°„ ì¡°ê±´ì´ ìˆëŠ” ê²½ìš° ë¬¸ì„œ í•„í„°ë§
            if time_conditions and time_conditions.get('is_time_query'):
                documents = self.search_manager.filter_documents_by_time_conditions(documents, time_conditions)
                
                if not documents:
                    time_desc = []
                    if time_conditions.get('daynight'):
                        time_desc.append(f"{time_conditions['daynight']}")
                    if time_conditions.get('week'):
                        time_desc.append(f"{time_conditions['week']}")
                    
                    return f"{''.join(time_desc)} ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì¥ì•  ë‚´ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ ì¡°ê±´ì„ ì‹œë„í•´ë³´ì„¸ìš”."
            
            # ë¶€ì„œ ì¡°ê±´ì´ ìˆëŠ” ê²½ìš° ë¬¸ì„œ í•„í„°ë§
            if department_conditions and department_conditions.get('is_department_query'):
                documents = self.search_manager.filter_documents_by_department_conditions(documents, department_conditions)
                
                if not documents:
                    dept_desc = department_conditions.get('owner_depart', 'í•´ë‹¹ ë¶€ì„œ')
                    return f"{dept_desc} ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì¥ì•  ë‚´ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ ì¡°ê±´ì„ ì‹œë„í•´ë³´ì„¸ìš”."
            
            # ì¿¼ë¦¬ íƒ€ì…ë³„ ì²˜ë¦¬ ë°©ì‹ ê²°ì •
            use_llm_validation = query_type in ['repair', 'cause']
            
            if use_llm_validation:
                # repair/cause: ì •í™•ì„± ìš°ì„  ì²˜ë¦¬
                if self.debug_mode:
                    st.info("ğŸ¯ ì •í™•ì„± ìš°ì„  ì²˜ë¦¬ - ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„± ì¬ê²€ì¦ ì¤‘...")
                validated_documents = self.validate_document_relevance_with_llm(query, documents)
                
                if self.debug_mode:
                    if len(validated_documents) < len(documents):
                        removed_count = len(documents) - len(validated_documents)
                        st.success(f"âœ… ê´€ë ¨ì„± ê²€ì¦ ì™„ë£Œ: {len(validated_documents)}ê°œ ë¬¸ì„œ ì„ ë³„ (ê´€ë ¨ì„± ë‚®ì€ {removed_count}ê°œ ë¬¸ì„œ ì œì™¸)")
                    else:
                        st.success(f"âœ… ê´€ë ¨ì„± ê²€ì¦ ì™„ë£Œ: ëª¨ë“  {len(validated_documents)}ê°œ ë¬¸ì„œê°€ ê´€ë ¨ì„± ê¸°ì¤€ í†µê³¼")
                
                if not validated_documents:
                    return "ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ì•„ ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë‚˜ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
                
                processing_documents = validated_documents
                processing_info = "ê´€ë ¨ì„± ê²€ì¦ ì™„ë£Œ (70ì  ì´ìƒ)"
                
            else:
                # similar/default: í¬ê´„ì„± ìš°ì„  ì²˜ë¦¬
                if self.debug_mode:
                    st.info("ğŸ“‹ í¬ê´„ì„± ìš°ì„  ì²˜ë¦¬ - ê´‘ë²”ìœ„í•œ ê²€ìƒ‰ ê²°ê³¼ í™œìš© ì¤‘...")
                processing_documents = documents
                processing_info = "í¬ê´„ì  ê²€ìƒ‰ ê²°ê³¼ í™œìš©"
                if self.debug_mode:
                    st.success(f"âœ… í¬ê´„ì  ì²˜ë¦¬ ì™„ë£Œ: {len(processing_documents)}ê°œ ë¬¸ì„œ í™œìš©")

            # **ìˆ˜ì •: ì¤‘ë³µ ì œê±° ë° ì •í™•í•œ ì§‘ê³„ ì •ë³´ ê³„ì‚° (ì‹œê°„ ì¡°ê±´ ë° ë¶€ì„œ ì¡°ê±´ ë°˜ì˜)**
            # ì¥ì•  ID ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
            unique_documents = {}
            for doc in processing_documents:
                incident_id = doc.get('incident_id', '')
                if incident_id and incident_id not in unique_documents:
                    unique_documents[incident_id] = doc
            
            # ì¤‘ë³µ ì œê±°ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ì—…ë°ì´íŠ¸
            processing_documents = list(unique_documents.values())
            total_count = len(processing_documents)
            
            # **ìˆ˜ì •: ë” ì •í™•í•œ í†µê³„ ê³„ì‚°**
            yearly_stats = {}
            time_stats = {'daynight': {}, 'week': {}}
            department_stats = {}
            service_stats = {}  # ì„œë¹„ìŠ¤ë³„ í†µê³„ ì¶”ê°€
            
            for doc in processing_documents:
                # ë…„ë„ í†µê³„
                error_date = doc.get('error_date', '')
                year_from_date = None
                if error_date and len(error_date) >= 4:
                    try:
                        year_from_date = int(error_date[:4])
                    except:
                        pass
                
                year_from_field = doc.get('year', '')
                if year_from_field:
                    try:
                        year_from_field = int(year_from_field)
                    except:
                        year_from_field = None
                
                final_year = year_from_date or year_from_field
                if final_year:
                    yearly_stats[final_year] = yearly_stats.get(final_year, 0) + 1
                
                # ì‹œê°„ëŒ€ í†µê³„
                daynight = doc.get('daynight', '')
                if daynight:
                    time_stats['daynight'][daynight] = time_stats['daynight'].get(daynight, 0) + 1
                
                # ìš”ì¼ í†µê³„  
                week = doc.get('week', '')
                if week:
                    time_stats['week'][week] = time_stats['week'].get(week, 0) + 1
                
                # ë¶€ì„œ í†µê³„
                owner_depart = doc.get('owner_depart', '')
                if owner_depart:
                    department_stats[owner_depart] = department_stats.get(owner_depart, 0) + 1
                
                # ì„œë¹„ìŠ¤ë³„ í†µê³„ ì¶”ê°€
                service_name = doc.get('service_name', '')
                if service_name:
                    service_stats[service_name] = service_stats.get(service_name, 0) + 1
            
            yearly_total = sum(yearly_stats.values())
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            
            # ì‹œê°„ ì¡°ê±´ ì •ë³´ ì¶”ê°€
            time_condition_info = ""
            if time_conditions and time_conditions.get('is_time_query'):
                time_desc = []
                if time_conditions.get('daynight'):
                    time_desc.append(f"ì‹œê°„ëŒ€: {time_conditions['daynight']}")
                if time_conditions.get('week'):
                    time_desc.append(f"ìš”ì¼: {time_conditions['week']}")
                time_condition_info = f" - ì‹œê°„ ì¡°ê±´: {', '.join(time_desc)}"
            
            # ë¶€ì„œ ì¡°ê±´ ì •ë³´ ì¶”ê°€
            department_condition_info = ""
            if department_conditions and department_conditions.get('is_department_query'):
                if department_conditions.get('owner_depart'):
                    department_condition_info = f" - ë¶€ì„œ ì¡°ê±´: {department_conditions['owner_depart']}"
                else:
                    department_condition_info = f" - ë¶€ì„œë³„ ì¡°íšŒ"
            
            # **ìˆ˜ì •: ì„œë¹„ìŠ¤ë³„ í†µê³„ ì •ë³´ ì¶”ê°€**
            service_stats_info = ""
            if len(service_stats) > 1:
                service_stats_info = f"\nì„œë¹„ìŠ¤ë³„ ë¶„í¬: {dict(sorted(service_stats.items(), key=lambda x: x[1], reverse=True))}"
            elif len(service_stats) == 1:
                service_name = list(service_stats.keys())[0]
                service_stats_info = f"\nëŒ€ìƒ ì„œë¹„ìŠ¤: {service_name} ({service_stats[service_name]}ê±´)"
            
            stats_info = f"""
=== ì •í™•í•œ ì§‘ê³„ ì •ë³´ ({processing_info}{time_condition_info}{department_condition_info}) ===
ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_count}ê±´
ë…„ë„ë³„ ë¶„í¬: {dict(sorted(yearly_stats.items()))}
ë…„ë„ë³„ í•©ê³„: {yearly_total}ê±´{service_stats_info}
ì§‘ê³„ ê²€ì¦: {'ì¼ì¹˜' if yearly_total == total_count else 'ë¶ˆì¼ì¹˜ - ì¬ê³„ì‚° í•„ìš”'}
ì²˜ë¦¬ ë°©ì‹: {'ì •í™•ì„± ìš°ì„  (LLM ê²€ì¦)' if use_llm_validation else 'í¬ê´„ì„± ìš°ì„  (ê´‘ë²”ìœ„ ê²€ìƒ‰)'}
"""
            
            # ì‹œê°„ í†µê³„ ì¶”ê°€ (ì‹œê°„ ì¿¼ë¦¬ì¸ ê²½ìš°)
            if time_conditions and time_conditions.get('is_time_query'):
                if time_stats['daynight']:
                    stats_info += f"ì‹œê°„ëŒ€ë³„ ë¶„í¬: {time_stats['daynight']}\n"
                if time_stats['week']:
                    stats_info += f"ìš”ì¼ë³„ ë¶„í¬: {time_stats['week']}\n"
            
            # ë¶€ì„œ í†µê³„ ì¶”ê°€ (ë¶€ì„œ ì¿¼ë¦¬ì¸ ê²½ìš°)
            if department_conditions and department_conditions.get('is_department_query'):
                if department_stats:
                    stats_info += f"ë¶€ì„œë³„ ë¶„í¬: {department_stats}\n"
            
            stats_info += "==========================="
            
            context_parts.append(stats_info)
            
            for i, doc in enumerate(processing_documents):
                final_score = doc.get('final_score', 0)
                quality_tier = doc.get('quality_tier', 'Standard')
                filter_reason = doc.get('filter_reason', 'ê¸°ë³¸ ì„ ë³„')
                service_match_type = doc.get('service_match_type', 'unknown')
                relevance_score = doc.get('relevance_score', 0) if use_llm_validation else "N/A"
                validation_reason = doc.get('validation_reason', 'ê²€ì¦ë¨') if use_llm_validation else "í¬ê´„ì  ì²˜ë¦¬"
                
                validation_info = f" - ê´€ë ¨ì„±: {relevance_score}ì  ({validation_reason})" if use_llm_validation else " - í¬ê´„ì  ê²€ìƒ‰"
                
                # ì‹œê°„ ì •ë³´ ì¶”ê°€
                time_info = ""
                if doc.get('daynight'):
                    time_info += f" - ì‹œê°„ëŒ€: {doc.get('daynight')}"
                if doc.get('week'):
                    time_info += f" - ìš”ì¼: {doc.get('week')}"
                
                # ë¶€ì„œ ì •ë³´ ì¶”ê°€
                department_info = ""
                if doc.get('owner_depart'):
                    department_info += f" - ë‹´ë‹¹ë¶€ì„œ: {doc.get('owner_depart')}"
                
                context_part = f"""ë¬¸ì„œ {i+1} [{quality_tier}ê¸‰ - {filter_reason} - {service_match_type} ë§¤ì¹­{validation_info}{time_info}{department_info}]:
ì¥ì•  ID: {doc['incident_id']}
ì„œë¹„ìŠ¤ëª…: {doc['service_name']}
ì¥ì• ì‹œê°„: {doc['error_time']}
ì˜í–¥ë„: {doc['effect']}
í˜„ìƒ: {doc['symptom']}
ë³µêµ¬ê³µì§€: {doc['repair_notice']}
ë°œìƒì¼ì: {doc['error_date']}
ìš”ì¼: {doc['week']}
ì‹œê°„ëŒ€: {doc['daynight']}
ì¥ì• ì›ì¸: {doc['root_cause']}
ë³µêµ¬ë°©ë²•: {doc['incident_repair']}
ê°œì„ ê³„íš: {doc['incident_plan']}
ì›ì¸ìœ í˜•: {doc['cause_type']}
ì²˜ë¦¬ìœ í˜•: {doc['done_type']}
ì¥ì• ë“±ê¸‰: {doc['incident_grade']}
ë‹´ë‹¹ë¶€ì„œ: {doc['owner_depart']}
ë…„ë„: {doc['year']}
ì›”: {doc['month']}
í’ˆì§ˆì ìˆ˜: {final_score:.2f}
"""
                if use_llm_validation:
                    context_part += f"ê´€ë ¨ì„±ì ìˆ˜: {relevance_score}ì \n"
                
                context_parts.append(context_part)
            
            context = "\n\n".join(context_parts)
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            system_prompt = SystemPrompts.get_prompt(query_type)

            user_prompt = f"""
ë‹¤ìŒ ì¥ì•  ì´ë ¥ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
(ì²˜ë¦¬ ë°©ì‹: {'ì •í™•ì„± ìš°ì„  - LLM ê´€ë ¨ì„± ê²€ì¦ ì ìš©' if use_llm_validation else 'í¬ê´„ì„± ìš°ì„  - ê´‘ë²”ìœ„í•œ ê²€ìƒ‰ ê²°ê³¼ í™œìš©'}):

**ì¤‘ìš”! ì •í™•í•œ ì§‘ê³„ ê²€ì¦ í•„ìˆ˜ì‚¬í•­:**
- ì‹¤ì œ ì œê³µëœ ë¬¸ì„œ ìˆ˜: {total_count}ê±´ (ì¤‘ë³µ ì œê±° ì™„ë£Œ)
- ë…„ë„ë³„ ê±´ìˆ˜: {dict(sorted(yearly_stats.items()))}
- ë…„ë„ë³„ í•©ê³„: {yearly_total}ê±´
- ì„œë¹„ìŠ¤ë³„ ë¶„í¬: {dict(sorted(service_stats.items(), key=lambda x: x[1], reverse=True)) if service_stats else 'ì •ë³´ì—†ìŒ'}
- **ë‹µë³€ ì‹œ ë°˜ë“œì‹œ ì‹¤ì œ ë¬¸ì„œ ìˆ˜({total_count}ê±´)ì™€ ì¼ì¹˜í•´ì•¼ í•¨**
- **í‘œì‹œí•˜ëŠ” ë‚´ì—­ ìˆ˜ì™€ ì´ ê±´ìˆ˜ê°€ ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•¨**
- **ë¶ˆì¼ì¹˜ ì‹œ ë°˜ë“œì‹œ ì¬ê³„ì‚° í›„ ë‹µë³€í•  ê²ƒ**

**ê²€ì¦ ì ˆì°¨:**
1. ë‹µë³€í•˜ê¸° ì „ì— ì‹¤ì œ ì œê³µëœ ë¬¸ì„œê°€ ëª‡ ê°œì¸ì§€ ë‹¤ì‹œ ì„¸ì–´ë³´ì„¸ìš”
2. í‘œì‹œí•  ë‚´ì—­ ìˆ˜ê°€ ì´ ê±´ìˆ˜ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”  
3. ë¶ˆì¼ì¹˜í•˜ë©´ ì •í™•í•œ ìˆ˜ë¡œ ìˆ˜ì •í•´ì„œ ë‹µë³€í•˜ì„¸ìš”

{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""

            # Azure OpenAI API í˜¸ì¶œ
            response = self.azure_openai_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.0,  # ì •í™•ì„±ì„ ìœ„í•´ 0.0ìœ¼ë¡œ ì„¤ì •
                max_tokens=1500
            )
            
            return response.choices[0].message.content
        
        except Exception as e:
            st.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def process_query(self, query, query_type=None):
        """ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì í™”ëœ ì²˜ë¦¬ ë¡œì§ì„ ì ìš©í•œ ë©”ì¸ ì¿¼ë¦¬ ì²˜ë¦¬ - ì‹œê°„ ì¡°ê±´ ë° ë¶€ì„œ ì¡°ê±´ ì§€ì› + ì •í™•í•œ ì„œë¹„ìŠ¤ëª… í•„í„°ë§ ê°•í™”"""
        with st.chat_message("assistant"):
            # ì‹œê°„ ì¡°ê±´ ì¶”ì¶œ
            time_conditions = self.extract_time_conditions(query)
            
            # ë¶€ì„œ ì¡°ê±´ ì¶”ì¶œ
            department_conditions = self.extract_department_conditions(query)
            
            # 1ë‹¨ê³„: LLM ê¸°ë°˜ ì¿¼ë¦¬ íƒ€ì… ìë™ ë¶„ë¥˜
            if query_type is None:
                with st.spinner("ğŸ” ì§ˆë¬¸ ë¶„ì„ ì¤‘..."):
                    query_type = self.classify_query_type_with_llm(query)
                    
                # ì²˜ë¦¬ ë°©ì‹ ì•ˆë‚´ (ê°„ì†Œí™”)
                if self.debug_mode:
                    if query_type in ['repair', 'cause']:
                        st.info(f"ğŸ” ì§ˆë¬¸ ìœ í˜•: **{query_type.upper()}** (ğŸ¯ ì •í™•ì„± ìš°ì„  ì²˜ë¦¬ - LLM ê²€ì¦ ì ìš©)")
                    else:
                        st.info(f"ğŸ” ì§ˆë¬¸ ìœ í˜•: **{query_type.upper()}** (ğŸ“‹ í¬ê´„ì„± ìš°ì„  ì²˜ë¦¬ - ê´‘ë²”ìœ„í•œ ê²€ìƒ‰)")
            
            # ì‹œê°„ ì¡°ê±´ ì•ˆë‚´ (ê°„ì†Œí™”)
            if time_conditions.get('is_time_query') and self.debug_mode:
                time_desc = []
                if time_conditions.get('daynight'):
                    time_desc.append(f"ì‹œê°„ëŒ€: {time_conditions['daynight']}")
                if time_conditions.get('week'):
                    time_desc.append(f"ìš”ì¼: {time_conditions['week']}")
                st.info(f"â° ì‹œê°„ ì¡°ê±´ ê°ì§€: {', '.join(time_desc)}")
            
            # ë¶€ì„œ ì¡°ê±´ ì•ˆë‚´ (ê°„ì†Œí™”)
            if department_conditions.get('is_department_query') and self.debug_mode:
                if department_conditions.get('owner_depart'):
                    st.info(f"ğŸ¢ ë¶€ì„œ ì¡°ê±´ ê°ì§€: {department_conditions['owner_depart']}")
                else:
                    st.info(f"ğŸ¢ ë¶€ì„œë³„ ì¡°íšŒ ìš”ì²­ ê°ì§€")
            
            # 2ë‹¨ê³„: ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ
            target_service_name = self.search_manager.extract_service_name_from_query(query)
            if target_service_name and self.debug_mode:
                st.info(f"ğŸ·ï¸ ì¶”ì¶œëœ ì„œë¹„ìŠ¤ëª…: **{target_service_name}**")
            
            # 3ë‹¨ê³„: ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì í™”ëœ ê²€ìƒ‰ ìˆ˜í–‰
            with st.spinner("ğŸ”„ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
                documents = self.search_manager.semantic_search_with_adaptive_filtering(
                    query, target_service_name, query_type
                )
                
                if documents:
                    # **ìˆ˜ì •: ì •í™•í•œ ì„œë¹„ìŠ¤ëª… í•„í„°ë§ ì¶”ê°€**
                    if target_service_name:
                        original_count = len(documents)
                        documents = self.validate_service_specific_documents(documents, target_service_name)
                        filtered_count = len(documents)
                        
                        if self.debug_mode and filtered_count < original_count:
                            excluded_count = original_count - filtered_count
                            st.info(f"ğŸ¯ ì„œë¹„ìŠ¤ëª… ì •í™• ë§¤ì¹­: {target_service_name} ì„œë¹„ìŠ¤ë§Œ {filtered_count}ê°œ ì„ ë³„ ({excluded_count}ê°œ ì œì™¸)")
                    
                    # ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ë¶„ì„ (ê°„ì†Œí™”)
                    premium_count = sum(1 for doc in documents if doc.get('quality_tier') == 'Premium')
                    standard_count = sum(1 for doc in documents if doc.get('quality_tier') == 'Standard')
                    basic_count = sum(1 for doc in documents if doc.get('quality_tier') == 'Basic')
                    
                    # ì§‘ê³„ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
                    is_count_query = any(keyword in query.lower() for keyword in ['ê±´ìˆ˜', 'ê°œìˆ˜', 'ëª‡ê±´', 'ë…„ë„ë³„', 'ì›”ë³„', 'í†µê³„', 'í˜„í™©'])
                    
                    # ì§‘ê³„ ë¯¸ë¦¬ë³´ê¸° (ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ)
                    if is_count_query and self.debug_mode:
                        yearly_stats = {}
                        time_stats = {'daynight': {}, 'week': {}}
                        service_stats = {}
                        
                        for doc in documents:
                            error_date = doc.get('error_date', '')
                            year_from_date = None
                            if error_date and len(error_date) >= 4:
                                try:
                                    year_from_date = int(error_date[:4])
                                except:
                                    pass
                            
                            year_from_field = doc.get('year', '')
                            if year_from_field:
                                try:
                                    year_from_field = int(year_from_field)
                                except:
                                    year_from_field = None
                            
                            final_year = year_from_date or year_from_field
                            if final_year:
                                yearly_stats[final_year] = yearly_stats.get(final_year, 0) + 1
                            
                            # ì‹œê°„ í†µê³„
                            daynight = doc.get('daynight', '')
                            if daynight:
                                time_stats['daynight'][daynight] = time_stats['daynight'].get(daynight, 0) + 1
                            
                            week = doc.get('week', '')
                            if week:
                                time_stats['week'][week] = time_stats['week'].get(week, 0) + 1
                            
                            # ì„œë¹„ìŠ¤ë³„ í†µê³„
                            service_name = doc.get('service_name', '')
                            if service_name:
                                service_stats[service_name] = service_stats.get(service_name, 0) + 1
                        
                        yearly_total = sum(yearly_stats.values())
                        processing_method = "ì •í™•ì„± ìš°ì„ " if query_type in ['repair', 'cause'] else "í¬ê´„ì„± ìš°ì„ "
                        
                        preview_info = f"""
                        ğŸ“Š ì§‘ê³„ ë¯¸ë¦¬ë³´ê¸° ({processing_method} ì²˜ë¦¬ ì˜ˆì •)
                        - ì „ì²´ ê±´ìˆ˜: {len(documents)}ê±´
                        - ë…„ë„ë³„ ë¶„í¬: {dict(sorted(yearly_stats.items()))}
                        - ë…„ë„ë³„ í•©ê³„: {yearly_total}ê±´
                        - ê²€ì¦ ìƒíƒœ: {'ì¼ì¹˜' if yearly_total == len(documents) else 'ë¶ˆì¼ì¹˜'}
                        """
                        
                        if target_service_name and service_stats:
                            preview_info += f"\n                        - ì„œë¹„ìŠ¤ë³„ ë¶„í¬: {dict(sorted(service_stats.items(), key=lambda x: x[1], reverse=True))}"
                        
                        if time_conditions.get('is_time_query') and (time_stats['daynight'] or time_stats['week']):
                            if time_stats['daynight']:
                                preview_info += f"\n                        - ì‹œê°„ëŒ€ë³„ ë¶„í¬: {time_stats['daynight']}"
                            if time_stats['week']:
                                preview_info += f"\n                        - ìš”ì¼ë³„ ë¶„í¬: {time_stats['week']}"
                        
                        st.info(preview_info)
                    
                    # ê°„ì†Œí™”ëœ ì„±ê³µ ë©”ì‹œì§€
                    #if premium_count + standard_count + basic_count > 0:
                    #    st.success(f"âœ… {len(documents)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                    
                    # ê²€ìƒ‰ëœ ë¬¸ì„œ ìƒì„¸ í‘œì‹œ (ì„ íƒì )
                    with st.expander("ğŸ“„ ë§¤ì¹­ëœ ë¬¸ì„œ ìƒì„¸ ë³´ê¸°"):
                        self.ui_components.display_documents_with_quality_info(documents)
                    
                    # 4ë‹¨ê³„: ì ì‘í˜• RAG ì‘ë‹µ ìƒì„±
                    with st.spinner("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘..."):
                        response = self.generate_rag_response_with_adaptive_processing(
                            query, documents, query_type, time_conditions, department_conditions
                        )
                        
                        # ê¹”ë”í•œ ë‹µë³€ í‘œì‹œ
                        st.write(response)
                        
                        st.session_state.messages.append({"role": "assistant", "content": response})
                        
                else:
                    # 5ë‹¨ê³„: ëŒ€ì²´ ê²€ìƒ‰ ì‹œë„
                    with st.spinner("ğŸ”„ ì¶”ê°€ ê²€ìƒ‰ ì¤‘..."):
                        fallback_documents = self.search_manager.search_documents_fallback(query, target_service_name)
                        
                        if fallback_documents:
                            if self.debug_mode:
                                st.info(f"ğŸ”„ ëŒ€ì²´ ê²€ìƒ‰ìœ¼ë¡œ {len(fallback_documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
                            
                            response = self.generate_rag_response_with_adaptive_processing(
                                query, fallback_documents, query_type, time_conditions, department_conditions
                            )
                            st.write(response)
                            
                            st.session_state.messages.append({"role": "assistant", "content": response})
                        else:
                            self._show_no_results_message(target_service_name, query_type, time_conditions)
    
    def _show_no_results_message(self, target_service_name, query_type, time_conditions=None):
        """ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê°œì„  ë°©ì•ˆ ì œì‹œ - ì‹œê°„ ì¡°ê±´ ì•ˆë‚´ í¬í•¨"""
        time_condition_desc = ""
        if time_conditions and time_conditions.get('is_time_query'):
            time_desc = []
            if time_conditions.get('daynight'):
                time_desc.append(f"ì‹œê°„ëŒ€: {time_conditions['daynight']}")
            if time_conditions.get('week'):
                time_desc.append(f"ìš”ì¼: {time_conditions['week']}")
            time_condition_desc = f" ({', '.join(time_desc)} ì¡°ê±´)"
        
        error_msg = f"""
        '{target_service_name or 'í•´ë‹¹ ì¡°ê±´'}{time_condition_desc}'ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
        
        **ğŸ”§ ê°œì„  ë°©ì•ˆ:**
        - ì„œë¹„ìŠ¤ëª…ì˜ ì¼ë¶€ë§Œ ì…ë ¥í•´ë³´ì„¸ìš” (ì˜ˆ: 'API' ëŒ€ì‹  'API_Link')
        - ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”
        - ì „ì²´ ê²€ìƒ‰ì„ ì›í•˜ì‹œë©´ ì„œë¹„ìŠ¤ëª…ì„ ì œì™¸í•˜ê³  ê²€ìƒ‰í•´ì£¼ì„¸ìš”
        - ë” ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”
        
        **ì‹œê°„ ì¡°ê±´ ê´€ë ¨ ê°œì„  ë°©ì•ˆ:**
        - ì‹œê°„ëŒ€ ì¡°ê±´ì„ ì œê±°í•´ë³´ì„¸ìš” (ì£¼ê°„/ì•¼ê°„)
        - ìš”ì¼ ì¡°ê±´ì„ ì œê±°í•´ë³´ì„¸ìš”
        - ë” ë„“ì€ ì‹œê°„ ë²”ìœ„ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”
        
        **ğŸ’¡ {query_type.upper()} ì¿¼ë¦¬ ìµœì í™” íŒ:**
        """
        
        # ì¿¼ë¦¬ íƒ€ì…ë³„ ê°œì„  íŒ ì¶”ê°€
        if query_type == 'repair':
            error_msg += """
        - ì„œë¹„ìŠ¤ëª…ê³¼ ì¥ì• í˜„ìƒì„ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”
        - 'ë³µêµ¬ë°©ë²•', 'í•´ê²°ë°©ë²•' í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”
        """
        elif query_type == 'cause':
            error_msg += """
        - 'ì›ì¸', 'ì´ìœ ', 'ì™œ' ë“±ì˜ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”
        - ì¥ì•  í˜„ìƒì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
        """
        elif query_type == 'similar':
            error_msg += """
        - 'ìœ ì‚¬', 'ë¹„ìŠ·í•œ', 'ë™ì¼í•œ' í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”
        - í•µì‹¬ ì¥ì•  í˜„ìƒë§Œ ê°„ê²°í•˜ê²Œ ê¸°ìˆ í•˜ì„¸ìš”
        """
        else:
            error_msg += """
        - í†µê³„ë‚˜ í˜„í™© ì¡°íšŒ ì‹œ ê¸°ê°„ì„ ëª…ì‹œí•˜ì„¸ìš”
        - 'ê±´ìˆ˜', 'í†µê³„', 'í˜„í™©' ë“±ì˜ í‚¤ì›Œë“œë¥¼ í™œìš©í•˜ì„¸ìš”
        """
        
        st.write(error_msg)
        st.session_state.messages.append({"role": "assistant", "content": error_msg})