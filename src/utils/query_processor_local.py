import streamlit as st
import re
import time
import os
import json
from datetime import datetime
from config.prompts import SystemPrompts
from config.settings_local import AppConfigLocal
from utils.search_utils_local import SearchManagerLocal
from utils.ui_components_local import UIComponentsLocal
from utils.reprompting_db_manager import RepromptingDBManager
from utils.chart_utils import ChartManager

# MonitoringManager import
try:
    from utils.monitoring_manager import MonitoringManager
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False
    class MonitoringManager:
        def __init__(self, *args, **kwargs): pass
        def log_user_activity(self, *args, **kwargs): pass

LANGSMITH_ENABLED = os.getenv('LANGSMITH_TRACING', 'false').lower() == 'true'

if LANGSMITH_ENABLED:
    try:
        from langsmith import traceable, trace
        from langsmith.wrappers import wrap_openai
        LANGSMITH_AVAILABLE = True
    except ImportError:
        LANGSMITH_AVAILABLE = False
        LANGSMITH_ENABLED = False
        def traceable(name=None, **kwargs):
            def decorator(func): return func
            return decorator
        def trace(name=None, **kwargs):
            class DummyTrace:
                def __enter__(self): return self
                def __exit__(self, *args): pass
                def update(self, **kwargs): pass
            return DummyTrace()
else:
    LANGSMITH_AVAILABLE = False
    def traceable(name=None, **kwargs):
        def decorator(func): return func
        return decorator
    def trace(name=None, **kwargs):
        class DummyTrace:
            def __enter__(self): return self
            def __exit__(self, *args): pass
            def update(self, **kwargs): pass
        return DummyTrace()

class StatisticsValidator:
    def __init__(self):
        self.validation_errors = []
        self.validation_warnings = []
    
    def validate_document(self, doc, doc_index):
        errors, warnings = [], []
        required_fields = ['incident_id', 'service_name', 'error_date']
        for field in required_fields:
            if not doc.get(field):
                errors.append(f"ë¬¸ì„œ {doc_index}: {field} í•„ë“œê°€ ë¹„ì–´ìˆìŒ")
        
        error_time = doc.get('error_time')
        if error_time is not None:
            try:
                error_time_int = int(error_time)
                if error_time_int < 0:
                    warnings.append(f"ë¬¸ì„œ {doc_index}: error_timeì´ ìŒìˆ˜")
                elif error_time_int > 10080:
                    warnings.append(f"ë¬¸ì„œ {doc_index}: error_timeì´ ë¹„ì •ìƒì ìœ¼ë¡œ í¼")
            except (ValueError, TypeError):
                errors.append(f"ë¬¸ì„œ {doc_index}: error_time í˜•ì‹ ì˜¤ë¥˜")
        return errors, warnings
    
    def validate_statistics_result(self, stats, original_doc_count):
        errors, warnings = [], []
        total_count = stats.get('total_count', 0)
        if total_count != original_doc_count:
            errors.append(f"ì´ ê°œìˆ˜ ë¶ˆì¼ì¹˜: ê³„ì‚°({total_count}) != ì›ë³¸({original_doc_count})")
        return errors, warnings

class DataNormalizer:
    @staticmethod
    def normalize_error_time(error_time):
        if error_time is None: return 0
        try:
            if isinstance(error_time, str):
                error_time = error_time.strip()
                if error_time == '' or error_time.lower() in ['null', 'none', 'n/a']:
                    return 0
                return int(float(error_time))
            return int(error_time)
        except (ValueError, TypeError):
            return 0
    
    @staticmethod
    def normalize_date_fields(doc):
        normalized_doc = doc.copy()
        error_date = doc.get('error_date', '')
        
        if error_date:
            try:
                error_date_str = str(error_date).strip()
                if '-' in error_date_str and len(error_date_str) >= 7:
                    parts = error_date_str.split('-')
                    if len(parts) >= 2:
                        if parts[0].isdigit() and len(parts[0]) == 4:
                            normalized_doc['extracted_year'] = parts[0]
                        if parts[1].isdigit():
                            month_num = int(parts[1])
                            if 1 <= month_num <= 12:
                                normalized_doc['extracted_month'] = str(month_num)
                elif len(error_date_str) >= 8 and error_date_str.isdigit():
                    normalized_doc['extracted_year'] = error_date_str[:4]
                    month_str = error_date_str[4:6]
                    try:
                        month_num = int(month_str)
                        if 1 <= month_num <= 12:
                            normalized_doc['extracted_month'] = str(month_num)
                    except (ValueError, TypeError):
                        pass
                elif len(error_date_str) >= 4 and error_date_str[:4].isdigit():
                    normalized_doc['extracted_year'] = error_date_str[:4]
            except (ValueError, TypeError):
                pass
        
        if not normalized_doc.get('year') and normalized_doc.get('extracted_year'):
            normalized_doc['year'] = normalized_doc['extracted_year']
        if not normalized_doc.get('month') and normalized_doc.get('extracted_month'):
            normalized_doc['month'] = normalized_doc['extracted_month']
        
        if normalized_doc.get('year'):
            normalized_doc['year'] = str(normalized_doc['year']).strip()
        if normalized_doc.get('month'):
            try:
                month_val = int(normalized_doc['month'])
                if 1 <= month_val <= 12:
                    normalized_doc['month'] = str(month_val)
                else:
                    normalized_doc['month'] = ''
            except (ValueError, TypeError):
                normalized_doc['month'] = ''
        
        return normalized_doc
    
    @staticmethod
    def normalize_document(doc):
        normalized_doc = DataNormalizer.normalize_date_fields(doc)
        normalized_doc['error_time'] = DataNormalizer.normalize_error_time(doc.get('error_time'))
        
        string_fields = ['service_name', 'incident_grade', 'owner_depart', 'daynight', 'week']
        for field in string_fields:
            value = normalized_doc.get(field)
            normalized_doc[field] = str(value).strip() if value else ''
        
        return normalized_doc

class ImprovedStatisticsCalculator:
    def __init__(self, remove_duplicates=False):
        self.validator = StatisticsValidator()
        self.normalizer = DataNormalizer()
        self.remove_duplicates = remove_duplicates
    
    def _extract_filter_conditions(self, query):
        conditions = {
            'year': None, 'month': None, 'start_month': None, 'end_month': None,
            'daynight': None, 'week': None, 'service_name': None, 'department': None, 'grade': None
        }
        
        if not query: return conditions
        query_lower = query.lower()
        
        year_match = re.search(r'\b(202[0-9]|201[0-9])\b', query_lower)
        if year_match:
            conditions['year'] = year_match.group(1)
        
        month_range_patterns = [
            r'\b(\d+)\s*~\s*(\d+)ì›”\b', r'\b(\d+)ì›”\s*~\s*(\d+)ì›”\b',
            r'\b(\d+)\s*-\s*(\d+)ì›”\b', r'\b(\d+)ì›”\s*-\s*(\d+)ì›”\b'
        ]
        
        month_range_found = False
        for pattern in month_range_patterns:
            month_range_match = re.search(pattern, query_lower)
            if month_range_match:
                start_month = int(month_range_match.group(1))
                end_month = int(month_range_match.group(2))
                if 1 <= start_month <= 12 and 1 <= end_month <= 12 and start_month <= end_month:
                    conditions['start_month'] = start_month
                    conditions['end_month'] = end_month
                    month_range_found = True
                    break
        
        if not month_range_found:
            month_match = re.search(r'\b(\d{1,2})ì›”\b', query_lower)
            if month_match:
                month_num = int(month_match.group(1))
                if 1 <= month_num <= 12:
                    conditions['month'] = str(month_num)
        
        if any(word in query_lower for word in ['ì•¼ê°„', 'ë°¤', 'ìƒˆë²½', 'ì‹¬ì•¼']):
            conditions['daynight'] = 'ì•¼ê°„'
        elif any(word in query_lower for word in ['ì£¼ê°„', 'ë‚®', 'ì˜¤ì „', 'ì˜¤í›„']):
            conditions['daynight'] = 'ì£¼ê°„'
        
        week_patterns = {
            'ì›”': ['ì›”ìš”ì¼', 'ì›”'], 'í™”': ['í™”ìš”ì¼', 'í™”'], 'ìˆ˜': ['ìˆ˜ìš”ì¼', 'ìˆ˜'],
            'ëª©': ['ëª©ìš”ì¼', 'ëª©'], 'ê¸ˆ': ['ê¸ˆìš”ì¼', 'ê¸ˆ'], 'í† ': ['í† ìš”ì¼', 'í† '],
            'ì¼': ['ì¼ìš”ì¼', 'ì¼']
        }
        
        for week_key, patterns in week_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                conditions['week'] = week_key
                break
        
        if 'í‰ì¼' in query_lower:
            conditions['week'] = 'í‰ì¼'
        elif 'ì£¼ë§' in query_lower:
            conditions['week'] = 'ì£¼ë§'
        
        grade_match = re.search(r'(\d+)ë“±ê¸‰', query_lower)
        if grade_match:
            conditions['grade'] = f"{grade_match.group(1)}ë“±ê¸‰"
        
        return conditions
    
    def _validate_document_against_conditions(self, doc, conditions):
        incident_id = doc.get('incident_id', 'N/A')
        
        if conditions['year']:
            doc_year = self._extract_year_from_document(doc)
            if not doc_year or doc_year != conditions['year']:
                return False, f"year mismatch"
        
        if conditions['start_month'] and conditions['end_month']:
            doc_month = self._extract_month_from_document(doc)
            if not doc_month:
                return False, "no month information"
            try:
                month_num = int(doc_month)
                if not (conditions['start_month'] <= month_num <= conditions['end_month']):
                    return False, f"month not in range"
            except (ValueError, TypeError):
                return False, f"invalid month format"
        elif conditions['month']:
            doc_month = self._extract_month_from_document(doc)
            if not doc_month or str(doc_month) != conditions['month']:
                return False, f"month mismatch"
        
        if conditions['daynight']:
            doc_daynight = doc.get('daynight', '').strip()
            if not doc_daynight or doc_daynight != conditions['daynight']:
                return False, f"daynight mismatch"
        
        if conditions['week']:
            doc_week = doc.get('week', '').strip()
            required_week = conditions['week']
            if required_week == 'í‰ì¼':
                if doc_week not in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ']:
                    return False, f"not weekday"
            elif required_week == 'ì£¼ë§':
                if doc_week not in ['í† ', 'ì¼']:
                    return False, f"not weekend"
            else:
                if not doc_week or doc_week != required_week:
                    return False, f"week mismatch"
        
        if conditions['grade']:
            doc_grade = doc.get('incident_grade', '')
            if doc_grade != conditions['grade']:
                return False, f"grade mismatch"
        
        return True, "passed"
    
    def _extract_year_from_document(self, doc):
        year = doc.get('year')
        if year:
            year_str = str(year).strip()
            if len(year_str) == 4 and year_str.isdigit():
                return year_str
        
        year = doc.get('extracted_year')
        if year:
            year_str = str(year).strip()
            if len(year_str) == 4 and year_str.isdigit():
                return year_str
        
        error_date = doc.get('error_date', '')
        if error_date:
            error_date_str = str(error_date).strip()
            if len(error_date_str) >= 4:
                if '-' in error_date_str:
                    parts = error_date_str.split('-')
                    if len(parts) > 0 and len(parts[0]) == 4 and parts[0].isdigit():
                        return parts[0]
                elif len(error_date_str) >= 8 and error_date_str[:4].isdigit():
                    return error_date_str[:4]
                elif len(error_date_str) >= 4 and error_date_str[:4].isdigit():
                    return error_date_str[:4]
        return None
    
    def _extract_month_from_document(self, doc):
        month = doc.get('month')
        if month:
            try:
                month_num = int(month)
                if 1 <= month_num <= 12:
                    return str(month_num)
            except (ValueError, TypeError):
                pass
        
        month = doc.get('extracted_month')
        if month:
            try:
                month_num = int(month)
                if 1 <= month_num <= 12:
                    return str(month_num)
            except (ValueError, TypeError):
                pass
        
        error_date = doc.get('error_date', '')
        if error_date:
            error_date_str = str(error_date).strip()
            if '-' in error_date_str:
                parts = error_date_str.split('-')
                if len(parts) >= 2 and parts[1].isdigit():
                    try:
                        month_num = int(parts[1])
                        if 1 <= month_num <= 12:
                            return str(month_num)
                    except (ValueError, TypeError):
                        pass
            elif len(error_date_str) >= 6 and error_date_str.isdigit():
                try:
                    month_num = int(error_date_str[4:6])
                    if 1 <= month_num <= 12:
                        return str(month_num)
                except (ValueError, TypeError):
                    pass
        return None
    
    def _apply_filters(self, documents, conditions):
        filtered_docs = []
        for doc in documents:
            is_valid, reason = self._validate_document_against_conditions(doc, conditions)
            if is_valid:
                filtered_docs.append(doc)
        return filtered_docs
    
    def _empty_statistics(self):
        return {
            'total_count': 0, 'yearly_stats': {}, 'monthly_stats': {},
            'time_stats': {'daynight': {}, 'week': {}}, 'department_stats': {},
            'service_stats': {}, 'grade_stats': {}, 'is_error_time_query': False,
            'validation': {'errors': [], 'warnings': [], 'is_valid': True},
            'primary_stat_type': None
        }
    
    def _is_error_time_query(self, query):
        if not query: return False
        error_time_keywords = ['ì¥ì• ì‹œê°„', 'ì¥ì•  ì‹œê°„', 'error_time', 'ì‹œê°„ í†µê³„', 'ì‹œê°„ í•©ê³„', 'ì‹œê°„ í•©ì‚°', 'ë¶„']
        return any(keyword in query.lower() for keyword in error_time_keywords)
    
    def _determine_primary_stat_type(self, query, yearly_stats, monthly_stats, time_stats, service_stats, department_stats, grade_stats):
        """ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” í†µê³„ ìœ í˜• ê²°ì •"""
        if not query:
            # ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ë°ì´í„°ê°€ ê°€ì¥ ë§ì€ í†µê³„ ì‚¬ìš©
            stat_counts = {
                'yearly': len(yearly_stats),
                'monthly': len(monthly_stats),
                'service': len(service_stats),
                'department': len(department_stats),
                'grade': len(grade_stats),
                'time': len(time_stats.get('daynight', {})) + len(time_stats.get('week', {}))
            }
            return max(stat_counts.items(), key=lambda x: x[1])[0] if any(stat_counts.values()) else None
        
        query_lower = query.lower()
        
        # ëª…ì‹œì  í‚¤ì›Œë“œ í™•ì¸ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
        if any(kw in query_lower for kw in ['ì—°ë„ë³„', 'ë…„ë„ë³„', 'ë…„ë³„', 'ì—°ë³„']):
            return 'yearly'
        elif any(kw in query_lower for kw in ['ì›”ë³„']) or re.search(r'\b\d+ì›”\b', query_lower):
            return 'monthly'
        elif any(kw in query_lower for kw in ['ì‹œê°„ëŒ€ë³„', 'ì£¼ê°„', 'ì•¼ê°„']):
            return 'time'
        elif any(kw in query_lower for kw in ['ìš”ì¼ë³„']):
            return 'weekday'
        elif any(kw in query_lower for kw in ['ë¶€ì„œë³„', 'íŒ€ë³„']):
            return 'department'
        elif any(kw in query_lower for kw in ['ì„œë¹„ìŠ¤ë³„']):
            return 'service'
        elif any(kw in query_lower for kw in ['ë“±ê¸‰ë³„']):
            return 'grade'
        
        # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ë°ì´í„°ê°€ ê°€ì¥ ë§ì€ í†µê³„ ì‚¬ìš©
        stat_counts = {
            'yearly': len(yearly_stats),
            'monthly': len(monthly_stats),
            'service': len(service_stats),
            'department': len(department_stats),
            'grade': len(grade_stats),
            'time': len(time_stats.get('daynight', {})) + len(time_stats.get('week', {}))
        }
        return max(stat_counts.items(), key=lambda x: x[1])[0] if any(stat_counts.values()) else 'yearly'
    
    def _calculate_detailed_statistics(self, documents, conditions, is_error_time_query):
        stats = {
            'total_count': len(documents), 'yearly_stats': {}, 'monthly_stats': {},
            'time_stats': {'daynight': {}, 'week': {}}, 'department_stats': {},
            'service_stats': {}, 'grade_stats': {}, 'is_error_time_query': is_error_time_query,
            'filter_conditions': conditions, 'calculation_details': {}
        }
        
        # ì—°ë„ë³„ í†µê³„ (ì •ë ¬ì„ ìœ„í•´ ì„ì‹œ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©)
        yearly_temp = {}
        for doc in documents:
            year = self._extract_year_from_document(doc)
            if year:
                if is_error_time_query:
                    error_time = doc.get('error_time', 0)
                    yearly_temp[year] = yearly_temp.get(year, 0) + error_time
                else:
                    yearly_temp[year] = yearly_temp.get(year, 0) + 1
        
        # ì—°ë„ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
        for year in sorted(yearly_temp.keys()):
            stats['yearly_stats'][f"{year}ë…„"] = yearly_temp[year]
        
        # ì›”ë³„ í†µê³„ (1ì›”~12ì›” ìˆœì„œ ë³´ì¥)
        monthly_temp = {}
        for doc in documents:
            month = self._extract_month_from_document(doc)
            if month:
                try:
                    month_num = int(month)
                    if 1 <= month_num <= 12:
                        if is_error_time_query:
                            error_time = doc.get('error_time', 0)
                            monthly_temp[month_num] = monthly_temp.get(month_num, 0) + error_time
                        else:
                            monthly_temp[month_num] = monthly_temp.get(month_num, 0) + 1
                except (ValueError, TypeError):
                    continue
        
        # ì›” ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (1ì›”ë¶€í„° 12ì›”ê¹Œì§€)
        for month_num in sorted(monthly_temp.keys()):
            stats['monthly_stats'][f"{month_num}ì›”"] = monthly_temp[month_num]
        
        # ì‹œê°„ëŒ€/ìš”ì¼/ë¶€ì„œ/ì„œë¹„ìŠ¤/ë“±ê¸‰ë³„ í†µê³„
        daynight_temp = {}
        week_temp = {}
        department_temp = {}
        service_temp = {}
        grade_temp = {}
        
        for doc in documents:
            daynight = doc.get('daynight', '')
            week = doc.get('week', '')
            department = doc.get('owner_depart', '')
            service = doc.get('service_name', '')
            grade = doc.get('incident_grade', '')
            error_time = doc.get('error_time', 0) if is_error_time_query else 1
            
            if daynight:
                daynight_temp[daynight] = daynight_temp.get(daynight, 0) + error_time
            if week:
                week_temp[week] = week_temp.get(week, 0) + error_time
            if department:
                department_temp[department] = department_temp.get(department, 0) + error_time
            if service:
                service_temp[service] = service_temp.get(service, 0) + error_time
            if grade:
                grade_temp[grade] = grade_temp.get(grade, 0) + error_time
        
        # ì‹œê°„ëŒ€ ì •ë ¬ (ì£¼ê°„ ë¨¼ì €)
        time_order = ['ì£¼ê°„', 'ì•¼ê°„']
        for time_key in time_order:
            if time_key in daynight_temp:
                stats['time_stats']['daynight'][time_key] = daynight_temp[time_key]
        
        # ìš”ì¼ ì •ë ¬ (ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼ ìˆœì„œ)
        week_order = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼', 'í‰ì¼', 'ì£¼ë§']
        for week_key in week_order:
            if week_key in week_temp:
                week_display = f"{week_key}ìš”ì¼" if week_key in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼'] else week_key
                stats['time_stats']['week'][week_display] = week_temp[week_key]
        
        # ë¶€ì„œë³„ ì •ë ¬ (ê°’ ë‚´ë¦¼ì°¨ìˆœ, ìƒìœ„ 10ê°œ)
        sorted_departments = sorted(department_temp.items(), key=lambda x: x[1], reverse=True)[:10]
        stats['department_stats'] = dict(sorted_departments)
        
        # ì„œë¹„ìŠ¤ë³„ ì •ë ¬ (ê°’ ë‚´ë¦¼ì°¨ìˆœ, ìƒìœ„ 10ê°œ)
        sorted_services = sorted(service_temp.items(), key=lambda x: x[1], reverse=True)[:10]
        stats['service_stats'] = dict(sorted_services)
        
        # ë“±ê¸‰ë³„ ì •ë ¬ (1ë“±ê¸‰, 2ë“±ê¸‰, 3ë“±ê¸‰, 4ë“±ê¸‰ ìˆœì„œ)
        grade_order = ['1ë“±ê¸‰', '2ë“±ê¸‰', '3ë“±ê¸‰', '4ë“±ê¸‰']
        for grade_key in grade_order:
            if grade_key in grade_temp:
                stats['grade_stats'][grade_key] = grade_temp[grade_key]
        # ê·¸ ì™¸ ë“±ê¸‰
        for grade_key, value in sorted(grade_temp.items()):
            if grade_key not in stats['grade_stats']:
                stats['grade_stats'][grade_key] = value
        
        # ê³„ì‚° ì„¸ë¶€ì‚¬í•­
        total_error_time = sum(doc.get('error_time', 0) for doc in documents)
        stats['calculation_details'] = {
            'total_error_time_minutes': total_error_time,
            'total_error_time_hours': round(total_error_time / 60, 2),
            'average_error_time': round(total_error_time / len(documents), 2) if documents else 0,
            'max_error_time': max((doc.get('error_time', 0) for doc in documents), default=0),
            'min_error_time': min((doc.get('error_time', 0) for doc in documents), default=0),
            'documents_with_error_time': len([doc for doc in documents if doc.get('error_time', 0) > 0])
        }
        
        # ì£¼ìš” í†µê³„ ìœ í˜• ê²°ì • - ì¿¼ë¦¬ ë‚´ìš© ì „ë‹¬ í•„ìš”
        stats['primary_stat_type'] = None  # ë‚˜ì¤‘ì— ì¿¼ë¦¬ì™€ í•¨ê»˜ ê²°ì •
        
        return stats
    
    def calculate_comprehensive_statistics(self, documents, query, query_type="default"):
        if not documents:
            return self._empty_statistics()
        
        # ë¬¸ì„œ ì •ê·œí™”
        normalized_docs = []
        validation_errors = []
        validation_warnings = []
        
        for i, doc in enumerate(documents):
            if doc is None: continue
            errors, warnings = self.validator.validate_document(doc, i)
            validation_errors.extend(errors)
            validation_warnings.extend(warnings)
            normalized_doc = self.normalizer.normalize_document(doc)
            normalized_docs.append(normalized_doc)
        
        # ì¤‘ë³µ ì œê±° (ì˜µì…˜)
        if self.remove_duplicates:
            unique_docs = {}
            for doc in normalized_docs:
                incident_id = doc.get('incident_id', '')
                if incident_id and incident_id not in unique_docs:
                    unique_docs[incident_id] = doc
            clean_documents = list(unique_docs.values())
        else:
            clean_documents = normalized_docs
        
        # í•„í„° ì¡°ê±´ ì¶”ì¶œ
        filter_conditions = self._extract_filter_conditions(query)
        
        # í†µê³„ì„± ì§ˆë¬¸ì˜ ê²½ìš° í•„í„°ë§ ìµœì†Œí™”
        is_stats_query = any(keyword in query.lower() for keyword in ['ê±´ìˆ˜', 'í†µê³„', 'ì—°ë„ë³„', 'ì›”ë³„', 'í˜„í™©', 'ë¶„í¬', 'ì•Œë ¤ì¤˜', 'ëª‡ê±´', 'ê°œìˆ˜'])
        
        if is_stats_query:
            filtered_docs = clean_documents
        else:
            filtered_docs = self._apply_filters(clean_documents, filter_conditions)
        
        # ì¥ì• ì‹œê°„ ì¿¼ë¦¬ ì—¬ë¶€ í™•ì¸
        is_error_time_query = self._is_error_time_query(query)
        
        # í†µê³„ ê³„ì‚°
        stats = self._calculate_detailed_statistics(filtered_docs, filter_conditions, is_error_time_query)
        
        # ì£¼ìš” í†µê³„ ìœ í˜• ê²°ì •
        stats['primary_stat_type'] = self._determine_primary_stat_type(
            query, 
            stats['yearly_stats'], 
            stats['monthly_stats'], 
            stats['time_stats'], 
            stats['service_stats'], 
            stats['department_stats'], 
            stats['grade_stats']
        )
        
        # ê²°ê³¼ ê²€ì¦
        result_errors, result_warnings = self.validator.validate_statistics_result(stats, len(filtered_docs))
        validation_errors.extend(result_errors)
        validation_warnings.extend(result_warnings)
        
        stats['validation'] = {
            'errors': validation_errors,
            'warnings': validation_warnings,
            'is_valid': len(validation_errors) == 0
        }
        
        return stats

class QueryProcessorLocal:
    def __init__(self, azure_openai_client, search_client, model_name, config=None):
        self.azure_openai_client = azure_openai_client
        self.search_client = search_client
        self.model_name = model_name
        self.config = config if config else AppConfigLocal()
        self.search_manager = SearchManagerLocal(search_client, self.config)
        self.ui_components = UIComponentsLocal()
        self.reprompting_db_manager = RepromptingDBManager()
        self.chart_manager = ChartManager()
        self.statistics_calculator = ImprovedStatisticsCalculator(remove_duplicates=False)
        self.debug_mode = True
        
        if MONITORING_AVAILABLE:
            try:
                self.monitoring_manager = MonitoringManager()
                self.monitoring_enabled = True
            except Exception:
                self.monitoring_manager = MonitoringManager()
                self.monitoring_enabled = False
        else:
            self.monitoring_manager = MonitoringManager()
            self.monitoring_enabled = False
        
        self.langsmith_enabled = LANGSMITH_ENABLED
        self._setup_langsmith()
    
    def _setup_langsmith(self):
        if not self.langsmith_enabled: return
        try:
            langsmith_status = self.config.get_langsmith_status()
            if langsmith_status['enabled'] and LANGSMITH_AVAILABLE:
                success = self.config.setup_langsmith()
                if success:
                    self.azure_openai_client = wrap_openai(self.azure_openai_client)
        except Exception:
            pass

    def safe_trace_update(self, trace_obj, **kwargs):
        if not self.langsmith_enabled: return
        try:
            if hasattr(trace_obj, 'update'):
                trace_obj.update(**kwargs)
            elif hasattr(trace_obj, 'add_outputs'):
                if 'outputs' in kwargs:
                    trace_obj.add_outputs(kwargs['outputs'])
                if 'metadata' in kwargs:
                    trace_obj.add_metadata(kwargs['metadata'])
        except Exception:
            pass

    def calculate_unified_statistics(self, documents, query, query_type="default"):
        if not documents:
            return self.statistics_calculator._empty_statistics()
        return self.statistics_calculator.calculate_comprehensive_statistics(documents, query, query_type)

    @traceable(name="check_reprompting_question")
    def check_and_transform_query_with_reprompting(self, user_query):
        if not user_query:
            return {'transformed': False, 'original_query': user_query, 'transformed_query': user_query, 'match_type': 'none'}
        
        start_time = time.time()
        
        with trace(name="reprompting_check", inputs={"user_query": user_query}) as trace_context:
            try:
                exact_result = self.reprompting_db_manager.check_reprompting_question(user_query)
                
                if exact_result['exists']:
                    result = {
                        'transformed': True,
                        'original_query': user_query,
                        'transformed_query': exact_result['custom_prompt'],
                        'question_type': exact_result['question_type'],
                        'wrong_answer_summary': exact_result['wrong_answer_summary'],
                        'match_type': 'exact'
                    }
                    if not self.debug_mode:
                        st.success("âœ… ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
                    return result
                
                similar_questions = self.reprompting_db_manager.find_similar_questions(user_query, similarity_threshold=0.7, limit=3)
                
                if similar_questions:
                    best_match = similar_questions[0]
                    db_question = best_match['question']
                    custom_prompt_for_part = best_match['custom_prompt']
                    
                    try:
                        transformed_query = re.sub(re.escape(db_question), custom_prompt_for_part, user_query, flags=re.IGNORECASE)
                    except:
                        transformed_query = user_query.replace(db_question, custom_prompt_for_part)
                    
                    is_transformed = transformed_query != user_query
                    
                    result = {
                        'transformed': is_transformed,
                        'original_query': user_query,
                        'transformed_query': transformed_query,
                        'question_type': best_match['question_type'],
                        'wrong_answer_summary': best_match['wrong_answer_summary'],
                        'similarity': best_match['similarity'],
                        'similar_question': best_match['question'],
                        'match_type': 'similar'
                    }
                    
                    if is_transformed and not self.debug_mode:
                        st.info(f"ğŸ“‹ ìœ ì‚¬ ì§ˆë¬¸ íŒ¨í„´ì„ ê°ì§€í•˜ì—¬ ì§ˆë¬¸ì„ ìµœì í™”í–ˆìŠµë‹ˆë‹¤.")
                    return result
                
                return {'transformed': False, 'original_query': user_query, 'transformed_query': user_query, 'match_type': 'none'}
                
            except Exception as e:
                return {'transformed': False, 'original_query': user_query, 'transformed_query': user_query, 'match_type': 'error', 'error': str(e)}
    
    def extract_time_conditions(self, query):
        if not query:
            return {'daynight': None, 'week': None, 'is_time_query': False}
        
        time_conditions = {'daynight': None, 'week': None, 'is_time_query': False}
        
        if any(keyword in query.lower() for keyword in ['ì•¼ê°„', 'ë°¤', 'ìƒˆë²½', 'ì‹¬ì•¼']):
            time_conditions['is_time_query'] = True
            time_conditions['daynight'] = 'ì•¼ê°„'
        elif any(keyword in query.lower() for keyword in ['ì£¼ê°„', 'ë‚®', 'ì˜¤ì „', 'ì˜¤í›„']):
            time_conditions['is_time_query'] = True
            time_conditions['daynight'] = 'ì£¼ê°„'
        
        week_map = {
            'ì›”ìš”ì¼': 'ì›”', 'í™”ìš”ì¼': 'í™”', 'ìˆ˜ìš”ì¼': 'ìˆ˜', 'ëª©ìš”ì¼': 'ëª©',
            'ê¸ˆìš”ì¼': 'ê¸ˆ', 'í† ìš”ì¼': 'í† ', 'ì¼ìš”ì¼': 'ì¼', 'í‰ì¼': 'í‰ì¼', 'ì£¼ë§': 'ì£¼ë§'
        }
        
        for keyword, value in week_map.items():
            if keyword in query.lower():
                time_conditions['is_time_query'] = True
                time_conditions['week'] = value
                break
        
        return time_conditions
    
    def extract_department_conditions(self, query):
        if not query:
            return {'owner_depart': None, 'is_department_query': False}
        
        department_conditions = {'owner_depart': None, 'is_department_query': False}
        
        department_keywords = ['ë‹´ë‹¹ë¶€ì„œ', 'ì¡°ì¹˜ë¶€ì„œ', 'ì²˜ë¦¬ë¶€ì„œ', 'ì±…ì„ë¶€ì„œ', 'ê´€ë¦¬ë¶€ì„œ', 'ë¶€ì„œ', 'íŒ€', 'ì¡°ì§']
        
        if any(keyword in query for keyword in department_keywords):
            department_conditions['is_department_query'] = True
        
        return department_conditions
    
    @traceable(name="classify_query_type")
    def classify_query_type_with_llm(self, query):
        if not query:
            return 'default'
        
        with trace(name="llm_query_classification", inputs={"query": query}) as trace_context:
            try:
                classification_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ì„¸ìš”.

ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬:
1. repair: ì„œë¹„ìŠ¤ëª…ê³¼ ì¥ì• í˜„ìƒì´ í¬í•¨ëœ ë³µêµ¬ë°©ë²• ë¬¸ì˜
2. cause: ì¥ì• ì›ì¸ ë¶„ì„ ë¬¸ì˜
3. similar: ì„œë¹„ìŠ¤ëª… ì—†ì´ ì¥ì• í˜„ìƒë§Œìœ¼ë¡œ ìœ ì‚¬ì‚¬ë¡€ ë¬¸ì˜
4. inquiry: íŠ¹ì • ì¡°ê±´ì˜ ì¥ì•  ë‚´ì—­ ì¡°íšŒ
5. statistics: í†µê³„ ì „ìš© ì§ˆë¬¸ (ê±´ìˆ˜, í†µê³„, í˜„í™©, ë¶„í¬ ë“±)
6. default: ê·¸ ì™¸

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì‘ë‹µ í˜•ì‹: repair, cause, similar, inquiry, statistics, default ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

                response = self.azure_openai_client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ IT ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": classification_prompt}
                    ],
                    temperature=0.1,
                    max_tokens=50
                )
                
                query_type = response.choices[0].message.content.strip().lower()
                
                if query_type not in ['repair', 'cause', 'similar', 'inquiry', 'statistics', 'default']:
                    query_type = 'default'
                
                return query_type
                
            except Exception:
                return 'default'

    def _generate_chart_title(self, query, stats):
        """í†µê³„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì°¨íŠ¸ ì œëª© ìƒì„±"""
        primary_type = stats.get('primary_stat_type', 'general')
        is_error_time = stats.get('is_error_time_query', False)
        
        title_map = {
            'yearly': 'ì—°ë„ë³„ ì¥ì•  ë°œìƒ í˜„í™©',
            'monthly': 'ì›”ë³„ ì¥ì•  ë°œìƒ í˜„í™©',
            'time': 'ì‹œê°„ëŒ€ë³„ ì¥ì•  ë°œìƒ ë¶„í¬',
            'weekday': 'ìš”ì¼ë³„ ì¥ì•  ë°œìƒ ë¶„í¬',
            'department': 'ë¶€ì„œë³„ ì¥ì•  ì²˜ë¦¬ í˜„í™©',
            'service': 'ì„œë¹„ìŠ¤ë³„ ì¥ì•  ë°œìƒ í˜„í™©',
            'grade': 'ì¥ì• ë“±ê¸‰ë³„ ë°œìƒ ë¹„ìœ¨',
            'general': 'ì¥ì•  ë°œìƒ í†µê³„'
        }
        
        base_title = title_map.get(primary_type, 'ì¥ì•  í†µê³„')
        
        # ì¥ì• ì‹œê°„ ì¿¼ë¦¬ë©´ ì œëª© ìˆ˜ì •
        if is_error_time:
            base_title = base_title.replace('ë°œìƒ', 'ì‹œê°„').replace('ê±´ìˆ˜', 'ì‹œê°„')
        
        # ì¿¼ë¦¬ì—ì„œ ì—°ë„ ì¶”ì¶œ
        if query:
            year_match = re.search(r'\b(202[0-9])\b', query)
            if year_match:
                base_title = f"{year_match.group(1)}ë…„ {base_title}"
        
        return base_title

    def _get_chart_data_from_stats(self, stats):
        """í†µê³„ ë°ì´í„°ì—ì„œ ì°¨íŠ¸ ë°ì´í„°ì™€ íƒ€ì… ê²°ì •"""
        primary_type = stats.get('primary_stat_type')
        
        if not primary_type:
            return None, None
        
        # í†µê³„ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ë°ì´í„°ì™€ ì°¨íŠ¸ íƒ€ì… ì„ íƒ
        if primary_type == 'yearly':
            data = stats.get('yearly_stats', {})
            chart_type = 'line' if len(data) > 1 else 'bar'
        elif primary_type == 'monthly':
            data = stats.get('monthly_stats', {})
            chart_type = 'line' if len(data) > 1 else 'bar'
        elif primary_type == 'time':
            time_stats = stats.get('time_stats', {})
            data = time_stats.get('daynight', {}) or time_stats.get('week', {})
            chart_type = 'bar'
        elif primary_type == 'weekday':
            data = stats.get('time_stats', {}).get('week', {})
            chart_type = 'bar'
        elif primary_type == 'department':
            data = stats.get('department_stats', {})
            # ìƒìœ„ 10ê°œë§Œ
            sorted_data = dict(sorted(data.items(), key=lambda x: x[1], reverse=True)[:10])
            data = sorted_data
            chart_type = 'horizontal_bar'
        elif primary_type == 'service':
            data = stats.get('service_stats', {})
            # ìƒìœ„ 10ê°œë§Œ
            sorted_data = dict(sorted(data.items(), key=lambda x: x[1], reverse=True)[:10])
            data = sorted_data
            chart_type = 'horizontal_bar'
        elif primary_type == 'grade':
            data = stats.get('grade_stats', {})
            chart_type = 'pie'
        else:
            # fallback
            data = stats.get('yearly_stats', {})
            chart_type = 'line'
        
        return data, chart_type

    def remove_text_charts_from_response(self, response_text):
        if not response_text:
            return response_text
        
        text_chart_patterns = [
            r'ê°\s*ì›”ë³„.*?ì°¨íŠ¸ë¡œ\s*ë‚˜íƒ€ë‚¼\s*ìˆ˜\s*ìˆìŠµë‹ˆë‹¤:.*?(?=\n\n|\n[^ì›”"\d]|$)',
            r'\d+ì›”:\s*[â–ˆâ–“â–’â–‘â–¬\*\-\|]+.*?(?=\n\n|\n[^ì›”"\d]|$)',
            r'\n.*[â–ˆâ–“â–’â–‘â–¬]{2,}.*\n',
            r'```[^`]*[â–ˆâ–“â–’â–‘â–¬\*\-\|]{2,}[^`]*```',
        ]
        
        cleaned_response = response_text
        for pattern in text_chart_patterns:
            cleaned_response = re.sub(pattern, '', cleaned_response, flags=re.MULTILINE | re.DOTALL)
        
        cleaned_response = re.sub(r'\n{3,}', '\n\n', cleaned_response)
        return cleaned_response.strip()

    def _extract_incident_id_sort_key(self, incident_id):
        if not incident_id:
            return 999999999999999
        try:
            if incident_id.startswith('INM') and len(incident_id) > 3:
                return int(incident_id[3:])
            return hash(incident_id) % 999999999999999
        except (ValueError, TypeError):
            return hash(str(incident_id)) % 999999999999999

    def _apply_default_sorting(self, documents):
        if not documents:
            return documents
        try:
            def default_sort_key(doc):
                error_date = doc.get('error_date', '1900-01-01') or '1900-01-01'
                try:
                    error_time_val = int(doc.get('error_time', 0) or 0)
                except (ValueError, TypeError):
                    error_time_val = 0
                incident_sort_key = self._extract_incident_id_sort_key(doc.get('incident_id', 'INM99999999999'))
                return (error_date, error_time_val, -incident_sort_key)
            
            documents.sort(key=default_sort_key, reverse=True)
        except Exception:
            pass
        return documents

    def detect_sorting_requirements(self, query):
        sort_info = {
            'requires_custom_sort': False, 'sort_field': None,
            'sort_direction': 'desc', 'sort_type': None,
            'limit': None, 'secondary_sort': 'default'
        }
        
        if not query:
            return sort_info
        
        query_lower = query.lower()
        
        error_time_patterns = [
            r'ì¥ì• ì‹œê°„.*(?:ê°€ì¥.*?ê¸´|ê¸´.*?ìˆœ|ì˜¤ë˜.*?ê±¸ë¦°|ìµœëŒ€|í°.*?ìˆœ)',
            r'(?:ìµœì¥|ìµœëŒ€|ê°€ì¥.*?ì˜¤ë˜).*ì¥ì• ',
            r'top.*\d+.*ì¥ì• ì‹œê°„',
        ]
        
        for pattern in error_time_patterns:
            if re.search(pattern, query_lower):
                sort_info['requires_custom_sort'] = True
                sort_info['sort_field'] = 'error_time'
                sort_info['sort_type'] = 'error_time'
                sort_info['sort_direction'] = 'desc'
                break
        
        top_match = re.search(r'top\s*(\d+)|ìƒìœ„\s*(\d+)', query_lower)
        if top_match:
            limit = int(top_match.group(1) or top_match.group(2))
            sort_info['limit'] = min(limit, 50)
            if not sort_info['requires_custom_sort']:
                sort_info['requires_custom_sort'] = True
                sort_info['sort_field'] = 'error_time'
                sort_info['sort_type'] = 'error_time'
        
        return sort_info

    def apply_custom_sorting(self, documents, sort_info):
        if not documents:
            return documents
        
        try:
            if sort_info['requires_custom_sort']:
                if sort_info['sort_type'] == 'error_time':
                    def error_time_sort_key(doc):
                        try:
                            error_time_val = int(doc.get('error_time', 0) or 0)
                        except (ValueError, TypeError):
                            error_time_val = 0
                        
                        error_date = doc.get('error_date', '1900-01-01')
                        incident_sort_key = self._extract_incident_id_sort_key(doc.get('incident_id', 'INM99999999999'))
                        
                        if sort_info['sort_direction'] == 'desc':
                            return (-error_time_val, error_date, incident_sort_key)
                        return (error_time_val, error_date, incident_sort_key)
                    
                    documents.sort(key=error_time_sort_key)
                
                if sort_info['limit']:
                    documents = documents[:sort_info['limit']]
            else:
                self._apply_default_sorting(documents)
            
            return documents
        except Exception:
            self._apply_default_sorting(documents)
            return documents

    @traceable(name="generate_rag_response")
    def generate_rag_response_with_adaptive_processing(self, query, documents, query_type="default", time_conditions=None, department_conditions=None, reprompting_info=None):
        if documents is None:
            documents = []
        
        if not documents:
            return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì–´ì„œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        with trace(name="adaptive_rag_processing", inputs={"query": query, "document_count": len(documents)}) as trace_context:
            try:
                start_time = time.time()

                # í†µê³„ ê³„ì‚°
                unified_stats = self.calculate_unified_statistics(documents, query, query_type)

                # ì°¨íŠ¸ ìƒì„± - í†µê³„ ë°ì´í„° ì§ì ‘ ì‚¬ìš©
                chart_fig = None
                chart_info = None
                
                # ëª…ì‹œì  ì°¨íŠ¸ ìš”ì²­ í™•ì¸
                chart_keywords = ['ì°¨íŠ¸', 'ê·¸ë˜í”„', 'ì‹œê°í™”', 'ê·¸ë ¤', 'ê·¸ë ¤ì¤˜', 'ë³´ì—¬ì¤˜', 'ì‹œê°ì ìœ¼ë¡œ', 'ë„í‘œ', 'ë„ì‹í™”']
                has_explicit_chart_request = any(keyword in query.lower() for keyword in chart_keywords)
                
                if has_explicit_chart_request and unified_stats.get('total_count', 0) > 0:
                    # í†µê³„ ë°ì´í„°ì—ì„œ ì°¨íŠ¸ ë°ì´í„°ì™€ íƒ€ì… ê°€ì ¸ì˜¤ê¸°
                    chart_data, chart_type = self._get_chart_data_from_stats(unified_stats)
                    
                    if chart_data and len(chart_data) > 0:
                        chart_title = self._generate_chart_title(query, unified_stats)
                        try:
                            chart_fig = self.chart_manager.create_chart(chart_type, chart_data, chart_title)
                            if chart_fig:
                                chart_info = {
                                    'chart': chart_fig, 
                                    'chart_type': chart_type,
                                    'chart_data': chart_data, 
                                    'chart_title': chart_title,
                                    'query': query, 
                                    'is_error_time_query': unified_stats.get('is_error_time_query', False)
                                }
                                print(f"DEBUG: Chart created - type: {chart_type}, data: {chart_data}")
                        except Exception as e:
                            print(f"DEBUG: Chart creation failed: {e}")
                            chart_info = None
                
                sort_info = self.detect_sorting_requirements(query)
                
                if time_conditions and time_conditions.get('is_time_query'):
                    documents = self.search_manager.filter_documents_by_time_conditions(documents, time_conditions)
                    if not documents:
                        return "í•´ë‹¹ ì‹œê°„ëŒ€ ì¡°ê±´ì— ë§ëŠ” ì¥ì•  ë‚´ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                if department_conditions and department_conditions.get('is_department_query'):
                    documents = self.search_manager.filter_documents_by_department_conditions(documents, department_conditions)
                    if not documents:
                        return "í•´ë‹¹ ë¶€ì„œ ì¡°ê±´ì— ë§ëŠ” ì¥ì•  ë‚´ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                processing_documents = self.apply_custom_sorting(documents, sort_info)
                
                total_count = len(processing_documents)
                yearly_stats = unified_stats.get('yearly_stats', {})
                monthly_stats = unified_stats.get('monthly_stats', {})
                is_error_time_query = unified_stats.get('is_error_time_query', False)
                
                # ğŸ†• í†µê³„ ì¿¼ë¦¬ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                if query_type == "statistics":
                    context_parts = []
                    
                    # í†µê³„ ì •ë³´ ìš”ì•½
                    stats_summary = f"""## ğŸ“Š í†µê³„ ì§‘ê³„ ì •ë³´

    **ì „ì²´ ë¬¸ì„œ ìˆ˜**: {unified_stats['total_count']}ê±´
    **ì—°ë„ë³„ ë¶„í¬**: {dict(sorted(unified_stats['yearly_stats'].items()))}
    **ì›”ë³„ ë¶„í¬**: {unified_stats['monthly_stats']}
    **ë°ì´í„° íƒ€ì…**: {'ì¥ì• ì‹œê°„ í•©ì‚° (ë¶„ ë‹¨ìœ„)' if unified_stats['is_error_time_query'] else 'ë°œìƒ ê±´ìˆ˜ ì§‘ê³„'}

    ---
    """
                    context_parts.append(stats_summary)
                    
                    # ğŸ†• ëª¨ë“  ë¬¸ì„œ ë‚´ì—­ì„ contextì— ìƒì„¸íˆ ì¶”ê°€
                    doc_details_header = """## ğŸ“‹ í†µê³„ ê·¼ê±°ê°€ ë˜ëŠ” ì‹¤ì œ ì¥ì•  ë¬¸ì„œ ë‚´ì—­

    **ì•„ë˜ëŠ” ìœ„ í†µê³„ì— ì‹¤ì œë¡œ ì§‘ê³„ëœ ëª¨ë“  ì¥ì•  ê±´ë“¤ì…ë‹ˆë‹¤:**
    **ì´ ë¬¸ì„œë“¤ì„ ê·¸ëŒ€ë¡œ ë‹µë³€ í•˜ë‹¨ì— ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤!**

    """
                    context_parts.append(doc_details_header)
                    
                    for i, doc in enumerate(processing_documents, 1):
                        doc_detail = f"""### ë¬¸ì„œ {i}:
    - **ì¥ì•  ID**: {doc.get('incident_id', 'N/A')}
    - **ì„œë¹„ìŠ¤ëª…**: {doc.get('service_name', 'N/A')}
    - **ë°œìƒì¼ì**: {doc.get('error_date', 'N/A')}
    - **ë°œìƒë…„ë„**: {doc.get('year', 'N/A')}
    - **ë°œìƒì›”**: {doc.get('month', 'N/A')}
    - **ì¥ì• ì‹œê°„**: {doc.get('error_time', 0)}ë¶„
    - **ì¥ì• ë“±ê¸‰**: {doc.get('incident_grade', 'N/A')}
    - **ë‹´ë‹¹ë¶€ì„œ**: {doc.get('owner_depart', 'N/A')}
    - **ì‹œê°„ëŒ€**: {doc.get('daynight', 'N/A')}
    - **ìš”ì¼**: {doc.get('week', 'N/A')}
    - **ì¥ì• í˜„ìƒ**: {doc.get('symptom', '')[:150]}{'...' if len(doc.get('symptom', '')) > 150 else ''}
    - **ì¥ì• ì›ì¸**: {doc.get('root_cause', '')[:150]}{'...' if len(doc.get('root_cause', '')) > 150 else ''}

    ---
    """
                        context_parts.append(doc_detail)
                    
                    context = "\n".join(context_parts)
                    
                    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - í†µê³„ ì „ìš©
                    system_prompt = SystemPrompts.get_prompt(query_type)
                    final_query = reprompting_info.get('transformed_query', query) if reprompting_info and reprompting_info.get('transformed') else query
                    
                    user_prompt = f"""ë‹¤ìŒ ì¥ì•  ì´ë ¥ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ í†µê³„ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

    {context}

    **ğŸš¨ ì ˆëŒ€ ì¤€ìˆ˜ì‚¬í•­ - ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”:**

    1. **ë¬¸ì„œ ê°œìˆ˜ ì¼ì¹˜ì„± ê²€ì¦**:
    - ì œê³µëœ ì‹¤ì œ ë¬¸ì„œ ìˆ˜: {unified_stats['total_count']}ê±´
    - í†µê³„ ì§‘ê³„ ê²°ê³¼ì™€ ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•¨
    - ì›”ë³„ í•©ê³„ = ì „ì²´ í•©ê³„ ì¼ì¹˜ í™•ì¸

    2. **ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥**:
    - ëª¨ë“  ë°œìƒì¼ì(error_date)ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ
    - ëª¨ë“  ì¥ì• ì‹œê°„(error_time)ì€ ì›ë³¸ ê·¸ëŒ€ë¡œ
    - ì ˆëŒ€ë¡œ ë°ì´í„°ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ì¶”ì •í•˜ì§€ ë§ˆì„¸ìš”

    3. **â­ ê·¼ê±° ë¬¸ì„œ ë‚´ì—­ í•„ìˆ˜ ì¶œë ¥ â­**:
    - í†µê³„ ë‹µë³€ í•˜ë‹¨ì— ë°˜ë“œì‹œ "## ğŸ” í†µê³„ ê·¼ê±° ë¬¸ì„œ ë‚´ì—­ (ì´ Nê±´)" ì„¹ì…˜ í¬í•¨
    - ìœ„ì— ì œê³µëœ ëª¨ë“  ë¬¸ì„œ({unified_stats['total_count']}ê±´)ë¥¼ ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ì¶œë ¥
    - ê° ë¬¸ì„œë§ˆë‹¤ ì¥ì• ID, ì„œë¹„ìŠ¤ëª…, ë°œìƒì¼ì, ì¥ì• ì‹œê°„, ì¥ì• í˜„ìƒ, ì¥ì• ì›ì¸ í¬í•¨
    - ê·¼ê±° ë¬¸ì„œ ê°œìˆ˜ì™€ í†µê³„ ì§‘ê³„ ê±´ìˆ˜ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸

    4. **ë‹µë³€ êµ¬ì¡°**:
    ```
    [ì§ˆë¬¸ì— ëŒ€í•œ í†µê³„ ìš”ì•½]
    
    ğŸ“Š ìƒì„¸ í†µê³„:
    - í•­ëª©1: Xê±´
    - í•­ëª©2: Yê±´
    ...
    
    ğŸ“ˆ ì´ í•©ê³„: Nê±´
    
    ---
    
    ## ğŸ” í†µê³„ ê·¼ê±° ë¬¸ì„œ ë‚´ì—­ (ì´ Nê±´)
    
    **ì•„ë˜ëŠ” ìœ„ í†µê³„ì— ì‹¤ì œë¡œ ì§‘ê³„ëœ ì¥ì•  ê±´ë“¤ì…ë‹ˆë‹¤:**
    
    ### 1. ì¥ì•  ID: [ID]
    - ì„œë¹„ìŠ¤ëª…: [ì„œë¹„ìŠ¤ëª…]
    - ë°œìƒì¼ì: [ë‚ ì§œ]
    - ì¥ì• ì‹œê°„: [ì‹œê°„]ë¶„
    - ì¥ì• í˜„ìƒ: [í˜„ìƒ]
    - ì¥ì• ì›ì¸: [ì›ì¸]
    
    ### 2. ì¥ì•  ID: [ID]
    ...
    
    (ëª¨ë“  ë¬¸ì„œë¥¼ ì´ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥)
    ```

    **ì§ˆë¬¸**: {final_query}

    **ë‹µë³€ì„ ì‹œì‘í•˜ì„¸ìš”:**
    """

                    # í†µê³„ ì „ìš© max_tokens ì¦ê°€
                    max_tokens_for_stats = 5000
                    
                    response = self.azure_openai_client.chat.completions.create(
                        model=self.model_name,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        temperature=0.0,
                        max_tokens=max_tokens_for_stats
                    )
                    
                    final_answer = response.choices[0].message.content
                    
                    # ì°¨íŠ¸ê°€ ìˆëŠ” ê²½ìš° í•¨ê»˜ ë°˜í™˜
                    if chart_info:
                        return final_answer, chart_info
                    return final_answer
                
                # ì¼ë°˜ ì¿¼ë¦¬ ì²˜ë¦¬ (repair, cause, similar, inquiry, default)
                else:
                    context_parts = []
                    
                    stats_info = f"""ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_count}ê±´
    ì—°ë„ë³„ ë¶„í¬: {dict(sorted(yearly_stats.items()))}
    ì›”ë³„ ë¶„í¬: {monthly_stats}"""
                    
                    if is_error_time_query:
                        stats_info += f"\në°ì´í„° ìœ í˜•: ì¥ì• ì‹œê°„ í•©ì‚°(ë¶„ ë‹¨ìœ„)"
                    
                    context_parts.append(stats_info)
                    
                    for i, doc in enumerate(processing_documents[:30]):  # ìµœëŒ€ 30ê°œë§Œ
                        context_part = f"""ë¬¸ì„œ {i+1}:
    ì¥ì•  ID: {doc['incident_id']}
    ì„œë¹„ìŠ¤ëª…: {doc['service_name']}
    ì¥ì• ì‹œê°„: {doc['error_time']}
    ì¦ìƒ: {doc['symptom']}
    ë³µêµ¬ë°©ë²•: {doc['incident_repair']}
    ë°œìƒì¼ì: {doc['error_date']}
    """
                        context_parts.append(context_part)
                    
                    context = "\n\n".join(context_parts)
                    
                    system_prompt = SystemPrompts.get_prompt(query_type)
                    final_query = reprompting_info.get('transformed_query', query) if reprompting_info and reprompting_info.get('transformed') else query

                    user_prompt = f"""ë‹¤ìŒ ì¥ì•  ì´ë ¥ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

    **ì¤‘ìš”! ë³µêµ¬ë°©ë²• ê´€ë ¨:**
    - ë³µêµ¬ë°©ë²• ì§ˆë¬¸ì—ëŠ” incident_repair í•„ë“œ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
    - incident_planì€ ë³„ë„ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì œê³µí•˜ì„¸ìš”

    **ì¤‘ìš”! ì •í™•í•œ ì§‘ê³„:**
    - ì‹¤ì œ ì œê³µëœ ë¬¸ì„œ ìˆ˜: {total_count}ê±´
    - ì—°ë„ë³„: {dict(sorted(yearly_stats.items()))}
    - ì›”ë³„: {monthly_stats}
    - ë‹µë³€ ì‹œ ì‹¤ì œ ë¬¸ì„œ ìˆ˜ì™€ ì¼ì¹˜í•´ì•¼ í•¨

    {context}

    ì§ˆë¬¸: {final_query}

    ë‹µë³€:"""

                    max_tokens_initial = 2500 if query_type == 'inquiry' else 3000 if query_type == 'cause' else 1500

                    response = self.azure_openai_client.chat.completions.create(
                        model=self.model_name,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        temperature=0.0,
                        max_tokens=max_tokens_initial
                    )
                    
                    final_answer = response.choices[0].message.content
                    
                    if chart_info:
                        return final_answer, chart_info
                    return final_answer
            
            except Exception as e:
                st.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _display_response_with_marker_conversion(self, response, chart_info=None):
        if not response:
            st.write("ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        if isinstance(response, tuple):
            response_text, chart_info = response
        else:
            response_text = response
        
        if chart_info and chart_info.get('chart'):
            response_text = self.remove_text_charts_from_response(response_text)
            
        converted_content = response_text
        html_converted = False
        
        if '[REPAIR_BOX_START]' in converted_content:
            converted_content, has_repair_html = self.ui_components.convert_repair_box_to_html(converted_content)
            if has_repair_html:
                html_converted = True
        
        if '[CAUSE_BOX_START]' in converted_content:
            converted_content, has_cause_html = self.ui_components.convert_cause_box_to_html(converted_content)
            if has_cause_html:
                html_converted = True
        
        if html_converted:
            st.markdown(converted_content, unsafe_allow_html=True)
        else:
            st.write(converted_content)
        
        if chart_info and chart_info.get('chart'):
            st.markdown("---")
            try:
                self.chart_manager.display_chart_with_data(
                    chart_info['chart'], chart_info['chart_data'],
                    chart_info['chart_type'], chart_info.get('query', '')
                )
            except Exception as e:
                st.error(f"ì°¨íŠ¸ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    @traceable(name="process_user_query")
    def process_query(self, query, query_type=None):
        if not query:
            st.error("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        with st.chat_message("assistant"):
            start_time = time.time()
            
            try:
                reprompting_info = self.check_and_transform_query_with_reprompting(query)
                processing_query = reprompting_info.get('transformed_query', query)
                
                time_conditions = self.extract_time_conditions(processing_query)
                department_conditions = self.extract_department_conditions(processing_query)
                
                if query_type is None:
                    with st.spinner("ğŸ” ì§ˆë¬¸ ë¶„ì„ ì¤‘..."):
                        query_type = self.classify_query_type_with_llm(processing_query)
                
                target_service_name = self.search_manager.extract_service_name_from_query(processing_query)
                
                with st.spinner("ğŸ“„ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
                    documents = self.search_manager.semantic_search_with_adaptive_filtering(
                        processing_query, target_service_name, query_type
                    )
                    
                    if documents is None:
                        documents = []
                    
                    if documents and len(documents) > 0:
                        with st.expander("ğŸ“„ ë§¤ì¹­ëœ ë¬¸ì„œ ìƒì„¸ ë³´ê¸°"):
                            self.ui_components.display_documents_with_quality_info(documents)
                        
                        with st.spinner("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘..."):
                            response = self.generate_rag_response_with_adaptive_processing(
                                query, documents, query_type, time_conditions, department_conditions, reprompting_info
                            )
                            
                            if response is None:
                                response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            
                            if isinstance(response, tuple):
                                response_text, chart_info = response
                                self._display_response_with_marker_conversion(response_text, chart_info)
                                st.session_state.messages.append({"role": "assistant", "content": response_text})
                            else:
                                self._display_response_with_marker_conversion(response)
                                st.session_state.messages.append({"role": "assistant", "content": response})
                    else:
                        with st.spinner("ğŸ“„ ì¶”ê°€ ê²€ìƒ‰ ì¤‘..."):
                            fallback_documents = self.search_manager.search_documents_fallback(processing_query, target_service_name)
                            
                            if fallback_documents and len(fallback_documents) > 0:
                                response = self.generate_rag_response_with_adaptive_processing(
                                    query, fallback_documents, query_type, time_conditions, department_conditions, reprompting_info
                                )
                                
                                if isinstance(response, tuple):
                                    response_text, chart_info = response
                                    self._display_response_with_marker_conversion(response_text, chart_info)
                                    st.session_state.messages.append({"role": "assistant", "content": response_text})
                                else:
                                    self._display_response_with_marker_conversion(response)
                                    st.session_state.messages.append({"role": "assistant", "content": response})
                            else:
                                error_msg = f"'{target_service_name or 'í•´ë‹¹ ì¡°ê±´'}'ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                st.write(error_msg)
                                st.session_state.messages.append({"role": "assistant", "content": error_msg})
            
            except Exception as e:
                error_msg = f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})