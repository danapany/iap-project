# -*- coding: utf-8 -*-
# query_processor_local.py
import streamlit as st
import re
import time
import os
import json
from datetime import datetime
from config.prompts import SystemPrompts
from config.settings_local import AppConfigLocal
from utils.search_utils_local import SearchManagerLocal
from utils.ui_components_local import UIComponentsLocal
from utils.reprompting_db_manager import RepromptingDBManager
from utils.chart_utils import ChartManager
from utils.statistics_db_manager import StatisticsDBManager

try:
    from utils.monitoring_manager import MonitoringManager
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False
    class MonitoringManager:
        def __init__(self, *args, **kwargs): pass
        def log_user_activity(self, *args, **kwargs): pass

LANGSMITH_ENABLED = os.getenv('LANGSMITH_TRACING', 'false').lower() == 'true'

if LANGSMITH_ENABLED:
    try:
        from langsmith import traceable, trace
        from langsmith.wrappers import wrap_openai
        LANGSMITH_AVAILABLE = True
    except ImportError:
        LANGSMITH_AVAILABLE = False
        LANGSMITH_ENABLED = False
        def traceable(name=None, **kwargs):
            def decorator(func): return func
            return decorator
        def trace(name=None, **kwargs):
            class DummyTrace:
                def __enter__(self): return self
                def __exit__(self, *args): pass
                def update(self, **kwargs): pass
            return DummyTrace()
else:
    LANGSMITH_AVAILABLE = False
    def traceable(name=None, **kwargs):
        def decorator(func): return func
        return decorator
    def trace(name=None, **kwargs):
        class DummyTrace:
            def __enter__(self): return self
            def __exit__(self, *args): pass
            def update(self, **kwargs): pass
        return DummyTrace()

class StatisticsValidator:
    def __init__(self):
        self.validation_errors = []
        self.validation_warnings = []
    
    def validate_document(self, doc, doc_index):
        errors, warnings = [], []
        for field in ['incident_id', 'service_name', 'error_date']:
            if not doc.get(field):
                errors.append(f"Document {doc_index}: {field} field is empty")
        
        error_time = doc.get('error_time')
        if error_time is not None:
            try:
                error_time_int = int(error_time)
                if error_time_int < 0:
                    warnings.append(f"Document {doc_index}: error_time is negative")
                elif error_time_int > 10080:
                    warnings.append(f"Document {doc_index}: error_time is abnormally large")
            except (ValueError, TypeError):
                errors.append(f"Document {doc_index}: error_time format error")
        return errors, warnings
    
    def validate_statistics_result(self, stats, original_doc_count):
        errors, warnings = [], []
        if stats.get('total_count', 0) != original_doc_count:
            errors.append(f"Total count mismatch: calculated({stats.get('total_count', 0)}) != original({original_doc_count})")
        return errors, warnings

class DataNormalizer:
    @staticmethod
    def normalize_error_time(error_time):
        if error_time is None: return 0
        try:
            if isinstance(error_time, str):
                error_time = error_time.strip()
                if error_time == '' or error_time.lower() in ['null', 'none', 'n/a']:
                    return 0
                return int(float(error_time))
            return int(error_time)
        except (ValueError, TypeError):
            return 0
    
    @staticmethod
    def normalize_date_fields(doc):
        normalized_doc = doc.copy()
        error_date = str(doc.get('error_date', '')).strip()
        
        if error_date:
            try:
                if '-' in error_date and len(error_date) >= 7:
                    parts = error_date.split('-')
                    if len(parts) >= 2:
                        if parts[0].isdigit() and len(parts[0]) == 4:
                            normalized_doc['extracted_year'] = parts[0]
                        if parts[1].isdigit() and 1 <= int(parts[1]) <= 12:
                            normalized_doc['extracted_month'] = str(int(parts[1]))
                elif len(error_date) >= 8 and error_date.isdigit():
                    normalized_doc['extracted_year'] = error_date[:4]
                    month_num = int(error_date[4:6])
                    if 1 <= month_num <= 12:
                        normalized_doc['extracted_month'] = str(month_num)
                elif len(error_date) >= 4 and error_date[:4].isdigit():
                    normalized_doc['extracted_year'] = error_date[:4]
            except (ValueError, TypeError):
                pass
        
        if not normalized_doc.get('year') and normalized_doc.get('extracted_year'):
            normalized_doc['year'] = normalized_doc['extracted_year']
        if not normalized_doc.get('month') and normalized_doc.get('extracted_month'):
            normalized_doc['month'] = normalized_doc['extracted_month']
        
        if normalized_doc.get('year'):
            normalized_doc['year'] = str(normalized_doc['year']).strip()
        if normalized_doc.get('month'):
            try:
                month_val = int(normalized_doc['month'])
                normalized_doc['month'] = str(month_val) if 1 <= month_val <= 12 else ''
            except (ValueError, TypeError):
                normalized_doc['month'] = ''
        
        return normalized_doc
    
    @staticmethod
    def normalize_document(doc):
        normalized_doc = DataNormalizer.normalize_date_fields(doc)
        normalized_doc['error_time'] = DataNormalizer.normalize_error_time(doc.get('error_time'))
        for field in ['service_name', 'incident_grade', 'owner_depart', 'daynight', 'week']:
            value = normalized_doc.get(field)
            normalized_doc[field] = str(value).strip() if value else ''
        return normalized_doc

class ImprovedStatisticsCalculator:
    def __init__(self, remove_duplicates=False):
        self.validator = StatisticsValidator()
        self.normalizer = DataNormalizer()
        self.remove_duplicates = remove_duplicates
    
    def _extract_filter_conditions(self, query):
        conditions = {'year': None, 'month': None, 'start_month': None, 'end_month': None, 'daynight': None, 'week': None, 'service_name': None, 'department': None, 'grade': None}
        if not query: return conditions
        query_lower = query.lower()
        
        year_match = re.search(r'\b(202[0-9]|201[0-9])\b', query_lower)
        if year_match:
            conditions['year'] = year_match.group(1)
        
        for pattern in [r'\b(\d+)\s*~\s*(\d+)Ïõî\b', r'\b(\d+)Ïõî\s*~\s*(\d+)Ïõî\b', r'\b(\d+)\s*-\s*(\d+)Ïõî\b', r'\b(\d+)Ïõî\s*-\s*(\d+)Ïõî\b']:
            month_range_match = re.search(pattern, query_lower)
            if month_range_match:
                start_month, end_month = int(month_range_match.group(1)), int(month_range_match.group(2))
                if 1 <= start_month <= 12 and 1 <= end_month <= 12 and start_month <= end_month:
                    conditions['start_month'], conditions['end_month'] = start_month, end_month
                    break
        
        if not conditions['start_month']:
            month_match = re.search(r'\b(\d{1,2})Ïõî\b', query_lower)
            if month_match and 1 <= int(month_match.group(1)) <= 12:
                conditions['month'] = str(int(month_match.group(1)))
        
        if any(word in query_lower for word in ['ÏïºÍ∞Ñ', 'Î∞§', 'ÏÉàÎ≤Ω', 'Ïã¨Ïïº']):
            conditions['daynight'] = 'ÏïºÍ∞Ñ'
        elif any(word in query_lower for word in ['Ï£ºÍ∞Ñ', 'ÎÇÆ', 'Ïò§Ï†Ñ', 'Ïò§ÌõÑ']):
            conditions['daynight'] = 'Ï£ºÍ∞Ñ'
        
        week_patterns = {'Ïõî': ['ÏõîÏöîÏùº', 'Ïõî'], 'Ìôî': ['ÌôîÏöîÏùº', 'Ìôî'], 'Ïàò': ['ÏàòÏöîÏùº', 'Ïàò'], 'Î™©': ['Î™©ÏöîÏùº', 'Î™©'], 'Í∏à': ['Í∏àÏöîÏùº', 'Í∏à'], 'ÌÜ†': ['ÌÜ†ÏöîÏùº', 'ÌÜ†'], 'Ïùº': ['ÏùºÏöîÏùº', 'Ïùº']}
        for week_key, patterns in week_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                conditions['week'] = week_key
                break
        
        if 'ÌèâÏùº' in query_lower:
            conditions['week'] = 'ÌèâÏùº'
        elif 'Ï£ºÎßê' in query_lower:
            conditions['week'] = 'Ï£ºÎßê'
        
        grade_match = re.search(r'(\d+)Îì±Í∏â', query_lower)
        if grade_match:
            conditions['grade'] = f"{grade_match.group(1)}Îì±Í∏â"
        
        return conditions
    
    def _validate_document_against_conditions(self, doc, conditions):
        if conditions['year'] and self._extract_year_from_document(doc) != conditions['year']:
            return False, "year mismatch"
        
        if conditions['start_month'] and conditions['end_month']:
            doc_month = self._extract_month_from_document(doc)
            if not doc_month:
                return False, "no month information"
            try:
                if not (conditions['start_month'] <= int(doc_month) <= conditions['end_month']):
                    return False, "month not in range"
            except (ValueError, TypeError):
                return False, "invalid month format"
        elif conditions['month'] and str(self._extract_month_from_document(doc)) != conditions['month']:
            return False, "month mismatch"
        
        if conditions['daynight']:
            doc_daynight = doc.get('daynight', '').strip()
            if not doc_daynight or doc_daynight != conditions['daynight']:
                return False, "daynight mismatch"
        
        if conditions['week']:
            doc_week = doc.get('week', '').strip()
            required_week = conditions['week']
            if required_week == 'ÌèâÏùº':
                if doc_week not in ['Ïõî', 'Ìôî', 'Ïàò', 'Î™©', 'Í∏à']:
                    return False, "not weekday"
            elif required_week == 'Ï£ºÎßê':
                if doc_week not in ['ÌÜ†', 'Ïùº']:
                    return False, "not weekend"
            elif not doc_week or doc_week != required_week:
                return False, "week mismatch"
        
        if conditions['grade'] and doc.get('incident_grade', '') != conditions['grade']:
            return False, "grade mismatch"
        
        return True, "passed"
    
    def _extract_year_from_document(self, doc):
        for key in ['year', 'extracted_year']:
            year = doc.get(key)
            if year:
                year_str = str(year).strip()
                if len(year_str) == 4 and year_str.isdigit():
                    return year_str
        
        error_date = str(doc.get('error_date', '')).strip()
        if len(error_date) >= 4:
            if '-' in error_date:
                parts = error_date.split('-')
                if parts and len(parts[0]) == 4 and parts[0].isdigit():
                    return parts[0]
            elif error_date[:4].isdigit():
                return error_date[:4]
        return None
    
    def _extract_month_from_document(self, doc):
        for key in ['month', 'extracted_month']:
            month = doc.get(key)
            if month:
                try:
                    month_num = int(month)
                    if 1 <= month_num <= 12:
                        return str(month_num)
                except (ValueError, TypeError):
                    pass
        
        error_date = str(doc.get('error_date', '')).strip()
        if '-' in error_date:
            parts = error_date.split('-')
            if len(parts) >= 2 and parts[1].isdigit():
                try:
                    month_num = int(parts[1])
                    if 1 <= month_num <= 12:
                        return str(month_num)
                except (ValueError, TypeError):
                    pass
        elif len(error_date) >= 6 and error_date.isdigit():
            try:
                month_num = int(error_date[4:6])
                if 1 <= month_num <= 12:
                    return str(month_num)
            except (ValueError, TypeError):
                pass
        return None
    
    def _apply_filters(self, documents, conditions):
        return [doc for doc in documents if self._validate_document_against_conditions(doc, conditions)[0]]
    
    def _empty_statistics(self):
        return {'total_count': 0, 'yearly_stats': {}, 'monthly_stats': {}, 'time_stats': {'daynight': {}, 'week': {}}, 'department_stats': {}, 'service_stats': {}, 'grade_stats': {}, 'is_error_time_query': False, 'validation': {'errors': [], 'warnings': [], 'is_valid': True}, 'primary_stat_type': None}
    
    def _is_error_time_query(self, query):
        return query and any(keyword in query.lower() for keyword in ['Ïû•Ïï†ÏãúÍ∞Ñ', 'Ïû•Ïï† ÏãúÍ∞Ñ', 'error_time', 'ÏãúÍ∞Ñ ÌÜµÍ≥Ñ', 'ÏãúÍ∞Ñ Ìï©Í≥Ñ', 'ÏãúÍ∞Ñ Ìï©ÏÇ∞', 'Î∂Ñ'])
    
    def _determine_primary_stat_type(self, query, yearly_stats, monthly_stats, time_stats, service_stats, department_stats, grade_stats):
        if query:
            query_lower = query.lower()
            keywords = [('yearly', ['Ïó∞ÎèÑÎ≥Ñ', 'ÎÖÑÎèÑÎ≥Ñ', 'ÎÖÑÎ≥Ñ', 'Ïó∞Î≥Ñ']), ('monthly', ['ÏõîÎ≥Ñ']), ('time', ['ÏãúÍ∞ÑÎåÄÎ≥Ñ', 'Ï£ºÍ∞Ñ', 'ÏïºÍ∞Ñ']), ('weekday', ['ÏöîÏùºÎ≥Ñ']), ('department', ['Î∂ÄÏÑúÎ≥Ñ', 'ÌåÄÎ≥Ñ']), ('service', ['ÏÑúÎπÑÏä§Î≥Ñ']), ('grade', ['Îì±Í∏âÎ≥Ñ'])]
            for stat_type, kws in keywords:
                if any(kw in query_lower for kw in kws):
                    return stat_type
            if re.search(r'\b\d+Ïõî\b', query_lower):
                return 'monthly'
        
        stat_counts = {'yearly': len(yearly_stats), 'monthly': len(monthly_stats), 'service': len(service_stats), 'department': len(department_stats), 'grade': len(grade_stats), 'time': len(time_stats.get('daynight', {})) + len(time_stats.get('week', {}))}
        return max(stat_counts.items(), key=lambda x: x[1])[0] if any(stat_counts.values()) else 'yearly'
    
    def _calculate_detailed_statistics(self, documents, conditions, is_error_time_query):
        stats = {'total_count': len(documents), 'yearly_stats': {}, 'monthly_stats': {}, 'time_stats': {'daynight': {}, 'week': {}}, 'department_stats': {}, 'service_stats': {}, 'grade_stats': {}, 'is_error_time_query': is_error_time_query, 'filter_conditions': conditions, 'calculation_details': {}}
        
        yearly_temp, monthly_temp = {}, {}
        for doc in documents:
            year = self._extract_year_from_document(doc)
            month = self._extract_month_from_document(doc)
            error_time = doc.get('error_time', 0) if is_error_time_query else 1
            
            if year:
                yearly_temp[year] = yearly_temp.get(year, 0) + error_time
            if month:
                try:
                    month_num = int(month)
                    if 1 <= month_num <= 12:
                        monthly_temp[month_num] = monthly_temp.get(month_num, 0) + error_time
                except (ValueError, TypeError):
                    pass
        
        for year in sorted(yearly_temp.keys()):
            stats['yearly_stats'][f"{year}ÎÖÑ"] = yearly_temp[year]
        for month_num in sorted(monthly_temp.keys()):
            stats['monthly_stats'][f"{month_num}Ïõî"] = monthly_temp[month_num]
        
        daynight_temp, week_temp, department_temp, service_temp, grade_temp = {}, {}, {}, {}, {}
        for doc in documents:
            error_time = doc.get('error_time', 0) if is_error_time_query else 1
            for field, temp_dict in [('daynight', daynight_temp), ('week', week_temp), ('owner_depart', department_temp), ('service_name', service_temp), ('incident_grade', grade_temp)]:
                value = doc.get(field, '')
                if value:
                    temp_dict[value] = temp_dict.get(value, 0) + error_time
        
        for time_key in ['Ï£ºÍ∞Ñ', 'ÏïºÍ∞Ñ']:
            if time_key in daynight_temp:
                stats['time_stats']['daynight'][time_key] = daynight_temp[time_key]
        
        for week_key in ['Ïõî', 'Ìôî', 'Ïàò', 'Î™©', 'Í∏à', 'ÌÜ†', 'Ïùº', 'ÌèâÏùº', 'Ï£ºÎßê']:
            if week_key in week_temp:
                week_display = f"{week_key}ÏöîÏùº" if week_key in ['Ïõî', 'Ìôî', 'Ïàò', 'Î™©', 'Í∏à', 'ÌÜ†', 'Ïùº'] else week_key
                stats['time_stats']['week'][week_display] = week_temp[week_key]
        
        stats['department_stats'] = dict(sorted(department_temp.items(), key=lambda x: x[1], reverse=True)[:10])
        stats['service_stats'] = dict(sorted(service_temp.items(), key=lambda x: x[1], reverse=True)[:10])
        
        for grade_key in ['1Îì±Í∏â', '2Îì±Í∏â', '3Îì±Í∏â', '4Îì±Í∏â']:
            if grade_key in grade_temp:
                stats['grade_stats'][grade_key] = grade_temp[grade_key]
        for grade_key, value in sorted(grade_temp.items()):
            if grade_key not in stats['grade_stats']:
                stats['grade_stats'][grade_key] = value
        
        total_error_time = sum(doc.get('error_time', 0) for doc in documents)
        stats['calculation_details'] = {'total_error_time_minutes': total_error_time, 'total_error_time_hours': round(total_error_time / 60, 2), 'average_error_time': round(total_error_time / len(documents), 2) if documents else 0, 'max_error_time': max((doc.get('error_time', 0) for doc in documents), default=0), 'min_error_time': min((doc.get('error_time', 0) for doc in documents), default=0), 'documents_with_error_time': len([doc for doc in documents if doc.get('error_time', 0) > 0])}
        stats['primary_stat_type'] = None
        return stats
    
    def calculate_comprehensive_statistics(self, documents, query, query_type="default"):
        if not documents:
            return self._empty_statistics()
        
        normalized_docs, validation_errors, validation_warnings = [], [], []
        for i, doc in enumerate(documents):
            if doc is None: continue
            errors, warnings = self.validator.validate_document(doc, i)
            validation_errors.extend(errors)
            validation_warnings.extend(warnings)
            normalized_docs.append(self.normalizer.normalize_document(doc))
        
        if self.remove_duplicates:
            unique_docs = {}
            for doc in normalized_docs:
                incident_id = doc.get('incident_id', '')
                if incident_id and incident_id not in unique_docs:
                    unique_docs[incident_id] = doc
            clean_documents = list(unique_docs.values())
        else:
            clean_documents = normalized_docs
        
        filter_conditions = self._extract_filter_conditions(query)
        is_stats_query = any(keyword in query.lower() for keyword in ['Í±¥Ïàò', 'ÌÜµÍ≥Ñ', 'Ïó∞ÎèÑÎ≥Ñ', 'ÏõîÎ≥Ñ', 'ÌòÑÌô©', 'Î∂ÑÌè¨', 'ÏïåÎ†§Ï§ò', 'Î™áÍ±¥', 'Í∞úÏàò'])
        filtered_docs = clean_documents if is_stats_query else self._apply_filters(clean_documents, filter_conditions)
        
        is_error_time_query = self._is_error_time_query(query)
        stats = self._calculate_detailed_statistics(filtered_docs, filter_conditions, is_error_time_query)
        stats['primary_stat_type'] = self._determine_primary_stat_type(query, stats['yearly_stats'], stats['monthly_stats'], stats['time_stats'], stats['service_stats'], stats['department_stats'], stats['grade_stats'])
        
        result_errors, result_warnings = self.validator.validate_statistics_result(stats, len(filtered_docs))
        validation_errors.extend(result_errors)
        validation_warnings.extend(result_warnings)
        stats['validation'] = {'errors': validation_errors, 'warnings': validation_warnings, 'is_valid': len(validation_errors) == 0}
        return stats

class QueryProcessorLocal:
    def __init__(self, azure_openai_client, search_client, model_name, config=None):
        self.azure_openai_client = azure_openai_client
        self.search_client = search_client
        self.model_name = model_name
        self.config = config or AppConfigLocal()
        self.search_manager = SearchManagerLocal(search_client, self.config)
        self.ui_components = UIComponentsLocal()
        self.reprompting_db_manager = RepromptingDBManager()
        self.chart_manager = ChartManager()
        self.statistics_calculator = ImprovedStatisticsCalculator(remove_duplicates=False)
        self.statistics_db_manager = StatisticsDBManager()
        self.debug_mode = True
        
        # Ï§ëÎ≥µ Î°úÍπÖ Î∞©ÏßÄÎ•º ÏúÑÌïú ÌîåÎûòÍ∑∏Îì§ Ï∂îÍ∞Ä
        self._decorator_logging_enabled = False  # Îç∞ÏΩîÎ†àÏù¥ÌÑ∞ Î°úÍπÖ ÎπÑÌôúÏÑ±Ìôî
        self._manual_logging_enabled = True      # ÏàòÎèô Î°úÍπÖ ÌôúÏÑ±Ìôî
        
        if MONITORING_AVAILABLE:
            try:
                self.monitoring_manager = MonitoringManager()
                self.monitoring_enabled = True
            except Exception:
                self.monitoring_manager = MonitoringManager()
                self.monitoring_enabled = False
        else:
            self.monitoring_manager = MonitoringManager()
            self.monitoring_enabled = False
        
        self.langsmith_enabled = LANGSMITH_ENABLED
        self._setup_langsmith()
    
    def _setup_langsmith(self):
        if not self.langsmith_enabled: return
        try:
            langsmith_status = self.config.get_langsmith_status()
            if langsmith_status['enabled'] and LANGSMITH_AVAILABLE:
                if self.config.setup_langsmith():
                    self.azure_openai_client = wrap_openai(self.azure_openai_client)
        except Exception:
            pass

    def safe_trace_update(self, trace_obj, **kwargs):
        if not self.langsmith_enabled: return
        try:
            if hasattr(trace_obj, 'update'):
                trace_obj.update(**kwargs)
            elif hasattr(trace_obj, 'add_outputs'):
                if 'outputs' in kwargs:
                    trace_obj.add_outputs(kwargs['outputs'])
                if 'metadata' in kwargs:
                    trace_obj.add_metadata(kwargs['metadata'])
        except Exception:
            pass

    def calculate_unified_statistics(self, documents, query, query_type="default"):
        return self.statistics_calculator._empty_statistics() if not documents else self.statistics_calculator.calculate_comprehensive_statistics(documents, query, query_type)

    @traceable(name="check_reprompting_question")
    def check_and_transform_query_with_reprompting(self, user_query):
        """Í∞úÏÑ†Îêú Î¶¨ÌîÑÎ°¨ÌîÑÌåÖ - Í∞ïÏ†ú ÏπòÌôò Ï∂îÍ∞Ä"""
        if not user_query:
            return {'transformed': False, 'original_query': user_query, 'transformed_query': user_query, 'match_type': 'none'}
        
        # 1Îã®Í≥Ñ: Í∞ïÏ†ú ÏπòÌôò Î®ºÏ†Ä Ï†ÅÏö©
        force_replaced_query = self.force_replace_problematic_queries(user_query)
        
        with trace(name="reprompting_check", inputs={"user_query": user_query, "force_replaced": force_replaced_query}) as trace_context:
            try:
                # Í∞ïÏ†ú ÏπòÌôòÏù¥ Î∞úÏÉùÌïú Í≤ΩÏö∞
                if force_replaced_query != user_query:
                    if not self.debug_mode:
                        st.success("‚úÖ ÎßûÏ∂§Ìòï ÌîÑÎ°¨ÌîÑÌä∏Î•º Ï†ÅÏö©ÌïòÏó¨ Îçî Ï†ïÌôïÌïú ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.")
                    return {
                        'transformed': True, 
                        'original_query': user_query, 
                        'transformed_query': force_replaced_query, 
                        'question_type': 'statistics',
                        'wrong_answer_summary': 'ÎèôÏùòÏñ¥ ÌëúÌòÑ ÏµúÏ†ÅÌôî',
                        'match_type': 'force_replacement'
                    }
                
                # 2Îã®Í≥Ñ: Í∏∞Ï°¥ Î¶¨ÌîÑÎ°¨ÌîÑÌåÖ Î°úÏßÅ Ïã§Ìñâ
                exact_result = self.reprompting_db_manager.check_reprompting_question(user_query)
                if exact_result['exists']:
                    if not self.debug_mode:
                        st.success("‚úÖ ÎßûÏ∂§Ìòï ÌîÑÎ°¨ÌîÑÌä∏Î•º Ï†ÅÏö©ÌïòÏó¨ Îçî Ï†ïÌôïÌïú ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.")
                    return {
                        'transformed': True, 
                        'original_query': user_query, 
                        'transformed_query': exact_result['custom_prompt'], 
                        'question_type': exact_result['question_type'], 
                        'wrong_answer_summary': exact_result['wrong_answer_summary'], 
                        'match_type': 'exact'
                    }
                
                # 3Îã®Í≥Ñ: Ïú†ÏÇ¨ ÏßàÎ¨∏ Í≤ÄÏÉâ
                similar_questions = self.reprompting_db_manager.find_similar_questions(user_query, similarity_threshold=0.7, limit=3)
                if similar_questions:
                    best_match = similar_questions[0]
                    try:
                        transformed_query = re.sub(re.escape(best_match['question']), best_match['custom_prompt'], user_query, flags=re.IGNORECASE)
                    except:
                        transformed_query = user_query.replace(best_match['question'], best_match['custom_prompt'])
                    
                    is_transformed = transformed_query != user_query
                    if is_transformed and not self.debug_mode:
                        st.info("üìã Ïú†ÏÇ¨ ÏßàÎ¨∏ Ìå®ÌÑ¥ÏùÑ Í∞êÏßÄÌïòÏó¨ ÏßàÎ¨∏ÏùÑ ÏµúÏ†ÅÌôîÌñàÏäµÎãàÎã§.")
                    return {
                        'transformed': is_transformed, 
                        'original_query': user_query, 
                        'transformed_query': transformed_query, 
                        'question_type': best_match['question_type'], 
                        'wrong_answer_summary': best_match['wrong_answer_summary'], 
                        'similarity': best_match['similarity'], 
                        'similar_question': best_match['question'], 
                        'match_type': 'similar'
                    }
                
                return {'transformed': False, 'original_query': user_query, 'transformed_query': user_query, 'match_type': 'none'}
                
            except Exception as e:
                return {'transformed': False, 'original_query': user_query, 'transformed_query': user_query, 'match_type': 'error', 'error': str(e)}
    
    def extract_time_conditions(self, query):
        if not query:
            return {'daynight': None, 'week': None, 'is_time_query': False}
        
        time_conditions = {'daynight': None, 'week': None, 'is_time_query': False}
        if any(keyword in query.lower() for keyword in ['ÏïºÍ∞Ñ', 'Î∞§', 'ÏÉàÎ≤Ω', 'Ïã¨Ïïº']):
            time_conditions.update({'is_time_query': True, 'daynight': 'ÏïºÍ∞Ñ'})
        elif any(keyword in query.lower() for keyword in ['Ï£ºÍ∞Ñ', 'ÎÇÆ', 'Ïò§Ï†Ñ', 'Ïò§ÌõÑ']):
            time_conditions.update({'is_time_query': True, 'daynight': 'Ï£ºÍ∞Ñ'})
        
        week_map = {'ÏõîÏöîÏùº': 'Ïõî', 'ÌôîÏöîÏùº': 'Ìôî', 'ÏàòÏöîÏùº': 'Ïàò', 'Î™©ÏöîÏùº': 'Î™©', 'Í∏àÏöîÏùº': 'Í∏à', 'ÌÜ†ÏöîÏùº': 'ÌÜ†', 'ÏùºÏöîÏùº': 'Ïùº', 'ÌèâÏùº': 'ÌèâÏùº', 'Ï£ºÎßê': 'Ï£ºÎßê'}
        for keyword, value in week_map.items():
            if keyword in query.lower():
                time_conditions.update({'is_time_query': True, 'week': value})
                break
        
        return time_conditions
    
    def extract_department_conditions(self, query):
        if not query:
            return {'owner_depart': None, 'is_department_query': False}
        return {'owner_depart': None, 'is_department_query': any(keyword in query for keyword in ['Îã¥ÎãπÎ∂ÄÏÑú', 'Ï°∞ÏπòÎ∂ÄÏÑú', 'Ï≤òÎ¶¨Î∂ÄÏÑú', 'Ï±ÖÏûÑÎ∂ÄÏÑú', 'Í¥ÄÎ¶¨Î∂ÄÏÑú', 'Î∂ÄÏÑú', 'ÌåÄ', 'Ï°∞ÏßÅ'])}
    
    @traceable(name="classify_query_type")
    def classify_query_type_with_llm(self, query):
        if not query:
            return 'default'
        
        query_lower = query.lower()
        
        # üî• 0Îã®Í≥Ñ: ÌÜµÍ≥ÑÏÑ± '~Î≥Ñ' Ìå®ÌÑ¥ ÏµúÏö∞ÏÑ† Ï≤¥ÌÅ¨ (ÏÉàÎ°ú Ï∂îÍ∞Ä)
        statistics_by_patterns = [
            r'ÏõêÏù∏Ïú†ÌòïÎ≥Ñ\s*.*(Í±¥Ïàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨|Î™á|Í∞úÏàò|ÏãúÍ∞Ñ|Î∂Ñ)',
            r'Ïû•Ïï†ÏõêÏù∏Î≥Ñ\s*.*(Í±¥Ïàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨|Î™á|Í∞úÏàò|ÏãúÍ∞Ñ|Î∂Ñ)', 
            r'ÏõêÏù∏Î≥Ñ\s*.*(Í±¥Ïàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨|Î™á|Í∞úÏàò|ÏãúÍ∞Ñ|Î∂Ñ)',
            r'(Ïó∞ÎèÑ|ÎÖÑÎèÑ|Ïõî|ÏöîÏùº|ÏãúÍ∞ÑÎåÄ|Îì±Í∏â|Î∂ÄÏÑú|ÏÑúÎπÑÏä§)Î≥Ñ\s*.*(Í±¥Ïàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨|Î™á|Í∞úÏàò|ÏãúÍ∞Ñ|Î∂Ñ)',
            r'.*(Í±¥Ïàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨|Î™á|Í∞úÏàò|ÏãúÍ∞Ñ|Î∂Ñ)\s*.*(Ïó∞ÎèÑ|ÎÖÑÎèÑ|Ïõî|ÏöîÏùº|ÏãúÍ∞ÑÎåÄ|Îì±Í∏â|Î∂ÄÏÑú|ÏÑúÎπÑÏä§|ÏõêÏù∏Ïú†Ìòï|Ïû•Ïï†ÏõêÏù∏|ÏõêÏù∏)Î≥Ñ',
            r'\w*Î≥Ñ\s*\w*(Í±¥Ïàò|ÏãúÍ∞Ñ|Î∂ÑÏàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨)',
            r'(Í±¥Ïàò|ÏãúÍ∞Ñ|Î∂ÑÏàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨)\s*\w*Î≥Ñ',
        ]
        
        for pattern in statistics_by_patterns:
            if re.search(pattern, query_lower):
                if self.debug_mode:
                    print(f"DEBUG: ÌÜµÍ≥ÑÏÑ± '~Î≥Ñ' Ìå®ÌÑ¥ Í∞êÏßÄÎê® - Ï¶âÏãú statisticsÎ°ú Î∂ÑÎ•ò: {pattern}")
                return 'statistics'
        
        # 1Îã®Í≥Ñ: Î™ÖÌôïÌïú ÎπÑÌÜµÍ≥Ñ ÌÇ§ÏõåÎìú Ïö∞ÏÑ† Ï≤¥ÌÅ¨ (ÏàòÏ†ïÎê® - ÏõêÏù∏ Í¥ÄÎ†® ÏòàÏô∏ Ï∂îÍ∞Ä)
        non_statistics_keywords = [
            # Î≥µÍµ¨/Ìï¥Í≤∞ Í¥ÄÎ†®
            'Î≥µÍµ¨Î∞©Î≤ï', 'Ìï¥Í≤∞Î∞©Î≤ï', 'Ï°∞ÏπòÎ∞©Î≤ï', 'ÎåÄÏùëÎ∞©Î≤ï', 'Î≥µÍµ¨Ï†àÏ∞®', 'Ìï¥Í≤∞Ï†àÏ∞®',
            'Î≥µÍµ¨', 'Ìï¥Í≤∞', 'Ï°∞Ïπò', 'ÎåÄÏùë', 'ÏàòÏ†ï', 'Í∞úÏÑ†', 'Ï≤òÎ¶¨Î∞©Î≤ï',
            
            # Î¨∏Ï†ú/Ïû•Ïï† ÏÉÅÌô© Í¥ÄÎ†® (ÏõêÏù∏ Ï†úÏô∏ - ÌÜµÍ≥ÑÏóêÏÑú ÏûêÏ£º ÏÇ¨Ïö©Îê®)
            'Î∂àÍ∞Ä', 'Ïã§Ìå®', 'ÏïàÎê®', 'ÏïàÎèº', 'ÎêòÏßÄÏïä', 'Ïò§Î•ò', 'ÏóêÎü¨', 'error', 
            'Î¨∏Ï†ú', 'Ïû•Ïï†', 'Ïù¥Ïäà', 'issue', 'Î≤ÑÍ∑∏', 'bug',
            
            # üî• ÏõêÏù∏ Í¥ÄÎ†®ÏùÄ '~Î≥Ñ' Ìå®ÌÑ¥Ïù¥ ÏóÜÏùÑ ÎïåÎßå ÎπÑÌÜµÍ≥ÑÎ°ú Ï≤òÎ¶¨ (Ï°∞Í±¥Î∂Ä Ï†úÍ±∞)
            # 'ÏõêÏù∏', 'Ïù¥Ïú†', 'Ïôú' - Ïù¥ ÌÇ§ÏõåÎìúÎì§ÏùÄ ÏïÑÎûòÏóêÏÑú Î≥ÑÎèÑ Ï≤òÎ¶¨
            
            # Ïú†ÏÇ¨ÏÇ¨Î°Ä Í¥ÄÎ†®  
            'Ïú†ÏÇ¨', 'ÎπÑÏä∑Ìïú', 'Í∞ôÏùÄ', 'ÎèôÏùºÌïú', 'ÎπÑÍµê',
            
            # ÏÉÅÏÑ∏ÎÇ¥Ïó≠ Ï°∞Ìöå Í¥ÄÎ†®
            'ÎÇ¥Ïó≠', 'Î™©Î°ù', 'Î¶¨Ïä§Ìä∏', 'ÏÉÅÏÑ∏', 'ÏÑ∏Î∂Ä', 'Ï†ÑÏ≤¥ÎÇ¥Ïó≠',
            
            # Ï¶ùÏÉÅ/ÌòÑÏÉÅ Í¥ÄÎ†®
            'Ï¶ùÏÉÅ', 'ÌòÑÏÉÅ', 'ÏÉÅÌô©', 'ÏÉÅÌÉú', 'Ï°∞Í±¥'
        ]
        
        # üî• ÏõêÏù∏ Í¥ÄÎ†® ÌÇ§ÏõåÎìúÎäî '~Î≥Ñ' Ìå®ÌÑ¥Ïù¥ ÏóÜÏùÑ ÎïåÎßå ÎπÑÌÜµÍ≥ÑÎ°ú ÌåêÎã®
        cause_related_keywords = ['ÏõêÏù∏', 'Ïù¥Ïú†', 'Ïôú', 'why', 'Î∂ÑÏÑù', 'ÏßÑÎã®']
        has_cause_keyword = any(keyword in query_lower for keyword in cause_related_keywords)
        has_by_pattern = re.search(r'\w*Î≥Ñ\s', query_lower) or re.search(r'\s\w*Î≥Ñ', query_lower)
        
        # ÏõêÏù∏ ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÏßÄÎßå '~Î≥Ñ' Ìå®ÌÑ¥ÎèÑ ÏûàÏúºÎ©¥ ÌÜµÍ≥ÑÎ°ú Ïö∞ÏÑ† Ï≤òÎ¶¨
        if has_cause_keyword and has_by_pattern:
            if self.debug_mode:
                print(f"DEBUG: ÏõêÏù∏ ÌÇ§ÏõåÎìú + ~Î≥Ñ Ìå®ÌÑ¥ Í∞êÏßÄ - ÌÜµÍ≥Ñ Ïö∞ÏÑ† Ï≤òÎ¶¨")
            # ÌÜµÍ≥Ñ Ìå®ÌÑ¥ÏúºÎ°ú ÎÑòÏñ¥Í∞ÄÏÑú Ï∂îÍ∞Ä Í≤ÄÏ¶ù
        elif has_cause_keyword:
            if self.debug_mode:
                print(f"DEBUG: ÏõêÏù∏ ÌÇ§ÏõåÎìú Í∞êÏßÄ (Î≥Ñ Ìå®ÌÑ¥ ÏóÜÏùå) - ÎπÑÌÜµÍ≥ÑÎ°ú Î∂ÑÎ•ò")
            return self._classify_non_statistics_query(query_lower)
        
        # Í∏∞Ï°¥ ÎπÑÌÜµÍ≥Ñ ÌÇ§ÏõåÎìú Ï≤¥ÌÅ¨
        if any(keyword in query_lower for keyword in non_statistics_keywords):
            if self.debug_mode:
                print(f"DEBUG: Non-statistics keyword detected, classified as non-statistics")
            return self._classify_non_statistics_query(query_lower)
        
        # 2Îã®Í≥Ñ: Î™ÖÌôïÌïú ÌÜµÍ≥Ñ ÌÇ§ÏõåÎìú Ï≤¥ÌÅ¨ (Í∞ïÌôîÎê®)
        clear_statistics_keywords = [
            # Î™ÖÌôïÌïú ÌÜµÍ≥Ñ ÏßÄÏãúÏñ¥
            'Í±¥Ïàò', 'ÌÜµÍ≥Ñ', 'ÌòÑÌô©', 'Î∂ÑÌè¨', 'Í∞úÏàò', 'Î™áÍ±¥', 'Î™áÍ∞ú', 
            'Ïó∞ÎèÑÎ≥Ñ', 'ÏõîÎ≥Ñ', 'Îì±Í∏âÎ≥Ñ', 'Ïû•Ïï†Îì±Í∏âÎ≥Ñ', 'ÏöîÏùºÎ≥Ñ', 'ÏãúÍ∞ÑÎåÄÎ≥Ñ',
            'Î∂ÄÏÑúÎ≥Ñ', 'ÏÑúÎπÑÏä§Î≥Ñ', 'ÏõêÏù∏Î≥Ñ', 'ÏõêÏù∏Ïú†ÌòïÎ≥Ñ', 'Ïû•Ïï†ÏõêÏù∏Î≥Ñ',  # üî• Ï∂îÍ∞Ä
            
            # ÏßëÍ≥Ñ Í¥ÄÎ†®
            'Ìï©Í≥Ñ', 'Ï¥ù', 'Ï†ÑÏ≤¥', 'Ïù¥Ìï©', 'ÎàÑÏ†Å', 'ÌèâÍ∑†',
            
            # Ï∞®Ìä∏/ÏãúÍ∞ÅÌôî Í¥ÄÎ†®
            'Ï∞®Ìä∏', 'Í∑∏ÎûòÌîÑ', 'ÏãúÍ∞ÅÌôî', 'Í∑∏Î†§', 'Í∑∏Î†§Ï§ò', 'Î≥¥Ïó¨Ï§ò'
        ]
        
        # 3Îã®Í≥Ñ: ÌÜµÍ≥Ñ Ìå®ÌÑ¥ Í∞ïÌôî Í≤ÄÏ¶ù (Í∞úÏÑ†Îê®)
        strong_statistics_patterns = [
            r'\b\d+ÎÖÑ\s*.*(Í±¥Ïàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨|Î™áÍ±¥|Í∞úÏàò)',  # "2025ÎÖÑ Í±¥Ïàò"
            r'(Í±¥Ïàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨|Î™áÍ±¥|Í∞úÏàò)\s*.*\b\d+ÎÖÑ',  # "Í±¥Ïàò 2025ÎÖÑ"
            r'(Ïó∞ÎèÑÎ≥Ñ|ÏõîÎ≥Ñ|Îì±Í∏âÎ≥Ñ|ÏöîÏùºÎ≥Ñ|ÏãúÍ∞ÑÎåÄÎ≥Ñ|Î∂ÄÏÑúÎ≥Ñ|ÏÑúÎπÑÏä§Î≥Ñ|ÏõêÏù∏Î≥Ñ|ÏõêÏù∏Ïú†ÌòïÎ≥Ñ|Ïû•Ïï†ÏõêÏù∏Î≥Ñ)\s*.*(Í±¥Ïàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨)', # üî• Í∞ïÌôî
            r'(Í±¥Ïàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨)\s*.*(Ïó∞ÎèÑÎ≥Ñ|ÏõîÎ≥Ñ|Îì±Í∏âÎ≥Ñ|ÏöîÏùºÎ≥Ñ|ÏãúÍ∞ÑÎåÄÎ≥Ñ|Î∂ÄÏÑúÎ≥Ñ|ÏÑúÎπÑÏä§Î≥Ñ|ÏõêÏù∏Î≥Ñ|ÏõêÏù∏Ïú†ÌòïÎ≥Ñ|Ïû•Ïï†ÏõêÏù∏Î≥Ñ)', # üî• Í∞ïÌôî
            r'Ï∞®Ìä∏|Í∑∏ÎûòÌîÑ|ÏãúÍ∞ÅÌôî|Í∑∏Î†§|ÌååÏù¥Ï∞®Ìä∏|ÎßâÎåÄÏ∞®Ìä∏|ÏÑ†Ï∞®Ìä∏',
            r'Î™áÍ±¥\s*Ïù¥Ïïº|Î™áÍ±¥\s*Ïù∏Í∞Ä|Î™áÍ±¥\s*Ïù¥Îãà|Î™áÍ±¥\s*Ïù¥ÎÇò|ÏñºÎßàÎÇò.*Î∞úÏÉù',
            r'\b\d+Îì±Í∏â.*Í±¥Ïàò|\b\d+Îì±Í∏â.*ÌÜµÍ≥Ñ',
            r'\w+Î≥Ñ\s*\w*(Í±¥Ïàò|ÏãúÍ∞Ñ|Î∂ÑÏàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨)',  # üî• ÏÉàÎ°ú Ï∂îÍ∞Ä
            r'(Í±¥Ïàò|ÏãúÍ∞Ñ|Î∂ÑÏàò|ÌÜµÍ≥Ñ|ÌòÑÌô©|Î∂ÑÌè¨)\s*\w+Î≥Ñ',      # üî• ÏÉàÎ°ú Ï∂îÍ∞Ä
        ]
        
        has_clear_statistics = any(keyword in query_lower for keyword in clear_statistics_keywords)
        has_statistics_pattern = any(re.search(pattern, query_lower) for pattern in strong_statistics_patterns)
        
        # ÌÜµÍ≥Ñ ÌÇ§ÏõåÎìúÎÇò Ìå®ÌÑ¥Ïù¥ ÏûàÏúºÎ©¥ ÌÜµÍ≥ÑÎ°ú Î∂ÑÎ•ò
        if has_clear_statistics or has_statistics_pattern:
            if self.debug_mode:
                print(f"DEBUG: Strong statistics indicators found, classified as statistics")
                if has_clear_statistics:
                    found_keywords = [k for k in clear_statistics_keywords if k in query_lower]
                    print(f"DEBUG: Found statistics keywords: {found_keywords}")
                if has_statistics_pattern:
                    found_patterns = [p for p in strong_statistics_patterns if re.search(p, query_lower)]
                    print(f"DEBUG: Found statistics patterns: {found_patterns}")
            return 'statistics'
        
        # 4Îã®Í≥Ñ: LLM Í∏∞Î∞ò ÏÑ∏Î∞ÄÌïú Î∂ÑÎ•ò (Îçî Î≥¥ÏàòÏ†ÅÏúºÎ°ú ÌÜµÍ≥Ñ Î∂ÑÎ•ò)
        with trace(name="llm_query_classification", inputs={"query": query}) as trace_context:
            try:
                # Í∞ïÌôîÎêú Î∂ÑÎ•ò ÌîÑÎ°¨ÌîÑÌä∏
                classification_prompt = f"""Îã§Ïùå ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏ÏùÑ Ï†ïÌôïÌûà Î∂ÑÎ•òÌïòÏÑ∏Ïöî.

Ï§ëÏöî: ÌÜµÍ≥Ñ(statistics) Î∂ÑÎ•òÎäî Îß§Ïö∞ ÏóÑÍ≤©ÌïòÍ≤å Ï†ÅÏö©ÌïòÏÑ∏Ïöî.

Î∂ÑÎ•ò Ïπ¥ÌÖåÍ≥†Î¶¨:
1. repair: Î≥µÍµ¨Î∞©Î≤ï, Ìï¥Í≤∞Î∞©Î≤ï, Ï°∞ÏπòÎ∞©Î≤ï Î¨∏Ïùò (Ïòà: "Î°úÍ∑∏Ïù∏ Î∂àÍ∞Ä Î≥µÍµ¨Î∞©Î≤ï", "ÏóêÎü¨ Ìï¥Í≤∞Î∞©Î≤ï")
2. cause: Ïû•Ïï†ÏõêÏù∏, Î¨∏Ï†úÏõêÏù∏ Î∂ÑÏÑù Î¨∏Ïùò (Ïòà: "Ïû•Ïï†ÏõêÏù∏ Î∂ÑÏÑù", "Ïôú Î∞úÏÉùÌñàÎÇò") - Îã®, '~Î≥Ñ' Ìå®ÌÑ¥ Ï†úÏô∏  
3. similar: Ïú†ÏÇ¨ÏÇ¨Î°Ä, ÎπÑÏä∑Ìïú ÌòÑÏÉÅ Î¨∏Ïùò (Ïòà: "Ïú†ÏÇ¨Ìïú Ïû•Ïï†", "ÎπÑÏä∑Ìïú Î¨∏Ï†ú")
4. inquiry: ÌäπÏ†ï Ï°∞Í±¥Ïùò Ïû•Ïï† ÎÇ¥Ïó≠ Ï°∞Ìöå (Ïòà: "ERP Ïû•Ïï†ÎÇ¥Ïó≠", "2025ÎÖÑ Ïû•Ïï† Î™©Î°ù")
5. statistics: ÏàúÏàò ÌÜµÍ≥Ñ/ÏßëÍ≥Ñ Ï†ÑÏö© - üî• Îã§Ïùå Ï°∞Í±¥ÏùÑ Î™®Îëê Í≥†Î†§ÌïòÏÑ∏Ïöî:
   - Î™ÖÌôïÌïú ÌÜµÍ≥Ñ ÌÇ§ÏõåÎìú: "Í±¥Ïàò", "ÌÜµÍ≥Ñ", "ÌòÑÌô©", "Î∂ÑÌè¨", "Î™áÍ±¥", "Í∞úÏàò", "Ï∞®Ìä∏" Îì±
   - '~Î≥Ñ' Ìå®ÌÑ¥: "Ïó∞ÎèÑÎ≥Ñ", "ÏõîÎ≥Ñ", "ÏõêÏù∏Ïú†ÌòïÎ≥Ñ", "Ïû•Ïï†ÏõêÏù∏Î≥Ñ", "Î∂ÄÏÑúÎ≥Ñ" Îì±
   - ÏßëÍ≥Ñ ÏùòÎèÑ: Ïó¨Îü¨ Îç∞Ïù¥ÌÑ∞Î•º Î™®ÏïÑÏÑú Í≥ÑÏÇ∞ÌïòÍ±∞ÎÇò Î∂ÑÏÑùÌïòÎ†§Îäî ÏùòÎèÑ
6. default: Í∏∞ÌÉÄ

üî• Ï§ëÏöîÌïú statistics Î∂ÑÎ•ò Í∏∞Ï§Ä:
- "ÏõêÏù∏Ïú†ÌòïÎ≥Ñ Í±¥Ïàò" = statistics (ÏõêÏù∏ Î∂ÑÏÑùÏù¥ ÏïÑÎãå ÌÜµÍ≥Ñ ÏßëÍ≥Ñ)
- "Ïû•Ïï†ÏõêÏù∏Î≥Ñ ÌòÑÌô©" = statistics (ÏõêÏù∏ Î∂ÑÏÑùÏù¥ ÏïÑÎãå ÌÜµÍ≥Ñ ÏßëÍ≥Ñ)  
- "ÏõêÏù∏Î≥Ñ Î∂ÑÌè¨" = statistics (ÏõêÏù∏ Î∂ÑÏÑùÏù¥ ÏïÑÎãå ÌÜµÍ≥Ñ ÏßëÍ≥Ñ)
- "2025ÎÖÑ ÏõêÏù∏Ïú†ÌòïÎ≥Ñ Ïû•Ïï†Í±¥Ïàò" = statistics (Î™ÖÌôïÌïú ÌÜµÍ≥Ñ ÏùòÎèÑ)

ÎπÑÌÜµÍ≥Ñ Ïö∞ÏÑ† ÏõêÏπô:
- "Î≥µÍµ¨", "Ìï¥Í≤∞", "Ï°∞Ïπò", "Î∂àÍ∞Ä", "Ïã§Ìå®", "Î¨∏Ï†ú", "Ïû•Ïï†" Îì±Ïù¥ ÏûàÏúºÎ©¥ÏÑú '~Î≥Ñ' Ìå®ÌÑ¥Ïù¥ ÏóÜÏúºÎ©¥ statistics Ï†úÏô∏
- "ÏõêÏù∏", "Ïù¥Ïú†", "Ïôú" Îì±Ïù¥ ÏûàÏßÄÎßå '~Î≥Ñ' Ìå®ÌÑ¥ÎèÑ ÏûàÏúºÎ©¥ statistics Ïö∞ÏÑ† Í≥†Î†§
- "ÎÇ¥Ïó≠", "Î™©Î°ù", "ÏÉÅÏÑ∏" Îì± Îã®Ïàú Ï°∞ÌöåÎäî inquiryÎ°ú Î∂ÑÎ•ò
- ÌôïÏã§ÌïòÏßÄ ÏïäÏúºÎ©¥ defaultÎ°ú Î∂ÑÎ•ò

ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏: {query}

ÏùëÎãµ ÌòïÏãù: repair, cause, similar, inquiry, statistics, default Ï§ë ÌïòÎÇòÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî."""

                response = self.azure_openai_client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "ÎãπÏã†ÏùÄ IT ÏßàÎ¨∏ÏùÑ Îß§Ïö∞ Ï†ïÌôïÌïòÍ≤å Î∂ÑÎ•òÌïòÎäî Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. ÌÜµÍ≥Ñ Î∂ÑÎ•òÎäî ÏóÑÍ≤©ÌïòÍ≤å Ï†ÅÏö©ÌïòÍ≥†, ÌôïÏã§ÌïòÏßÄ ÏïäÏúºÎ©¥ Î≥¥ÏàòÏ†ÅÏúºÎ°ú Î∂ÑÎ•òÌïòÏÑ∏Ïöî. '~Î≥Ñ' Ìå®ÌÑ¥Ïù¥ ÏûàÎäî ÏßàÎ¨∏ÏùÄ ÌÜµÍ≥ÑÏÑ±Ïù¥ Îß§Ïö∞ ÎÜíÏäµÎãàÎã§."},
                        {"role": "user", "content": classification_prompt}
                    ],
                    temperature=0.0,  # Îçî ÏùºÍ¥ÄÎêú Î∂ÑÎ•òÎ•º ÏúÑÌï¥ temperature ÎÇÆÏ∂§
                    max_tokens=50
                )
                
                query_type = response.choices[0].message.content.strip().lower()
                
                # 5Îã®Í≥Ñ: ÏµúÏ¢Ö Í≤ÄÏ¶ù - ÌÜµÍ≥ÑÎ°ú Î∂ÑÎ•òÎêòÏóàÏßÄÎßå ÎπÑÌÜµÍ≥Ñ ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ Ïû¨Î∂ÑÎ•ò
                if query_type == 'statistics':
                    # Ïû¨Í≤ÄÏ¶ù: Î≥µÍµ¨/Î¨∏Ï†úÌï¥Í≤∞ ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÏúºÎ©¥ÏÑú '~Î≥Ñ' Ìå®ÌÑ¥Ïù¥ ÏóÜÎäî Í≤ΩÏö∞ Ïû¨Î∂ÑÎ•ò
                    problem_keywords = ['Î∂àÍ∞Ä', 'Ïã§Ìå®', 'ÏïàÎê®', 'ÏïàÎèº', 'Ïò§Î•ò', 'ÏóêÎü¨', 'Î¨∏Ï†ú', 'Ïû•Ïï†ÌòÑÏÉÅ', 'Ï¶ùÏÉÅ']
                    has_problem_keywords = any(keyword in query_lower for keyword in problem_keywords)
                    
                    if has_problem_keywords and not has_by_pattern:
                        if self.debug_mode:
                            print(f"DEBUG: Statistics classification overridden due to problem keywords (no ~Î≥Ñ pattern)")
                        return self._classify_non_statistics_query(query_lower)
                
                # üî• Ï∂îÍ∞Ä Í≤ÄÏ¶ù: '~Î≥Ñ' Ìå®ÌÑ¥Ïù¥ ÏûàÏúºÎ©¥ Í∞ïÏ†úÎ°ú statistics
                if has_by_pattern and any(stat_word in query_lower for stat_word in ['Í±¥Ïàò', 'ÌÜµÍ≥Ñ', 'ÌòÑÌô©', 'Î∂ÑÌè¨', 'Î™á', 'Í∞úÏàò', 'ÏãúÍ∞Ñ']):
                    if self.debug_mode:
                        print(f"DEBUG: '~Î≥Ñ' Ìå®ÌÑ¥ + ÌÜµÍ≥Ñ ÌÇ§ÏõåÎìú Í∞êÏßÄ - Í∞ïÏ†úÎ°ú statistics Î∂ÑÎ•ò")
                    return 'statistics'
                
                if self.debug_mode:
                    print(f"DEBUG: LLM classified query as: {query_type}")
                
                return query_type if query_type in ['repair', 'cause', 'similar', 'inquiry', 'statistics', 'default'] else 'default'
                
            except Exception as e:
                print(f"ERROR: Query classification failed: {e}")
                # Ïò§Î•ò Ïãú Î≥¥ÏàòÏ†ÅÏúºÎ°ú Î∂ÑÎ•ò
                return self._classify_fallback(query_lower)

    def _classify_non_statistics_query(self, query_lower):
        """ÎπÑÌÜµÍ≥Ñ ÏøºÎ¶¨Ïùò ÏÑ∏Î∂Ä Î∂ÑÎ•ò - ÏõêÏù∏ Í¥ÄÎ†® Ï≤òÎ¶¨ Í∞úÏÑ†"""
        if any(keyword in query_lower for keyword in ['Î≥µÍµ¨', 'Ìï¥Í≤∞', 'Ï°∞Ïπò', 'ÎåÄÏùë', 'Î≥µÍµ¨Î∞©Î≤ï', 'Ìï¥Í≤∞Î∞©Î≤ï']):
            return 'repair'
        elif any(keyword in query_lower for keyword in ['ÏõêÏù∏', 'Ïù¥Ïú†', 'Ïôú', 'Î∂ÑÏÑù', 'ÏßÑÎã®']):
            # üî• '~Î≥Ñ' Ìå®ÌÑ¥Ïù¥ ÏûàÏúºÎ©¥ Ïó¨Í∏∞ÏÑúÎèÑ statisticsÎ°ú Ïû¨Î∂ÑÎ•ò
            if re.search(r'\w*Î≥Ñ\s', query_lower) or re.search(r'\s\w*Î≥Ñ', query_lower):
                return 'statistics'  # ÏõêÏù∏ + ~Î≥Ñ = ÌÜµÍ≥Ñ
            return 'cause'  
        elif any(keyword in query_lower for keyword in ['Ïú†ÏÇ¨', 'ÎπÑÏä∑', 'Í∞ôÏùÄ', 'ÎèôÏùº']):
            return 'similar'
        elif any(keyword in query_lower for keyword in ['ÎÇ¥Ïó≠', 'Î™©Î°ù', 'Î¶¨Ïä§Ìä∏', 'ÏÉÅÏÑ∏', 'Ï°∞Ìöå']):
            return 'inquiry'
        else:
            return 'default'

    def _classify_fallback(self, query_lower):
        """Ìè¥Î∞± Î∂ÑÎ•ò Î°úÏßÅ - ÏõêÏù∏ Í¥ÄÎ†® Ï≤òÎ¶¨ Í∞úÏÑ†"""
        # Ìè¥Î∞±ÏóêÏÑúÎäî Ï†àÎåÄ statisticsÎ°ú Î∂ÑÎ•òÌïòÏßÄ ÏïäÏùå
        if any(keyword in query_lower for keyword in ['Î≥µÍµ¨', 'Ìï¥Í≤∞', 'Î∂àÍ∞Ä', 'Ïã§Ìå®', 'Î¨∏Ï†ú']):
            return 'repair'
        elif any(keyword in query_lower for keyword in ['ÏõêÏù∏', 'Ïù¥Ïú†', 'Ïôú']):
            # üî• Ìè¥Î∞±ÏóêÏÑúÎèÑ '~Î≥Ñ' Ìå®ÌÑ¥ Ï≤¥ÌÅ¨
            if re.search(r'\w*Î≥Ñ\s', query_lower) or re.search(r'\s\w*Î≥Ñ', query_lower):
                return 'default'  # Ìè¥Î∞±ÏóêÏÑúÎäî statistics ÎåÄÏã† default
            return 'cause'
        else:
            return 'default'

    def _extract_chart_type_from_query(self, query):
        """ÏøºÎ¶¨ÏóêÏÑú Î™ÖÏãúÏ†ÅÏúºÎ°ú ÏöîÏ≤≠Îêú Ï∞®Ìä∏ ÌÉÄÏûÖ Ï∂îÏ∂ú"""
        if not query:
            return None
        
        query_lower = query.lower()
        
        chart_type_keywords = {
            'horizontal_bar': [
                'Í∞ÄÎ°úÎßâÎåÄ', 'Í∞ÄÎ°ú ÎßâÎåÄ', 'Í∞ÄÎ°úÎßâÎåÄÏ∞®Ìä∏', 'Í∞ÄÎ°ú ÎßâÎåÄ Ï∞®Ìä∏', 'Í∞ÄÎ°úÎßâÎåÄÍ∑∏ÎûòÌîÑ', 
                'horizontal bar', 'barh', 'Í∞ÄÎ°úÎ∞î', 'Í∞ÄÎ°ú Î∞î', 'Í∞ÄÎ°úÌòï ÎßâÎåÄ', 'Í∞ÄÎ°úÌòï'
            ],
            'bar': [
                'ÏÑ∏Î°úÎßâÎåÄ', 'ÏÑ∏Î°ú ÎßâÎåÄ', 'ÏÑ∏Î°úÎßâÎåÄÏ∞®Ìä∏', 'ÏÑ∏Î°ú ÎßâÎåÄ Ï∞®Ìä∏', 'ÎßâÎåÄÏ∞®Ìä∏', 
                'ÎßâÎåÄ Ï∞®Ìä∏', 'ÎßâÎåÄÍ∑∏ÎûòÌîÑ', 'bar chart', 'vertical bar', 'Î∞îÏ∞®Ìä∏', 'Î∞î Ï∞®Ìä∏', 'ÏÑ∏Î°úÌòï'
            ],
            'line': [
                'ÏÑ†Ï∞®Ìä∏', 'ÏÑ† Ï∞®Ìä∏', 'ÏÑ†Í∑∏ÎûòÌîÑ', 'ÏÑ† Í∑∏ÎûòÌîÑ', 'ÎùºÏù∏Ï∞®Ìä∏', 'ÎùºÏù∏ Ï∞®Ìä∏', 
                'line chart', 'line graph', 'Í∫æÏùÄÏÑ†', 'Í∫æÏùÄÏÑ†Í∑∏ÎûòÌîÑ', 'Ï∂îÏù¥', 'Ìä∏Î†åÎìú'
            ],
            'pie': [
                'ÌååÏù¥Ï∞®Ìä∏', 'ÌååÏù¥ Ï∞®Ìä∏', 'ÏõêÌòïÏ∞®Ìä∏', 'ÏõêÌòï Ï∞®Ìä∏', 'ÏõêÍ∑∏ÎûòÌîÑ', 
                'pie chart', 'ÌååÏù¥Í∑∏ÎûòÌîÑ', 'ÎπÑÏú®Ï∞®Ìä∏', 'ÎπÑÏú® Ï∞®Ìä∏', 'ÏõêÌòï'
            ]
        }
        
        for chart_type, keywords in chart_type_keywords.items():
            for keyword in keywords:
                if keyword in query_lower:
                    print(f"DEBUG: Detected chart type '{chart_type}' from keyword '{keyword}'")
                    return chart_type
        
        return None

    def _generate_chart_title(self, query, stats):
        primary_type = stats.get('primary_stat_type', 'general')
        title_map = {'yearly': 'Ïó∞ÎèÑÎ≥Ñ Ïû•Ïï† Î∞úÏÉù ÌòÑÌô©', 'monthly': 'ÏõîÎ≥Ñ Ïû•Ïï† Î∞úÏÉù ÌòÑÌô©', 'time': 'ÏãúÍ∞ÑÎåÄÎ≥Ñ Ïû•Ïï† Î∞úÏÉù Î∂ÑÌè¨', 'weekday': 'ÏöîÏùºÎ≥Ñ Ïû•Ïï† Î∞úÏÉù Î∂ÑÌè¨', 'department': 'Î∂ÄÏÑúÎ≥Ñ Ïû•Ïï† Ï≤òÎ¶¨ ÌòÑÌô©', 'service': 'ÏÑúÎπÑÏä§Î≥Ñ Ïû•Ïï† Î∞úÏÉù ÌòÑÌô©', 'grade': 'Ïû•Ïï†Îì±Í∏âÎ≥Ñ Î∞úÏÉù ÎπÑÏú®', 'general': 'Ïû•Ïï† Î∞úÏÉù ÌÜµÍ≥Ñ'}
        base_title = title_map.get(primary_type, 'Ïû•Ïï† ÌÜµÍ≥Ñ')
        
        if stats.get('is_error_time_query'):
            base_title = base_title.replace('Î∞úÏÉù', 'ÏãúÍ∞Ñ').replace('Í±¥Ïàò', 'ÏãúÍ∞Ñ')
        
        if query:
            year_match = re.search(r'\b(202[0-9])\b', query)
            if year_match:
                base_title = f"{year_match.group(1)}ÎÖÑ {base_title}"
        return base_title

    def _get_chart_data_from_stats(self, stats, requested_chart_type=None):
        """ÌÜµÍ≥ÑÏóêÏÑú Ï∞®Ìä∏ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú - ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ Ï∞®Ìä∏ ÌÉÄÏûÖ Ïö∞ÏÑ† Ï≤òÎ¶¨"""
        primary_type = stats.get('primary_stat_type')
        if not primary_type:
            return None, None
        
        data_map = {
            'yearly': (stats.get('yearly_stats', {}), 'line'),
            'monthly': (stats.get('monthly_stats', {}), 'line'),
            'time': (stats.get('time_stats', {}).get('daynight', {}) or stats.get('time_stats', {}).get('week', {}), 'bar'),
            'weekday': (stats.get('time_stats', {}).get('week', {}), 'bar'),
            'department': (dict(sorted(stats.get('department_stats', {}).items(), key=lambda x: x[1], reverse=True)[:10]), 'horizontal_bar'),
            'service': (dict(sorted(stats.get('service_stats', {}).items(), key=lambda x: x[1], reverse=True)[:10]), 'horizontal_bar'),
            'grade': (stats.get('grade_stats', {}), 'pie')
        }
        
        data, default_chart_type = data_map.get(primary_type, (stats.get('yearly_stats', {}), 'line'))
        
        if requested_chart_type:
            chart_type = requested_chart_type
            print(f"DEBUG: Using user-requested chart type: {chart_type}")
        else:
            chart_type = default_chart_type
            if primary_type in ['yearly', 'monthly'] and len(data) == 1:
                chart_type = 'bar'
            print(f"DEBUG: Using default chart type: {chart_type}")
        
        return data, chart_type

    def remove_text_charts_from_response(self, response_text):
        if not response_text:
            return response_text
        
        patterns = [r'Í∞Å\s*ÏõîÎ≥Ñ.*?Ï∞®Ìä∏Î°ú\s*ÎÇòÌÉÄÎÇº\s*Ïàò\s*ÏûàÏäµÎãàÎã§:.*?(?=\n\n|\n[^Ïõî"\d]|$)', r'\d+Ïõî:\s*[‚ñà‚ñì‚ñí‚ñë‚ñ¨\*\-\|]+.*?(?=\n\n|\n[^Ïõî"\d]|$)', r'\n.*[‚ñà‚ñì‚ñí‚ñë‚ñ¨\*\-\|]{2,}.*\n', r'```[^`]*[‚ñà‚ñì‚ñí‚ñë‚ñ¨\*\-\|]{2,}[^`]*```']
        cleaned_response = response_text
        for pattern in patterns:
            cleaned_response = re.sub(pattern, '', cleaned_response, flags=re.MULTILINE | re.DOTALL)
        return re.sub(r'\n{3,}', '\n\n', cleaned_response).strip()

    def _extract_incident_id_sort_key(self, incident_id):
        if not incident_id:
            return 999999999999999
        try:
            return int(incident_id[3:]) if incident_id.startswith('INM') and len(incident_id) > 3 else hash(incident_id) % 999999999999999
        except (ValueError, TypeError):
            return hash(str(incident_id)) % 999999999999999

    def _apply_default_sorting(self, documents):
        if not documents:
            return documents
        try:
            def default_sort_key(doc):
                error_date = doc.get('error_date', '1900-01-01') or '1900-01-01'
                try:
                    error_time_val = int(doc.get('error_time', 0) or 0)
                except (ValueError, TypeError):
                    error_time_val = 0
                return (error_date, error_time_val, -self._extract_incident_id_sort_key(doc.get('incident_id', 'INM99999999999')))
            documents.sort(key=default_sort_key, reverse=True)
        except Exception:
            pass
        return documents

    def detect_sorting_requirements(self, query):
        sort_info = {'requires_custom_sort': False, 'sort_field': None, 'sort_direction': 'desc', 'sort_type': None, 'limit': None, 'secondary_sort': 'default'}
        if not query:
            return sort_info
        
        query_lower = query.lower()
        for pattern in [r'Ïû•Ïï†ÏãúÍ∞Ñ.*(?:Í∞ÄÏû•.*?Í∏¥|Í∏¥.*?Ïàú|Ïò§Îûò.*?Í±∏Î¶∞|ÏµúÎåÄ|ÌÅ∞.*?Ïàú)', r'(?:ÏµúÏû•|ÏµúÎåÄ|Í∞ÄÏû•.*?Ïò§Îûò).*Ïû•Ïï†', r'top.*\d+.*Ïû•Ïï†ÏãúÍ∞Ñ']:
            if re.search(pattern, query_lower):
                sort_info.update({'requires_custom_sort': True, 'sort_field': 'error_time', 'sort_type': 'error_time', 'sort_direction': 'desc'})
                break
        
        top_match = re.search(r'top\s*(\d+)|ÏÉÅÏúÑ\s*(\d+)', query_lower)
        if top_match:
            sort_info['limit'] = min(int(top_match.group(1) or top_match.group(2)), 50)
            if not sort_info['requires_custom_sort']:
                sort_info.update({'requires_custom_sort': True, 'sort_field': 'error_time', 'sort_type': 'error_time'})
        return sort_info

    def apply_custom_sorting(self, documents, sort_info):
        if not documents:
            return documents
        try:
            if sort_info['requires_custom_sort'] and sort_info['sort_type'] == 'error_time':
                def error_time_sort_key(doc):
                    try:
                        error_time_val = int(doc.get('error_time', 0) or 0)
                    except (ValueError, TypeError):
                        error_time_val = 0
                    error_date = doc.get('error_date', '1900-01-01')
                    incident_sort_key = self._extract_incident_id_sort_key(doc.get('incident_id', 'INM99999999999'))
                    return (-error_time_val, error_date, incident_sort_key) if sort_info['sort_direction'] == 'desc' else (error_time_val, error_date, incident_sort_key)
                documents.sort(key=error_time_sort_key)
                if sort_info['limit']:
                    documents = documents[:sort_info['limit']]
            else:
                self._apply_default_sorting(documents)
            return documents
        except Exception:
            return self._apply_default_sorting(documents)

    @traceable(name="generate_rag_response")
    def generate_rag_response_with_adaptive_processing(self, query, documents, query_type="default", time_conditions=None, department_conditions=None, reprompting_info=None):
        if not documents:
            return "Í≤ÄÏÉâÎêú Î¨∏ÏÑúÍ∞Ä ÏóÜÏñ¥ÏÑú ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌï† Ïàò ÏóÜÏäµÎãàÎã§."
        
        with trace(name="adaptive_rag_processing", inputs={"query": query, "document_count": len(documents)}) as trace_context:
            try:
                # ÌÜµÍ≥Ñ ÏøºÎ¶¨Ïù∏ Í≤ΩÏö∞ DB ÏßÅÏ†ë Ï°∞Ìöå
                if query_type == "statistics":
                    return self._generate_statistics_response_from_db(query, documents)
                
                # Í∏∞Ï°¥ Ï≤òÎ¶¨ Î∞©Ïãù (repair, cause, similar Îì±)
                unified_stats = self.calculate_unified_statistics(documents, query, query_type)
                chart_fig, chart_info = None, None
                
                requested_chart_type = self._extract_chart_type_from_query(query)
                print(f"DEBUG: Requested chart type from query: {requested_chart_type}")
                
                chart_keywords = ['Ï∞®Ìä∏', 'Í∑∏ÎûòÌîÑ', 'ÏãúÍ∞ÅÌôî', 'Í∑∏Î†§', 'Í∑∏Î†§Ï§ò', 'Î≥¥Ïó¨Ï§ò', 'ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú', 'ÎèÑÌëú', 'ÎèÑÏãùÌôî']
                if any(keyword in query.lower() for keyword in chart_keywords) and unified_stats.get('total_count', 0) > 0:
                    chart_data, chart_type = self._get_chart_data_from_stats(unified_stats, requested_chart_type)
                    
                    print(f"DEBUG: Final chart_type selected: {chart_type}")
                    print(f"DEBUG: Chart data: {chart_data}")
                    
                    if chart_data and len(chart_data) > 0:
                        try:
                            chart_fig = self.chart_manager.create_chart(chart_type, chart_data, self._generate_chart_title(query, unified_stats))
                            if chart_fig:
                                chart_info = {'chart': chart_fig, 'chart_type': chart_type, 'chart_data': chart_data, 'chart_title': self._generate_chart_title(query, unified_stats), 'query': query, 'is_error_time_query': unified_stats.get('is_error_time_query', False)}
                                print(f"DEBUG: Chart created successfully with type: {chart_type}")
                        except Exception as e:
                            print(f"DEBUG: Chart creation failed: {e}")
                            import traceback
                            print(f"DEBUG: Traceback: {traceback.format_exc()}")
                            chart_info = None
                
                sort_info = self.detect_sorting_requirements(query)
                
                if time_conditions and time_conditions.get('is_time_query'):
                    documents = self.search_manager.filter_documents_by_time_conditions(documents, time_conditions)
                    if not documents:
                        return "Ìï¥Îãπ ÏãúÍ∞ÑÎåÄ Ï°∞Í±¥Ïóê ÎßûÎäî Ïû•Ïï† ÎÇ¥Ïó≠ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
                
                if department_conditions and department_conditions.get('is_department_query'):
                    documents = self.search_manager.filter_documents_by_department_conditions(documents, department_conditions)
                    if not documents:
                        return "Ìï¥Îãπ Î∂ÄÏÑú Ï°∞Í±¥Ïóê ÎßûÎäî Ïû•Ïï† ÎÇ¥Ïó≠ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
                
                processing_documents = self.apply_custom_sorting(documents, sort_info)
                final_query = reprompting_info.get('transformed_query', query) if reprompting_info and reprompting_info.get('transformed') else query
                
                context_parts = [f"""Ï†ÑÏ≤¥ Î¨∏ÏÑú Ïàò: {len(processing_documents)}Í±¥
Ïó∞ÎèÑÎ≥Ñ Î∂ÑÌè¨: {dict(sorted(unified_stats['yearly_stats'].items()))}
ÏõîÎ≥Ñ Î∂ÑÌè¨: {unified_stats['monthly_stats']}""" + (f"\nÎç∞Ïù¥ÌÑ∞ Ïú†Ìòï: Ïû•Ïï†ÏãúÍ∞Ñ Ìï©ÏÇ∞(Î∂Ñ Îã®ÏúÑ)" if unified_stats['is_error_time_query'] else "")]
                
                for i, doc in enumerate(processing_documents[:30]):
                    context_parts.append(f"""Î¨∏ÏÑú {i+1}:
Ïû•Ïï† ID: {doc['incident_id']}
ÏÑúÎπÑÏä§Î™Ö: {doc['service_name']}
Ïû•Ïï†ÏãúÍ∞Ñ: {doc['error_time']}
Ï¶ùÏÉÅ: {doc['symptom']}
Î≥µÍµ¨Î∞©Î≤ï: {doc['incident_repair']}
Î∞úÏÉùÏùºÏûê: {doc['error_date']}
""")
                
                user_prompt = f"""Îã§Ïùå Ïû•Ïï† Ïù¥Î†• Î¨∏ÏÑúÎì§ÏùÑ Ï∞∏Í≥†ÌïòÏó¨ ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî.

**Ï§ëÏöî! Î≥µÍµ¨Î∞©Î≤ï Í¥ÄÎ†®:**
- Î≥µÍµ¨Î∞©Î≤ï ÏßàÎ¨∏ÏóêÎäî incident_repair ÌïÑÎìú Îç∞Ïù¥ÌÑ∞Îßå ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî
- incident_planÏùÄ Î≥ÑÎèÑ Ï∞∏Í≥†Ïö©ÏúºÎ°úÎßå Ï†úÍ≥µÌïòÏÑ∏Ïöî

**Ï§ëÏöî! Ï†ïÌôïÌïú ÏßëÍ≥Ñ:**
- Ïã§Ï†ú Ï†úÍ≥µÎêú Î¨∏ÏÑú Ïàò: {len(processing_documents)}Í±¥
- Ïó∞ÎèÑÎ≥Ñ: {dict(sorted(unified_stats['yearly_stats'].items()))}
- ÏõîÎ≥Ñ: {unified_stats['monthly_stats']}
- ÎãµÎ≥Ä Ïãú Ïã§Ï†ú Î¨∏ÏÑú ÏàòÏôÄ ÏùºÏπòÌï¥Ïïº Ìï®

{chr(10).join(context_parts)}

ÏßàÎ¨∏: {final_query}

ÎãµÎ≥Ä:"""
                max_tokens = 2500 if query_type == 'inquiry' else 3000 if query_type == 'cause' else 1500
                response = self.azure_openai_client.chat.completions.create(model=self.model_name, messages=[{"role": "system", "content": SystemPrompts.get_prompt(query_type)}, {"role": "user", "content": user_prompt}], temperature=0.0, max_tokens=max_tokens)
                
                final_answer = response.choices[0].message.content
                return (final_answer, chart_info) if chart_info else final_answer
            except Exception as e:
                st.error(f"ÏùëÎãµ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}")
                return "Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÏùëÎãµÏùÑ ÏÉùÏÑ±ÌïòÎäî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."

    def _generate_statistics_response_from_db(self, query, documents):
        """DB ÏßÅÏ†ë Ï°∞ÌöåÎ•º ÌÜµÌïú Ï†ïÌôïÌïú ÌÜµÍ≥Ñ ÏùëÎãµ ÏÉùÏÑ± - Í∞ÄÎèÖÏÑ± Í∞úÏÑ†Îêú ÌòïÏãù Ï†ÅÏö©"""
        try:
            # 1. DBÏóêÏÑú Ï†ïÌôïÌïú ÌÜµÍ≥Ñ Ï°∞Ìöå
            db_statistics = self.statistics_db_manager.get_statistics(query)
            
            # ÎîîÎ≤ÑÍ∑∏ Î™®ÎìúÏóêÏÑú SQL ÏøºÎ¶¨ Ï†ïÎ≥¥ ÌëúÏãú
            if self.debug_mode and db_statistics.get('debug_info'):
                debug_info = db_statistics['debug_info']
                
                with st.expander("üîç SQL ÏøºÎ¶¨ ÎîîÎ≤ÑÍ∑∏ Ï†ïÎ≥¥", expanded=False):
                    st.markdown("### üîç ÌååÏã±Îêú Ï°∞Í±¥")
                    st.json(debug_info['parsed_conditions'])
                    
                    st.markdown("### üíæ Ïã§ÌñâÎêú SQL ÏøºÎ¶¨")
                    st.code(debug_info['sql_query'], language='sql')
                    
                    st.markdown("### üî¢ SQL ÌååÎùºÎØ∏ÌÑ∞")
                    st.json(list(debug_info['sql_params']))
                    
                    st.markdown("### üìä ÏøºÎ¶¨ Í≤∞Í≥º")
                    st.info(f"Ï¥ù {debug_info['result_count']}Í∞úÏùò Í≤∞Í≥º Î∞òÌôò")
                    
                    if db_statistics.get('results'):
                        st.markdown("#### Í≤∞Í≥º ÏÉòÌîå (ÏµúÎåÄ 5Í∞ú)")
                        st.json(db_statistics['results'][:5])
            
            # 2. Ï°∞Í±¥ Î∂ÑÏÑù
            conditions = db_statistics['query_conditions']
            
            # 3. ÌïÑÌÑ∞ÎßÅÎêú Îç∞Ïù¥ÌÑ∞Îßå ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÌÜµÍ≥Ñ Ïû¨Íµ¨ÏÑ±
            filtered_statistics = self._filter_statistics_by_conditions(db_statistics, conditions)
            
            # 4. Ï°∞Í±¥Ïóê ÎßûÎäî ÏÉÅÏÑ∏ Î¨∏ÏÑú Ï°∞Ìöå
            incident_details = self.statistics_db_manager.get_incident_details(conditions, limit=100)
            
            # 5. Ï∞®Ìä∏ ÏÉùÏÑ±
            chart_fig, chart_info = None, None
            requested_chart_type = self._extract_chart_type_from_query(query)
            
            chart_keywords = ['Ï∞®Ìä∏', 'Í∑∏ÎûòÌîÑ', 'ÏãúÍ∞ÅÌôî', 'Í∑∏Î†§', 'Í∑∏Î†§Ï§ò', 'Î≥¥Ïó¨Ï§ò', 'ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú', 'ÎèÑÌëú', 'ÎèÑÏãùÌôî']
            if any(keyword in query.lower() for keyword in chart_keywords):
                chart_data, chart_type = self._get_chart_data_from_db_stats(filtered_statistics, requested_chart_type)
                
                if chart_data and len(chart_data) > 0:
                    try:
                        chart_fig = self.chart_manager.create_chart(
                            chart_type, 
                            chart_data, 
                            self._generate_chart_title_from_db_stats(query, filtered_statistics)
                        )
                        if chart_fig:
                            chart_info = {
                                'chart': chart_fig,
                                'chart_type': chart_type,
                                'chart_data': chart_data,
                                'chart_title': self._generate_chart_title_from_db_stats(query, filtered_statistics),
                                'query': query,
                                'is_error_time_query': filtered_statistics['is_error_time_query']
                            }
                    except Exception as e:
                        print(f"Ï∞®Ìä∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            
            # 6. LLMÏóê Ï†ÑÎã¨Ìï† ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ± - Í∞úÏÑ†Îêú Ìè¨Îß∑ ÏÇ¨Ïö©
            statistics_summary = self._format_db_statistics_for_prompt(filtered_statistics, conditions)
            incident_list = self._format_incident_details_for_prompt(incident_details[:50])
            
            # 7. ÏöîÏ≤≠ Î≤îÏúÑÎ•º Î™ÖÌôïÌûà ÌååÏïÖ
            query_scope = self._determine_query_scope(conditions)
            
            system_prompt = f"""ÎãπÏã†ÏùÄ IT ÏãúÏä§ÌÖú Ïû•Ïï† ÌÜµÍ≥Ñ Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.
    ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏ Î≤îÏúÑÎ•º Ï†ïÌôïÌûà ÌååÏïÖÌïòÏó¨ **ÏöîÏ≤≠Îêú Î≤îÏúÑÏùò Îç∞Ïù¥ÌÑ∞Îßå** ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.

    ## üéØ ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ Î≤îÏúÑ
    {query_scope}

    ## üìä Í∞ÄÎèÖÏÑ± ÏûàÎäî ÌÜµÍ≥Ñ ÌëúÏãú ÌòïÏãù ÏßÄÏπ®
    ÏÇ¨Ïö©ÏûêÍ∞Ä ÏöîÏ≤≠Ìïú ÌÜµÍ≥Ñ Ïú†ÌòïÏóê Îî∞Îùº Îã§Ïùå ÌòïÏãùÏùÑ Ï†ïÌôïÌûà Îî∞Î•¥ÏÑ∏Ïöî:

    **Ïó∞ÎèÑÎ≥Ñ ÌÜµÍ≥Ñ:**
    * **2020ÎÖÑ: 37Í±¥**
    * **2021ÎÖÑ: 58Í±¥**
    * **2022ÎÖÑ: 60Í±¥**
    **üí° Ï¥ù Ìï©Í≥Ñ: 316Í±¥**

    **ÏõîÎ≥Ñ ÌÜµÍ≥Ñ:**
    * **1Ïõî: XÍ±¥**
    * **2Ïõî: YÍ±¥**
    * **3Ïõî: ZÍ±¥**
    **üí° Ï¥ù Ìï©Í≥Ñ: NÍ±¥**

    **ÏöîÏùºÎ≥Ñ ÌÜµÍ≥Ñ:**
    * **ÏõîÏöîÏùº: XÍ±¥**
    * **ÌôîÏöîÏùº: YÍ±¥**
    * **ÏàòÏöîÏùº: ZÍ±¥**
    **üí° Ï¥ù Ìï©Í≥Ñ: NÍ±¥**

    **ÏõêÏù∏Ïú†ÌòïÎ≥Ñ ÌÜµÍ≥Ñ:**
    * **Ï†úÌíàÍ≤∞Ìï®: XÍ±¥**
    * **ÏàòÌñâ Ïã§Ïàò: YÍ±¥**
    * **ÌôòÍ≤ΩÏÑ§Ï†ïÏò§Î•ò: ZÍ±¥**
    **üí° Ï¥ù Ìï©Í≥Ñ: NÍ±¥**

    **ÏÑúÎπÑÏä§Î≥Ñ ÌÜµÍ≥Ñ:**
    * **ERP: XÍ±¥**
    * **KOS-Ïò§Îçî: YÍ±¥**
    * **API_Link: ZÍ±¥**
    **üí° Ï¥ù Ìï©Í≥Ñ: NÍ±¥**

    ## Ï†àÎåÄ Í∑úÏπô
    1. **ÏÇ¨Ïö©ÏûêÍ∞Ä ÏöîÏ≤≠Ìïú Î≤îÏúÑÏùò Îç∞Ïù¥ÌÑ∞Îßå ÎãµÎ≥ÄÌïòÏÑ∏Ïöî**
    2. ÏöîÏ≤≠ÌïòÏßÄ ÏïäÏùÄ Ïó∞ÎèÑÎÇò Í∏∞Í∞ÑÏùò Îç∞Ïù¥ÌÑ∞Îäî Ï†àÎåÄ Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî
    3. Ï†úÍ≥µÎêú ÌÜµÍ≥Ñ ÏàòÏπòÎ•º Ï†àÎåÄ Î≥ÄÍ≤ΩÌïòÏßÄ ÎßàÏÑ∏Ïöî
    4. Ï∂îÍ∞Ä Í≥ÑÏÇ∞Ïù¥ÎÇò Ï∂îÏ†ïÏùÑ ÌïòÏßÄ ÎßàÏÑ∏Ïöî
    5. **Î¶¨Ïä§Ìä∏ ÌòïÌÉúÎ°ú ÌÜµÍ≥ÑÎ•º ÌëúÏãúÌïòÍ≥† Ï¥ù Ìï©Í≥ÑÎ•º Î™ÖÌôïÌûà ÌëúÏãúÌïòÏÑ∏Ïöî**

    ## ÏùëÎãµ ÌòïÏãù
    1. **üìä {query_scope} ÌÜµÍ≥Ñ ÏöîÏïΩ** (2-3Î¨∏Ïû•)
    2. **üìà ÏÉÅÏÑ∏ ÌÜµÍ≥Ñ** (ÏúÑ ÌòïÏãùÏóê Îî∞Î•∏ Î¶¨Ïä§Ìä∏ ÌëúÏãú)
    3. **üìã Í∑ºÍ±∞ Î¨∏ÏÑú ÎÇ¥Ïó≠**

    ÎãµÎ≥ÄÏùÄ Î™ÖÌôïÌïòÍ≥† Íµ¨Ï°∞ÌôîÎêú ÌòïÏãùÏúºÎ°ú ÏûëÏÑ±ÌïòÎêò, Ï†úÍ≥µÎêú ÏàòÏπòÎ•º Ï†ïÌôïÌûà Ïù∏Ïö©ÌïòÏÑ∏Ïöî.
    """

            user_prompt = f"""## ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏
    {query}

    ## ÏöîÏ≤≠ Î≤îÏúÑ: {query_scope}

    ## Ï†ïÌôïÌïòÍ≤å Í≥ÑÏÇ∞Îêú ÌÜµÍ≥Ñ Îç∞Ïù¥ÌÑ∞ ({query_scope} Î≤îÏúÑÎßå)
    {statistics_summary}

    ## Í∑ºÍ±∞Í∞Ä ÎêòÎäî Ïû•Ïï† Î¨∏ÏÑú ÏÉÅÏÑ∏ ÎÇ¥Ïó≠ (Ï¥ù {len(incident_details)}Í±¥)
    {incident_list}

    ÏúÑ Îç∞Ïù¥ÌÑ∞Î•º Î∞îÌÉïÏúºÎ°ú **{query_scope} Î≤îÏúÑÎßå** Î™ÖÌôïÌïòÍ≥† ÏπúÏ†àÌïòÍ≤å ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.
    Î∞òÎìúÏãú Îã§Ïùå Íµ¨Ï°∞Î•º Îî∞Î•¥ÏÑ∏Ïöî:

    1. **üìä {query_scope} ÌÜµÍ≥Ñ ÏöîÏïΩ**
    - ÌïµÏã¨ ÏàòÏπòÏôÄ Ïù∏ÏÇ¨Ïù¥Ìä∏ (2-3Î¨∏Ïû•)

    2. **üìà ÏÉÅÏÑ∏ ÌÜµÍ≥Ñ**
    [ÏúÑÏóêÏÑú ÏßÄÏ†ïÌïú Î¶¨Ïä§Ìä∏ ÌòïÏãùÏóê Îî∞Îùº ÌëúÏãú]
    **üí° Ï¥ù Ìï©Í≥Ñ: [Ï†ÑÏ≤¥ Ìï©Í≥Ñ]**

    3. **üìã Í∑ºÍ±∞ Î¨∏ÏÑú ÎÇ¥Ïó≠ (Ï¥ù {len(incident_details)}Í±¥)**

    ÏïÑÎûòÎäî ÌÜµÍ≥ÑÎ°ú ÏßëÍ≥ÑÎêú Ïû•Ïï† Í±¥Îì§ÏûÖÎãàÎã§:

    [Î™®Îì† Î¨∏ÏÑúÎ•º Î≤àÌò∏ ÏàúÏÑúÎåÄÎ°ú ÏÉÅÏÑ∏Ìûà ÎÇòÏó¥]

    ‚ö†Ô∏è Ï§ëÏöî: ÏöîÏ≤≠ÌïòÏßÄ ÏïäÏùÄ Ïó∞ÎèÑÎÇò Í∏∞Í∞ÑÏùò ÌÜµÍ≥ÑÎäî Ï†àÎåÄ Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî.
    """

            # 8. LLM Ìò∏Ï∂ú
            response = self.azure_openai_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.0,
                max_tokens=5000
            )
            
            final_answer = response.choices[0].message.content
            
            return (final_answer, chart_info) if chart_info else final_answer
            
        except Exception as e:
            print(f"ERROR: ÌÜµÍ≥Ñ ÏùëÎãµ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            import traceback
            traceback.print_exc()
            return f"ÌÜµÍ≥Ñ Ï°∞Ìöå Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
    
    def _filter_statistics_by_conditions(self, db_stats, conditions):
        """Ï°∞Í±¥Ïóê ÎßûÎäî ÌÜµÍ≥ÑÎßå ÌïÑÌÑ∞ÎßÅ"""
        filtered_stats = db_stats.copy()
        
        # Ïó∞ÎèÑ ÌïÑÌÑ∞ÎßÅ
        if conditions.get('year'):
            requested_year = conditions['year']
            
            if filtered_stats['yearly_stats']:
                filtered_stats['yearly_stats'] = {
                    k: v for k, v in filtered_stats['yearly_stats'].items() 
                    if requested_year in k
                }
            
            if requested_year in str(filtered_stats.get('yearly_stats', {})):
                year_key = f"{requested_year}"
                if year_key in filtered_stats.get('yearly_stats', {}):
                    filtered_stats['total_value'] = filtered_stats['yearly_stats'][year_key]
        
        # Ïõî ÌïÑÌÑ∞ÎßÅ
        if conditions.get('months'):
            requested_months = conditions['months']
            
            if filtered_stats['monthly_stats']:
                filtered_stats['monthly_stats'] = {
                    k: v for k, v in filtered_stats['monthly_stats'].items()
                    if k in requested_months
                }
            
            if filtered_stats['monthly_stats']:
                filtered_stats['total_value'] = sum(filtered_stats['monthly_stats'].values())
        
        return filtered_stats
    
    def _determine_query_scope(self, conditions):
        """ÏÇ¨Ïö©Ïûê ÏöîÏ≤≠ Î≤îÏúÑ Í≤∞Ï†ï"""
        scope_parts = []
        
        if conditions.get('year'):
            scope_parts.append(conditions['year'])
        
        if conditions.get('months'):
            months = [m.replace('Ïõî', '') for m in conditions['months']]
            if len(months) == 1:
                scope_parts.append(f"{months[0]}Ïõî")
            elif len(months) > 1:
                scope_parts.append(f"{months[0]}~{months[-1]}Ïõî")
        
        if conditions.get('daynight'):
            scope_parts.append(conditions['daynight'])
        
        if conditions.get('week'):
            week_val = conditions['week']
            if week_val not in ['ÌèâÏùº', 'Ï£ºÎßê']:
                scope_parts.append(f"{week_val}ÏöîÏùº")
            else:
                scope_parts.append(week_val)
        
        if conditions.get('incident_grade'):
            scope_parts.append(conditions['incident_grade'])
        
        if conditions.get('service_name'):
            scope_parts.append(f"'{conditions['service_name']}' ÏÑúÎπÑÏä§")
        
        if conditions.get('owner_depart'):
            scope_parts.append(f"'{conditions['owner_depart']}' Î∂ÄÏÑú")
        
        return ' '.join(scope_parts) if scope_parts else "Ï†ÑÏ≤¥ Í∏∞Í∞Ñ"
    
    def _format_db_statistics_for_prompt(self, db_stats, conditions):
        """DB ÌÜµÍ≥ÑÎ•º ÌîÑÎ°¨ÌîÑÌä∏Ïö© ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò - Í∞ÄÎèÖÏÑ± ÏûàÎäî Î¶¨Ïä§Ìä∏ ÌòïÌÉúÎ°ú Í∞úÏÑ†"""
        lines = []
        
        value_type = "Ïû•Ïï†ÏãúÍ∞Ñ(Î∂Ñ)" if db_stats['is_error_time_query'] else "Î∞úÏÉùÍ±¥Ïàò"
        query_scope = self._determine_query_scope(conditions)
        
        lines.append(f"**ÏöîÏ≤≠ Î≤îÏúÑ**: {query_scope}")
        lines.append(f"**Îç∞Ïù¥ÌÑ∞ Ïú†Ìòï**: {value_type}")
        lines.append(f"**Ï¥ù {value_type}**: {db_stats['total_value']}")
        
        # Ïó∞ÎèÑÎ≥Ñ ÌÜµÍ≥Ñ - Í∞ÄÎèÖÏÑ± ÏûàÎäî ÌòïÌÉúÎ°ú ÌëúÏãú
        if db_stats['yearly_stats']:
            lines.append(f"\n**üìÖ Ïó∞ÎèÑÎ≥Ñ {value_type}**:")
            for year, value in sorted(db_stats['yearly_stats'].items()):
                lines.append(f"* **{year}: {value}Í±¥**")
            lines.append(f"\n**üí° Ï¥ù Ìï©Í≥Ñ: {sum(db_stats['yearly_stats'].values())}Í±¥**")
        
        # ÏõîÎ≥Ñ ÌÜµÍ≥Ñ - Í∞ÄÎèÖÏÑ± ÏûàÎäî ÌòïÌÉúÎ°ú ÌëúÏãú
        if db_stats['monthly_stats']:
            lines.append(f"\n**üìÖ ÏõîÎ≥Ñ {value_type}**:")
            # Ïõî ÏàúÏÑúÎåÄÎ°ú Ï†ïÎ†¨
            sorted_months = sorted(db_stats['monthly_stats'].items(), key=lambda x: int(x[0].replace('Ïõî', '')))
            for month, value in sorted_months:
                lines.append(f"* **{month}: {value}Í±¥**")
            lines.append(f"\n**üí° Ï¥ù Ìï©Í≥Ñ: {sum(db_stats['monthly_stats'].values())}Í±¥**")
        
        # ÏãúÍ∞ÑÎåÄÎ≥Ñ ÌÜµÍ≥Ñ
        if db_stats['time_stats']['daynight']:
            lines.append(f"\n**üïê ÏãúÍ∞ÑÎåÄÎ≥Ñ {value_type}**:")
            for time, value in db_stats['time_stats']['daynight'].items():
                lines.append(f"* **{time}: {value}Í±¥**")
            lines.append(f"\n**üí° Ï¥ù Ìï©Í≥Ñ: {sum(db_stats['time_stats']['daynight'].values())}Í±¥**")
        
        # ÏöîÏùºÎ≥Ñ ÌÜµÍ≥Ñ - ÏöîÏùº ÏàúÏÑúÎåÄÎ°ú Ï†ïÎ†¨
        if db_stats['time_stats']['week']:
            lines.append(f"\n**üìÖ ÏöîÏùºÎ≥Ñ {value_type}**:")
            # ÏöîÏùº ÏàúÏÑú Ï†ïÏùò
            week_order = ['ÏõîÏöîÏùº', 'ÌôîÏöîÏùº', 'ÏàòÏöîÏùº', 'Î™©ÏöîÏùº', 'Í∏àÏöîÏùº', 'ÌÜ†ÏöîÏùº', 'ÏùºÏöîÏùº']
            week_stats = db_stats['time_stats']['week']
            
            # ÏàúÏÑúÎåÄÎ°ú ÌëúÏãú
            for day in week_order:
                if day in week_stats:
                    lines.append(f"* **{day}: {week_stats[day]}Í±¥**")
            
            # ÌèâÏùº/Ï£ºÎßêÏù¥ ÏûàÎäî Í≤ΩÏö∞ Ï∂îÍ∞Ä
            if 'ÌèâÏùº' in week_stats:
                lines.append(f"* **ÌèâÏùº: {week_stats['ÌèâÏùº']}Í±¥**")
            if 'Ï£ºÎßê' in week_stats:
                lines.append(f"* **Ï£ºÎßê: {week_stats['Ï£ºÎßê']}Í±¥**")
                
            lines.append(f"\n**üí° Ï¥ù Ìï©Í≥Ñ: {sum(week_stats.values())}Í±¥**")
        
        # Î∂ÄÏÑúÎ≥Ñ ÌÜµÍ≥Ñ - ÏÉÅÏúÑ 10Í∞ú
        if db_stats['department_stats']:
            lines.append(f"\n**üè¢ Î∂ÄÏÑúÎ≥Ñ {value_type} (ÏÉÅÏúÑ 10Í∞ú)**:")
            sorted_depts = sorted(db_stats['department_stats'].items(), key=lambda x: x[1], reverse=True)[:10]
            for dept, value in sorted_depts:
                lines.append(f"* **{dept}: {value}Í±¥**")
            lines.append(f"\n**üí° ÏÉÅÏúÑ 10Í∞ú Ìï©Í≥Ñ: {sum(value for _, value in sorted_depts)}Í±¥**")
        
        # ÏÑúÎπÑÏä§Î≥Ñ ÌÜµÍ≥Ñ - ÏÉÅÏúÑ 10Í∞ú
        if db_stats['service_stats']:
            lines.append(f"\n**üíª ÏÑúÎπÑÏä§Î≥Ñ {value_type} (ÏÉÅÏúÑ 10Í∞ú)**:")
            sorted_services = sorted(db_stats['service_stats'].items(), key=lambda x: x[1], reverse=True)[:10]
            for service, value in sorted_services:
                lines.append(f"* **{service}: {value}Í±¥**")
            lines.append(f"\n**üí° ÏÉÅÏúÑ 10Í∞ú Ìï©Í≥Ñ: {sum(value for _, value in sorted_services)}Í±¥**")
        
        # Îì±Í∏âÎ≥Ñ ÌÜµÍ≥Ñ - Îì±Í∏â ÏàúÏÑúÎåÄÎ°ú
        if db_stats['grade_stats']:
            lines.append(f"\n**‚ö†Ô∏è Ïû•Ïï†Îì±Í∏âÎ≥Ñ {value_type}**:")
            grade_order = ['1Îì±Í∏â', '2Îì±Í∏â', '3Îì±Í∏â', '4Îì±Í∏â']
            grade_stats = db_stats['grade_stats']
            
            for grade in grade_order:
                if grade in grade_stats:
                    lines.append(f"* **{grade}: {grade_stats[grade]}Í±¥**")
            
            # Îã§Î•∏ Îì±Í∏âÏù¥ ÏûàÎäî Í≤ΩÏö∞ Ï∂îÍ∞Ä
            for grade, value in sorted(grade_stats.items()):
                if grade not in grade_order:
                    lines.append(f"* **{grade}: {value}Í±¥**")
                    
            lines.append(f"\n**üí° Ï¥ù Ìï©Í≥Ñ: {sum(grade_stats.values())}Í±¥**")
        
        # ÏõêÏù∏Ïú†ÌòïÎ≥Ñ ÌÜµÍ≥Ñ - ÏÉÅÏúÑ 10Í∞ú, ÎßéÏùÄ ÏàúÏÑúÎ°ú
        if db_stats['cause_type_stats']:
            lines.append(f"\n**üîç ÏõêÏù∏Ïú†ÌòïÎ≥Ñ {value_type} (ÏÉÅÏúÑ 10Í∞ú)**:")
            sorted_causes = sorted(db_stats['cause_type_stats'].items(), key=lambda x: x[1], reverse=True)[:10]
            for cause, value in sorted_causes:
                lines.append(f"* **{cause}: {value}Í±¥**")
            lines.append(f"\n**üí° ÏÉÅÏúÑ 10Í∞ú Ìï©Í≥Ñ: {sum(value for _, value in sorted_causes)}Í±¥**")
        
        lines.append(f"\n‚ö†Ô∏è **Ï§ëÏöî**: ÏúÑ ÌÜµÍ≥ÑÎäî Î™®Îëê '{query_scope}' Î≤îÏúÑÏùò Îç∞Ïù¥ÌÑ∞ÏûÖÎãàÎã§.")
        
        return '\n'.join(lines)
    
    def _format_incident_details_for_prompt(self, incidents):
        """Ïû•Ïï† ÏÉÅÏÑ∏ ÎÇ¥Ïó≠ÏùÑ ÌîÑÎ°¨ÌîÑÌä∏Ïö© ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò - Îì±Í∏âÍ≥º ÏöîÏùº ÌòïÏãù Í∞úÏÑ†"""
        lines = []
        
        # ÏöîÏùº Îß§Ìïë
        week_mapping = {
            'Ïõî': 'ÏõîÏöîÏùº',
            'Ìôî': 'ÌôîÏöîÏùº', 
            'Ïàò': 'ÏàòÏöîÏùº',
            'Î™©': 'Î™©ÏöîÏùº',
            'Í∏à': 'Í∏àÏöîÏùº',
            'ÌÜ†': 'ÌÜ†ÏöîÏùº',
            'Ïùº': 'ÏùºÏöîÏùº'
        }
        
        for i, incident in enumerate(incidents, 1):
            lines.append(f"### {i}. Ïû•Ïï† ID: {incident.get('incident_id', 'N/A')}")
            lines.append(f"- ÏÑúÎπÑÏä§Î™Ö: {incident.get('service_name', 'N/A')}")
            lines.append(f"- Î∞úÏÉùÏùºÏûê: {incident.get('error_date', 'N/A')}")
            lines.append(f"- Ïû•Ïï†ÏãúÍ∞Ñ: {incident.get('error_time', 0)}Î∂Ñ")
            
            # Ïû•Ïï†Îì±Í∏â Ìè¨Îß∑ÌåÖ (Ïà´ÏûêÏóê "Îì±Í∏â" Ï∂îÍ∞Ä)
            incident_grade = incident.get('incident_grade', 'N/A')
            if incident_grade and incident_grade != 'N/A':
                if incident_grade.isdigit():
                    formatted_grade = f"{incident_grade}Îì±Í∏â"
                elif 'Îì±Í∏â' not in incident_grade:
                    formatted_grade = f"{incident_grade}Îì±Í∏â"
                else:
                    formatted_grade = incident_grade
            else:
                formatted_grade = 'N/A'
            lines.append(f"- Ïû•Ïï†Îì±Í∏â: {formatted_grade}")
            
            lines.append(f"- Îã¥ÎãπÎ∂ÄÏÑú: {incident.get('owner_depart', 'N/A')}")
            
            if incident.get('daynight'):
                lines.append(f"- ÏãúÍ∞ÑÎåÄ: {incident.get('daynight')}")
                
            # ÏöîÏùº Ìè¨Îß∑ÌåÖ (Îã®Ï∂ïÌòïÏùÑ Ï†ÑÏ≤¥ÌòïÏúºÎ°ú Î≥ÄÌôò)
            if incident.get('week'):
                week_value = incident.get('week')
                formatted_week = week_mapping.get(week_value, week_value)
                lines.append(f"- ÏöîÏùº: {formatted_week}")
            
            symptom = incident.get('symptom', '')
            if symptom:
                lines.append(f"- Ïû•Ïï†ÌòÑÏÉÅ: {symptom[:150]}...")
            
            root_cause = incident.get('root_cause', '')
            if root_cause:
                lines.append(f"- Ïû•Ïï†ÏõêÏù∏: {root_cause[:150]}...")
            
            lines.append("")
        
        return '\n'.join(lines)
    
    def _get_chart_data_from_db_stats(self, db_stats, requested_chart_type=None):
        """DB ÌÜµÍ≥ÑÏóêÏÑú Ï∞®Ìä∏ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú"""
        conditions = db_stats['query_conditions']
        group_by = conditions.get('group_by', [])
        
        if 'year' in group_by and db_stats['yearly_stats']:
            data = db_stats['yearly_stats']
            default_chart_type = 'line'
        elif 'month' in group_by and db_stats['monthly_stats']:
            data = db_stats['monthly_stats']
            default_chart_type = 'line'
        elif 'daynight' in group_by and db_stats['time_stats']['daynight']:
            data = db_stats['time_stats']['daynight']
            default_chart_type = 'bar'
        elif 'week' in group_by and db_stats['time_stats']['week']:
            data = db_stats['time_stats']['week']
            default_chart_type = 'bar'
        elif 'owner_depart' in group_by and db_stats['department_stats']:
            data = dict(sorted(db_stats['department_stats'].items(), key=lambda x: x[1], reverse=True)[:10])
            default_chart_type = 'horizontal_bar'
        elif 'service_name' in group_by and db_stats['service_stats']:
            data = dict(sorted(db_stats['service_stats'].items(), key=lambda x: x[1], reverse=True)[:10])
            default_chart_type = 'horizontal_bar'
        elif 'incident_grade' in group_by and db_stats['grade_stats']:
            data = db_stats['grade_stats']
            default_chart_type = 'pie'
        elif 'cause_type' in group_by and db_stats['cause_type_stats']:
            data = dict(sorted(db_stats['cause_type_stats'].items(), key=lambda x: x[1], reverse=True)[:10])
            default_chart_type = 'horizontal_bar'
        else:
            data = db_stats['yearly_stats'] or db_stats['monthly_stats']
            default_chart_type = 'line'
        
        chart_type = requested_chart_type or default_chart_type
        
        if default_chart_type == 'line' and len(data) == 1:
            chart_type = 'bar'
        
        return data, chart_type
    
    def _generate_chart_title_from_db_stats(self, query, db_stats):
        """DB ÌÜµÍ≥Ñ Í∏∞Î∞ò Ï∞®Ìä∏ Ï†úÎ™© ÏÉùÏÑ±"""
        conditions = db_stats['query_conditions']
        group_by = conditions.get('group_by', [])
        
        title_parts = []
        
        if conditions.get('year'):
            title_parts.append(conditions['year'])
        
        if 'year' in group_by:
            title_parts.append("Ïó∞ÎèÑÎ≥Ñ")
        elif 'month' in group_by:
            title_parts.append("ÏõîÎ≥Ñ")
        elif 'daynight' in group_by:
            title_parts.append("ÏãúÍ∞ÑÎåÄÎ≥Ñ")
        elif 'week' in group_by:
            title_parts.append("ÏöîÏùºÎ≥Ñ")
        elif 'owner_depart' in group_by:
            title_parts.append("Î∂ÄÏÑúÎ≥Ñ")
        elif 'service_name' in group_by:
            title_parts.append("ÏÑúÎπÑÏä§Î≥Ñ")
        elif 'incident_grade' in group_by:
            title_parts.append("Îì±Í∏âÎ≥Ñ")
        elif 'cause_type' in group_by:
            title_parts.append("ÏõêÏù∏Ïú†ÌòïÎ≥Ñ")
        
        if db_stats['is_error_time_query']:
            title_parts.append("Ïû•Ïï†ÏãúÍ∞Ñ")
        else:
            title_parts.append("Ïû•Ïï† Î∞úÏÉù ÌòÑÌô©")
        
        return ' '.join(title_parts)

    def _display_response_with_marker_conversion(self, response, chart_info=None):
        if not response:
            st.write("ÏùëÎãµÏù¥ ÏóÜÏäµÎãàÎã§.")
            return
        
        response_text, chart_info = response if isinstance(response, tuple) else (response, chart_info)
        if chart_info and chart_info.get('chart'):
            response_text = self.remove_text_charts_from_response(response_text)
        
        converted_content, html_converted = response_text, False
        if '[REPAIR_BOX_START]' in converted_content:
            converted_content, has_html = self.ui_components.convert_repair_box_to_html(converted_content)
            html_converted = html_converted or has_html
        if '[CAUSE_BOX_START]' in converted_content:
            converted_content, has_html = self.ui_components.convert_cause_box_to_html(converted_content)
            html_converted = html_converted or has_html
        
        if html_converted:
            st.markdown(converted_content, unsafe_allow_html=True)
        else:
            st.write(converted_content)
        
        if chart_info and chart_info.get('chart'):
            st.markdown("---")
            try:
                self.chart_manager.display_chart_with_data(chart_info['chart'], chart_info['chart_data'], chart_info['chart_type'], chart_info.get('query', ''))
            except Exception as e:
                st.error(f"Ï∞®Ìä∏ ÌëúÏãú Ï§ë Ïò§Î•ò: {str(e)}")

    @traceable(name="process_user_query")
    def process_query(self, query, query_type=None):
        if not query:
            st.error("ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.")
            return
        
        # ÏÉàÎ°úÏö¥ ÏøºÎ¶¨ ÏãúÏûë Ïãú Î°úÍπÖ ÌîåÎûòÍ∑∏ Ï¥àÍ∏∞Ìôî (Ï§ëÎ≥µ Î°úÍπÖ Î∞©ÏßÄ)
        if not hasattr(st.session_state, 'current_query_logged'):
            st.session_state.current_query_logged = False
        st.session_state.current_query_logged = False
        
        start_time = time.time()
        response_text = None
        document_count = 0
        error_message = None
        success = False
        
        with st.chat_message("assistant"):
            try:
                # üî• 1Îã®Í≥Ñ: Í∞ïÏ†ú ÏπòÌôò Î®ºÏ†Ä Ï†ÅÏö© üî•
                original_query = query
                force_replaced_query = self.force_replace_problematic_queries(query)
                
                # Í∞ïÏ†ú ÏπòÌôòÏù¥ Î∞úÏÉùÌïú Í≤ΩÏö∞ ÏïåÎ¶º
                if force_replaced_query != original_query:
                    if self.debug_mode:
                        st.info(f"üîÑ ÏøºÎ¶¨ Í∞ïÏ†ú ÏπòÌôò: '{original_query}' ‚Üí '{force_replaced_query}'")
                    # ÏπòÌôòÎêú ÏøºÎ¶¨Î°ú Í≥ÑÏÜç ÏßÑÌñâ
                    query = force_replaced_query
                
                # 2Îã®Í≥Ñ: Î¶¨ÌîÑÎ°¨ÌîÑÌåÖ Ï≤¥ÌÅ¨ (Í∞ïÏ†ú ÏπòÌôò Í≤∞Í≥º Ìè¨Ìï®)
                reprompting_info = self.check_and_transform_query_with_reprompting(query)
                processing_query = reprompting_info.get('transformed_query', query)
                
                # 3Îã®Í≥Ñ: ÎÇòÎ®∏ÏßÄ Ï≤òÎ¶¨ Î°úÏßÅ (Í∏∞Ï°¥Í≥º ÎèôÏùº)
                time_conditions = self.extract_time_conditions(processing_query)
                department_conditions = self.extract_department_conditions(processing_query)
                
                if query_type is None:
                    with st.spinner("üîç ÏßàÎ¨∏ Î∂ÑÏÑù Ï§ë..."):
                        query_type = self.classify_query_type_with_llm(processing_query)
                
                target_service_name = self.search_manager.extract_service_name_from_query(processing_query)
                
                with st.spinner("üìÑ Î¨∏ÏÑú Í≤ÄÏÉâ Ï§ë..."):
                    documents = self.search_manager.semantic_search_with_adaptive_filtering(processing_query, target_service_name, query_type) or []
                    document_count = len(documents)
                    
                    if documents:
                        with st.expander("üìÑ Îß§Ïπ≠Îêú Î¨∏ÏÑú ÏÉÅÏÑ∏ Î≥¥Í∏∞"):
                            self.ui_components.display_documents_with_quality_info(documents)
                        
                        with st.spinner("ü§ñ AI ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë..."):
                            response = self.generate_rag_response_with_adaptive_processing(query, documents, query_type, time_conditions, department_conditions, reprompting_info)
                            
                            if response:
                                response_text = response[0] if isinstance(response, tuple) else response
                                
                                # ÎãµÎ≥Ä ÏÑ±Í≥µ Ïó¨Î∂Ä ÌåêÎã®
                                success = self._is_successful_response(response_text, document_count)
                                if not success:
                                    error_message = self._get_failure_reason(response_text, document_count)
                                
                                self._display_response_with_marker_conversion(response)
                                st.session_state.messages.append({"role": "assistant", "content": response_text})
                            else:
                                response_text = "Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÏùëÎãµÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§."
                                success = False
                                error_message = "ÏùëÎãµ ÏÉùÏÑ± Ïã§Ìå®"
                                st.write(response_text)
                                st.session_state.messages.append({"role": "assistant", "content": response_text})
                    else:
                        with st.spinner("üìÑ Ï∂îÍ∞Ä Í≤ÄÏÉâ Ï§ë..."):
                            fallback_documents = self.search_manager.search_documents_fallback(processing_query, target_service_name)
                            document_count = len(fallback_documents)
                            
                            if fallback_documents:
                                response = self.generate_rag_response_with_adaptive_processing(query, fallback_documents, query_type, time_conditions, department_conditions, reprompting_info)
                                response_text = response[0] if isinstance(response, tuple) else response
                                
                                success = self._is_successful_response(response_text, document_count)
                                if not success:
                                    error_message = self._get_failure_reason(response_text, document_count)
                                
                                self._display_response_with_marker_conversion(response)
                                st.session_state.messages.append({"role": "assistant", "content": response_text})
                            else:
                                response_text = f"'{target_service_name or 'Ìï¥Îãπ Ï°∞Í±¥'}'Ïóê Ìï¥ÎãπÌïòÎäî Î¨∏ÏÑúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
                                success = False
                                error_message = "Í¥ÄÎ†® Î¨∏ÏÑú Í≤ÄÏÉâ Ïã§Ìå®"
                                st.write(response_text)
                                st.session_state.messages.append({"role": "assistant", "content": response_text})
                                
            except Exception as e:
                response_time = time.time() - start_time
                error_message = str(e)[:50] + ("..." if len(str(e)) > 50 else "")
                success = False
                response_text = f"ÏøºÎ¶¨ Ï≤òÎ¶¨ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
                st.error(response_text)
                st.session_state.messages.append({"role": "assistant", "content": response_text})
                
                # ÏòàÏô∏ Î∞úÏÉù Ïãú Ï§ëÎ≥µ Î°úÍπÖ Î∞©ÏßÄÌïòÏó¨ Î°úÍπÖ
                if not st.session_state.current_query_logged and self.monitoring_enabled and self._manual_logging_enabled:
                    self._log_query_activity(
                        query=query,
                        query_type=query_type,
                        response_time=response_time,
                        document_count=document_count,
                        success=success,
                        error_message=error_message,
                        response_content=response_text
                    )
                    st.session_state.current_query_logged = True
                return
            
            # Ï†ïÏÉÅ Ï≤òÎ¶¨ ÏôÑÎ£å Ïãú Ï§ëÎ≥µ Î°úÍπÖ Î∞©ÏßÄÌïòÏó¨ Î°úÍπÖ
            response_time = time.time() - start_time
            if not st.session_state.current_query_logged and self.monitoring_enabled and self._manual_logging_enabled:
                self._log_query_activity(
                    query=query,
                    query_type=query_type,
                    response_time=response_time,
                    document_count=document_count,
                    success=success,
                    error_message=error_message,
                    response_content=response_text
                )
                st.session_state.current_query_logged = True

    def _is_successful_response(self, response_text: str, document_count: int) -> bool:
        """ÏùëÎãµÏù¥ ÏÑ±Í≥µÏ†ÅÏù∏ÏßÄ ÌåêÎã® (RAG Í∏∞Î∞ò ÎãµÎ≥Ä Ïó¨Î∂Ä Ìè¨Ìï®)"""
        if not response_text or response_text.strip() == "":
            return False
        
        # Ïã§Ìå® Ìå®ÌÑ¥ Í≤ÄÏÇ¨
        failure_patterns = [
            r"Ìï¥Îãπ.*Ï°∞Í±¥.*Î¨∏ÏÑú.*Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§",
            r"Í≤ÄÏÉâÎêú Î¨∏ÏÑúÍ∞Ä ÏóÜÏñ¥ÏÑú ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌï† Ïàò ÏóÜÏäµÎãàÎã§",
            r"Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§",
            r"Î¨∏ÏÑúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§",
            r"ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§",
            r"Ï£ÑÏÜ°Ìï©ÎãàÎã§.*Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§",
            r"Ï≤òÎ¶¨ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§",
            r"Ïó∞Í≤∞Ïóê Ïã§Ìå®ÌñàÏäµÎãàÎã§",
            r"ÏÑúÎπÑÏä§Î•º Ïù¥Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§",
            r"Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§"
        ]
        
        for pattern in failure_patterns:
            if re.search(pattern, response_text, re.IGNORECASE):
                return False
        
        # ÏùëÎãµÏù¥ ÎÑàÎ¨¥ ÏßßÏùÄ Í≤ΩÏö∞
        if len(response_text.strip()) < 10:
            return False
        
        # Î¨∏ÏÑú ÏàòÍ∞Ä 0Ïù∏ Í≤ΩÏö∞
        if document_count == 0:
            return False
        
        # RAG Í∏∞Î∞ò ÎãµÎ≥ÄÏù∏ÏßÄ ÌåêÎã® (ÌïµÏã¨ Ï∂îÍ∞Ä Í≤ÄÏ¶ù)
        if not self._is_rag_based_response(response_text, document_count):
            return False
        
        return True

    def _is_rag_based_response(self, response_text: str, document_count: int = None) -> bool:
        """RAG ÏõêÏ≤ú Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò ÎãµÎ≥ÄÏù∏ÏßÄ ÌåêÎã®"""
        
        if not response_text:
            return False
        
        response_lower = response_text.lower()
        
        # 1. Î¨∏ÏÑú ÏàòÍ∞Ä Îß§Ïö∞ Ï†ÅÏùÄ Í≤ΩÏö∞ (2Í∞ú ÎØ∏Îßå)
        if document_count is not None and document_count < 2:
            return False
        
        # 2. RAG Í∏∞Î∞ò ÎãµÎ≥ÄÏùò ÌäπÏßïÏ†Å ÎßàÏª§Îì§ ÌôïÏù∏
        rag_markers = [
            '[repair_box_start]', '[cause_box_start]', 
            'case1', 'case2', 'case3',
            'Ïû•Ïï† id', 'incident_id', 'service_name',
            'Î≥µÍµ¨Î∞©Î≤ï:', 'Ïû•Ïï†ÏõêÏù∏:', 'ÏÑúÎπÑÏä§Î™Ö:',
            'Î∞úÏÉùÏùºÏãú:', 'Ïû•Ïï†ÏãúÍ∞Ñ:', 'Îã¥ÎãπÎ∂ÄÏÑú:',
            'Ï∞∏Ï°∞Ïû•Ïï†Ï†ïÎ≥¥', 'Ïû•Ïï†Îì±Í∏â:', 'inm2'
        ]
        
        rag_marker_count = sum(1 for marker in rag_markers if marker in response_lower)
        
        # 3. RAG Í∏∞Î∞ò ÎãµÎ≥ÄÏóê ÏûêÏ£º ÎÇòÌÉÄÎÇòÎäî Ìå®ÌÑ¥Îì§
        rag_patterns = [
            r'Ïû•Ïï†\s*id\s*:\s*inm\d+',  # INMÏúºÎ°ú ÏãúÏûëÌïòÎäî Ïû•Ïï† ID
            r'ÏÑúÎπÑÏä§Î™Ö\s*:\s*\w+',       # ÏÑúÎπÑÏä§Î™Ö: Ìå®ÌÑ¥
            r'Î∞úÏÉùÏùº[ÏãúÏûê]\s*:\s*\d{4}', # Î∞úÏÉùÏùºÏãú: ÎÖÑÎèÑ Ìå®ÌÑ¥
            r'Ïû•Ïï†ÏãúÍ∞Ñ\s*:\s*\d+Î∂Ñ',     # Ïû•Ïï†ÏãúÍ∞Ñ: Ïà´ÏûêÎ∂Ñ Ìå®ÌÑ¥
            r'Î≥µÍµ¨Î∞©Î≤ï\s*:\s*',          # Î≥µÍµ¨Î∞©Î≤ï: Ìå®ÌÑ¥
            r'Ïû•Ïï†ÏõêÏù∏\s*:\s*',          # Ïû•Ïï†ÏõêÏù∏: Ìå®ÌÑ¥
            r'\d+Îì±Í∏â',                 # XÎì±Í∏â Ìå®ÌÑ¥
            r'incident_repair',         # incident_repair ÌïÑÎìú Ïñ∏Í∏â
            r'error_date',              # error_date ÌïÑÎìú Ïñ∏Í∏â
            r'case\d+\.',               # Case1. Case2. Ìå®ÌÑ¥
        ]
        
        rag_pattern_count = sum(1 for pattern in rag_patterns if re.search(pattern, response_lower))
        
        # 4. ÏùºÎ∞òÏ†ÅÏù∏ ÎãµÎ≥Ä Ìå®ÌÑ¥Îì§ (RAGÍ∞Ä ÏïÑÎãå ÏùºÎ∞ò ÏßÄÏãù Í∏∞Î∞ò)
        general_patterns = [
            r'ÏùºÎ∞òÏ†ÅÏúºÎ°ú\s+',
            r'Î≥¥ÌÜµ\s+',
            r'ÎåÄÎ∂ÄÎ∂Ñ\s+',
            r'ÌùîÌûà\s+',
            r'Ï£ºÎ°ú\s+',
            r'Îã§ÏùåÍ≥º\s+Í∞ôÏùÄ\s+Î∞©Î≤ï',
            r'Îã§Ïùå\s+Îã®Í≥Ñ',
            r'Í∏∞Î≥∏Ï†ÅÏù∏\s+',
            r'ÌëúÏ§ÄÏ†ÅÏù∏\s+',
            r'Í∂åÏû•ÏÇ¨Ìï≠',
            r'best\s+practice',
            r'Î™®Î≤î\s+ÏÇ¨Î°Ä',
            r'Îã§ÏùåÍ≥º\s+Í∞ôÏù¥\s+Ï†ëÍ∑º',
            r'ÏãúÏä§ÌÖú\s+Í¥ÄÎ¶¨Ïûê',
            r'ÎÑ§Ìä∏ÏõåÌÅ¨\s+Í¥ÄÎ¶¨',
            r'ÏÑúÎ≤Ñ\s+Í¥ÄÎ¶¨',
        ]
        
        general_pattern_count = sum(1 for pattern in general_patterns if re.search(pattern, response_lower))
        
        # 5. RAG ÎãµÎ≥ÄÏóêÏÑú ÌùîÌûà ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÎäî ÏùºÎ∞òÏ†Å ÌëúÌòÑÎì§
        non_rag_keywords = [
            'ÏùºÎ∞òÏ†ÅÏúºÎ°ú', 'Î≥¥ÌÜµ', 'ÎåÄÎ∂ÄÎ∂Ñ', 'ÌùîÌûà', 'Ï£ºÎ°ú',
            'Í∏∞Î≥∏Ï†ÅÏúºÎ°ú', 'ÌëúÏ§ÄÏ†ÅÏúºÎ°ú', 'Í∂åÏû•ÏÇ¨Ìï≠', 'Î™®Î≤îÏÇ¨Î°Ä',
            'Îã§ÏùåÍ≥º Í∞ôÏùÄ Î∞©Î≤ï', 'Îã§Ïùå Îã®Í≥Ñ', 'Í∏∞Î≥∏Ï†ÅÏù∏ Ï†êÍ≤Ä',
            'ÏãúÏä§ÌÖú Í¥ÄÎ¶¨', 'ÎÑ§Ìä∏ÏõåÌÅ¨ Í¥ÄÎ¶¨', 'ÏÑúÎ≤Ñ Í¥ÄÎ¶¨',
            'ÏùºÎ∞òÏ†ÅÏù∏ Ìï¥Í≤∞Ï±Ö', 'ÌëúÏ§Ä Ï†àÏ∞®', 'Í∏∞Î≥∏ ÏõêÏπô',
            'Îã§ÏùåÍ≥º Í∞ôÏùÄ Ï°∞Ïπò', 'Í∏∞Î≥∏Ï†ÅÏù∏ ÏàúÏÑú'
        ]
        
        non_rag_keyword_count = sum(1 for keyword in non_rag_keywords if keyword in response_lower)
        
        # 6. ÌäπÏ†ï ÏßàÎ¨∏ Ïú†ÌòïÎ≥Ñ RAG Í∏∞Î∞ò ÌåêÎã®
        statistics_indicators = ['Í±¥Ïàò', 'ÌÜµÍ≥Ñ', 'ÌòÑÌô©', 'Î∂ÑÌè¨', 'ÎÖÑÎèÑÎ≥Ñ', 'ÏõîÎ≥Ñ', 'Ï∞®Ìä∏']
        statistics_count = sum(1 for indicator in statistics_indicators if indicator in response_lower)
        
        # 7. ÌåêÎã® Î°úÏßÅ
        print(f"DEBUG RAG ÌåêÎã®: rag_markers={rag_marker_count}, rag_patterns={rag_pattern_count}, general_patterns={general_pattern_count}, non_rag_keywords={non_rag_keyword_count}")
        
        # RAG ÎßàÏª§ÎÇò Ìå®ÌÑ¥Ïù¥ Ï∂©Î∂ÑÌûà ÏûàÏúºÎ©¥ RAG Í∏∞Î∞òÏúºÎ°ú ÌåêÎã®
        if rag_marker_count >= 3 or rag_pattern_count >= 2:
            return True
        
        # ÌÜµÍ≥Ñ Í¥ÄÎ†® ÎãµÎ≥ÄÏù¥Í≥† ÌÜµÍ≥Ñ ÏßÄÌëúÍ∞Ä ÏûàÏúºÎ©¥ RAG Í∏∞Î∞òÏúºÎ°ú ÌåêÎã®
        if statistics_count >= 2 and any(word in response_lower for word in ['Ï∞®Ìä∏', 'Ìëú', 'Ïù¥', 'Ìï©Í≥Ñ']):
            return True
        
        # ÏùºÎ∞òÏ†Å ÌëúÌòÑÏù¥ ÎßéÍ≥† RAG ÌäπÏßïÏù¥ Ï†ÅÏúºÎ©¥ ÏùºÎ∞ò ÎãµÎ≥ÄÏúºÎ°ú ÌåêÎã®
        if general_pattern_count >= 2 or non_rag_keyword_count >= 3:
            if rag_marker_count == 0 and rag_pattern_count == 0:
                print(f"DEBUG: ÏùºÎ∞òÏ†Å ÎãµÎ≥ÄÏúºÎ°ú ÌåêÎã®Îê® (general_pattern_count={general_pattern_count}, non_rag_keyword_count={non_rag_keyword_count})")
                return False
        
        # RAG ÎßàÏª§Í∞Ä ÌïòÎÇòÎùºÎèÑ ÏûàÏúºÎ©¥ RAG Í∏∞Î∞òÏúºÎ°ú ÌåêÎã®
        if rag_marker_count > 0 or rag_pattern_count > 0:
            return True
        
        # ÏùëÎãµÏù¥ Í∏∏Í≥† Íµ¨Ï≤¥Ï†ÅÏù¥Î©¥ÏÑú Î¨∏ÏÑúÍ∞Ä Ï∂©Î∂ÑÌûà ÏûàÏúºÎ©¥ RAG Í∏∞Î∞òÏúºÎ°ú ÌåêÎã®
        if len(response_text) > 200 and document_count and document_count >= 3:
            # ÌïòÏßÄÎßå ÏùºÎ∞òÏ†Å ÌëúÌòÑÏù¥ ÎÑàÎ¨¥ ÎßéÏúºÎ©¥ Ï†úÏô∏
            if non_rag_keyword_count < 2:
                return True
        
        # Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏùºÎ∞ò ÎãµÎ≥ÄÏúºÎ°ú ÌåêÎã®
        print(f"DEBUG: Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏùºÎ∞ò ÎãµÎ≥ÄÏúºÎ°ú ÌåêÎã®Îê®")
        return False

    def _get_failure_reason(self, response_text: str, document_count: int) -> str:
        """Ïã§Ìå® ÏõêÏù∏ Î∂ÑÏÑù (RAG Í∏∞Î∞ò Ïó¨Î∂Ä Ìè¨Ìï®)"""
        if not response_text or response_text.strip() == "":
            return "ÏùëÎãµ ÎÇ¥Ïö© ÏóÜÏùå"
        
        if document_count == 0:
            return "Í¥ÄÎ†® Î¨∏ÏÑú Í≤ÄÏÉâ Ïã§Ìå®"
        
        if len(response_text.strip()) < 10:
            return "ÏùëÎãµ Í∏∏Ïù¥ Î∂ÄÏ°±"
        
        # ÌäπÏ†ï Ïã§Ìå® Ìå®ÌÑ¥ Îß§Ïπ≠
        if re.search(r"Ìï¥Îãπ.*Ï°∞Í±¥.*Î¨∏ÏÑú.*Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§", response_text, re.IGNORECASE):
            return "Ï°∞Í±¥ ÎßûÎäî Î¨∏ÏÑú ÏóÜÏùå"
        
        if re.search(r"Í≤ÄÏÉâÎêú Î¨∏ÏÑúÍ∞Ä ÏóÜÏñ¥ÏÑú", response_text, re.IGNORECASE):
            return "Í≤ÄÏÉâ Í≤∞Í≥º ÏóÜÏùå"
        
        if re.search(r"Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§", response_text, re.IGNORECASE):
            return "ÏãúÏä§ÌÖú Ïò§Î•ò Î∞úÏÉù"
        
        if re.search(r"ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§", response_text, re.IGNORECASE):
            return "ÎãµÎ≥Ä ÏÉùÏÑ± Ïã§Ìå®"
        
        # RAG Í∏∞Î∞òÏù¥ ÏïÑÎãå ÏùºÎ∞ò ÎãµÎ≥ÄÏù∏ Í≤ΩÏö∞ (ÌïµÏã¨ Ï∂îÍ∞Ä Í≤ÄÏ¶ù)
        if not self._is_rag_based_response(response_text, document_count):
            return "RAG Í∏∞Î∞ò ÎãµÎ≥Ä ÏïÑÎãò"
        
        return "Ï†ÅÏ†àÌïú ÎãµÎ≥Ä ÏÉùÏÑ± Ïã§Ìå®"
    
    def _log_query_activity(self, query: str, query_type: str = None, response_time: float = None,
                        document_count: int = None, success: bool = None, 
                        error_message: str = None, response_content: str = None):
        """ÏøºÎ¶¨ ÌôúÎèô Î°úÍπÖ - Ï§ëÎ≥µ Î∞©ÏßÄ Í∏∞Îä• Ï∂îÍ∞Ä"""
        try:
            # Ï§ëÎ≥µ Î°úÍπÖ Î∞©ÏßÄ: Îç∞ÏΩîÎ†àÏù¥ÌÑ∞ Î°úÍπÖÏù¥ ÌôúÏÑ±ÌôîÎêú Í≤ΩÏö∞ ÏàòÎèô Î°úÍπÖ Ïä§ÌÇµ
            if hasattr(self, '_decorator_logging_enabled') and self._decorator_logging_enabled:
                print(f"DEBUG: Îç∞ÏΩîÎ†àÏù¥ÌÑ∞ Î°úÍπÖÏù¥ ÌôúÏÑ±ÌôîÎêòÏñ¥ ÏàòÎèô Î°úÍπÖÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.")
                return
                
            # ÏàòÎèô Î°úÍπÖÏù¥ ÎπÑÌôúÏÑ±ÌôîÎêú Í≤ΩÏö∞ Ïä§ÌÇµ
            if hasattr(self, '_manual_logging_enabled') and not self._manual_logging_enabled:
                print(f"DEBUG: ÏàòÎèô Î°úÍπÖÏù¥ ÎπÑÌôúÏÑ±ÌôîÎêòÏñ¥ Î°úÍπÖÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.")
                return
            
            # ÏÑ∏ÏÖò Í∏∞Î∞ò Ï§ëÎ≥µ Î°úÍπÖ Î∞©ÏßÄ
            if hasattr(st.session_state, 'current_query_logged') and st.session_state.current_query_logged:
                print(f"DEBUG: ÌòÑÏû¨ ÏøºÎ¶¨Í∞Ä Ïù¥ÎØ∏ Î°úÍπÖÎêòÏñ¥ Ï§ëÎ≥µ Î°úÍπÖÏùÑ Î∞©ÏßÄÌï©ÎãàÎã§.")
                return
                
            if self.monitoring_manager:
                # IP Ï£ºÏÜå Í∞ÄÏ†∏Ïò§Í∏∞ (ÏÑ∏ÏÖòÏóêÏÑú ÏÑ§Ï†ïÎêú Í≤ΩÏö∞ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ Í∏∞Î≥∏Í∞í)
                ip_address = getattr(st.session_state, 'client_ip', '127.0.0.1')
                
                self.monitoring_manager.log_user_activity(
                    ip_address=ip_address,
                    question=query,
                    query_type=query_type,
                    user_agent="Streamlit/ChatBot",
                    response_time=response_time,
                    document_count=document_count,
                    success=success,
                    error_message=error_message,
                    response_content=response_content
                )
                
                # Î°úÍπÖ ÏôÑÎ£å ÌëúÏãú
                if hasattr(st.session_state, 'current_query_logged'):
                    st.session_state.current_query_logged = True
                    
                print(f"DEBUG: ÏøºÎ¶¨ Î°úÍπÖ ÏôÑÎ£å - Query: {query[:50]}..., Success: {success}")
                
        except Exception as e:
            print(f"Î™®ÎãàÌÑ∞ÎßÅ Î°úÍ∑∏ Ïã§Ìå®: {str(e)}")  # Î°úÍπÖ Ïã§Ìå®Ìï¥ÎèÑ Î©îÏù∏ Í∏∞Îä•ÏóêÎäî ÏòÅÌñ• ÏóÜÏùå

    def force_replace_problematic_queries(self, query):
        """Î¨∏Ï†úÍ∞Ä ÎêòÎäî ÌëúÌòÑÎì§ÏùÑ Ï†ïÏÉÅ ÏûëÎèôÌïòÎäî ÌëúÌòÑÏúºÎ°ú Í∞ïÏ†ú ÏπòÌôò - Î≥µÍµ¨Î∞©Î≤ï ÏßàÎ¨∏ Î≥¥Ìò∏"""
        if not query:
            return query
        
        original_query = query
        query_lower = query.lower()
        
        # 1Îã®Í≥Ñ: Î≥µÍµ¨Î∞©Î≤ï/Î¨∏Ï†úÌï¥Í≤∞ Í¥ÄÎ†® ÏßàÎ¨∏ÏùÄ ÏπòÌôòÌïòÏßÄ ÏïäÏùå (Î≥¥Ìò∏)
        protection_keywords = [
            'Î≥µÍµ¨Î∞©Î≤ï', 'Ìï¥Í≤∞Î∞©Î≤ï', 'Ï°∞ÏπòÎ∞©Î≤ï', 'ÎåÄÏùëÎ∞©Î≤ï', 'Î≥µÍµ¨Ï†àÏ∞®',
            'Î≥µÍµ¨', 'Ìï¥Í≤∞', 'Ï°∞Ïπò', 'ÎåÄÏùë', 'ÏàòÏ†ï', 'Í∞úÏÑ†',
            'Î∂àÍ∞Ä', 'Ïã§Ìå®', 'ÏïàÎê®', 'ÏïàÎêò', 'ÎêòÏßÄÏïä', 'Ïò§Î•ò', 'ÏóêÎü¨', 
            'Î¨∏Ï†ú', 'Ïû•Ïï†', 'Ïù¥Ïäà', 'Î≤ÑÍ∑∏', 'ÏõêÏù∏', 'Ïù¥Ïú†', 'Ïôú',
            'Ïú†ÏÇ¨', 'ÎπÑÏä∑Ìïú', 'Í∞ôÏùÄ', 'ÎèôÏùºÌïú'
        ]
        
        is_protected = any(keyword in query_lower for keyword in protection_keywords)
        if is_protected:
            if self.debug_mode:
                print(f"üîí PROTECTED QUERY: Î≥µÍµ¨/Î¨∏Ï†úÌï¥Í≤∞ Í¥ÄÎ†® ÏßàÎ¨∏ÏúºÎ°ú ÌåêÎã®ÎêòÏñ¥ ÏπòÌôòÌïòÏßÄ ÏïäÏùå")
            return query
        
        # 2Îã®Í≥Ñ: ÏàúÏàò ÌÜµÍ≥Ñ ÏßàÎ¨∏Îßå ÏπòÌôò Ï†ÅÏö©
        pure_statistics_indicators = [
            'Í±¥Ïàò', 'ÌÜµÍ≥Ñ', 'ÌòÑÌô©', 'Î∂ÑÌè¨', 'Î™áÍ±¥', 'Í∞úÏàò', 
            'Ïó∞ÎèÑÎ≥Ñ', 'ÏõîÎ≥Ñ', 'Îì±Í∏âÎ≥Ñ', 'ÏöîÏùºÎ≥Ñ', 'ÏãúÍ∞ÑÎåÄÎ≥Ñ',
            'ÏïåÎ†§Ï§ò', 'Î≥¥Ïó¨Ï§ò', 'ÎßêÌï¥Ï§ò', 'ÌôïÏù∏Ìï¥Ï§ò'
        ]
        
        has_statistics_intent = any(indicator in query_lower for indicator in pure_statistics_indicators)
        if not has_statistics_intent:
            if self.debug_mode:
                print(f"üìã NON-STATISTICS QUERY: ÌÜµÍ≥Ñ ÏùòÎèÑÍ∞Ä ÏóÜÏñ¥ ÏπòÌôòÌïòÏßÄ ÏïäÏùå")
            return query
        
        # 3Îã®Í≥Ñ: Í∏∞Ï°¥ Í∞ïÏ†ú ÏπòÌôò Í∑úÏπô Ï†ÅÏö© (ÌÜµÍ≥Ñ ÏßàÎ¨∏Îßå)
        # ... (Í∏∞Ï°¥ replacement_rules ÏΩîÎìú ÎèôÏùº) ...
        
        # ÎÇòÎ®∏ÏßÄ Í∏∞Ï°¥ ÏΩîÎìúÏôÄ ÎèôÏùº
        # (ÏÑúÎπÑÏä§Î™Ö, Ïó∞ÎèÑ, Ïõî, ÏãúÍ∞ÑÎåÄ, ÏöîÏùº Ï†ïÎ≥¥ Î≥¥Ï°¥ Î°úÏßÅ)
        
        return modified_query