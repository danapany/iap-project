import streamlit as st
import re
from config.prompts import SystemPrompts
from config.settings_local import AppConfigLocal
from utils.search_utils_local import SearchManagerLocal
from utils.ui_components_local import UIComponentsLocal

class QueryProcessorLocal:
    """ì¿¼ë¦¬ ì²˜ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤ - ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì í™”ëœ ì ì‘í˜• ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, azure_openai_client, search_client, model_name, config=None):
        self.azure_openai_client = azure_openai_client
        self.search_client = search_client
        self.model_name = model_name
        self.config = config if config else AppConfigLocal()
        self.search_manager = SearchManagerLocal(search_client, self.config)
        self.ui_components = UIComponentsLocal()
    
    def classify_query_type_with_llm(self, query):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ íƒ€ì…ì„ ìë™ìœ¼ë¡œ ë¶„ë¥˜"""
        try:
            classification_prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.

**ë¶„ë¥˜ ê¸°ì¤€:**
1. **repair**: ì„œë¹„ìŠ¤ëª…ê³¼ ì¥ì• í˜„ìƒì´ ëª¨ë‘ í¬í•¨ëœ ë³µêµ¬ë°©ë²• ë¬¸ì˜
   - ì˜ˆ: "ERP ì ‘ì†ë¶ˆê°€ ë³µêµ¬ë°©ë²•", "API_Link ì‘ë‹µì§€ì—° í•´ê²°ë°©ë²•"
   
2. **cause**: ì¥ì• ì›ì¸ ë¶„ì„ì´ë‚˜ ì›ì¸ íŒŒì•…ì„ ìš”ì²­í•˜ëŠ” ë¬¸ì˜
   - ì˜ˆ: "ERP ì ‘ì†ë¶ˆê°€ ì›ì¸ì´ ë­ì•¼?", "API ì‘ë‹µì§€ì—° ì¥ì• ì›ì¸", "ì™œ ì¥ì• ê°€ ë°œìƒí–ˆì–´?"
   
3. **similar**: ì„œë¹„ìŠ¤ëª… ì—†ì´ ì¥ì• í˜„ìƒë§Œìœ¼ë¡œ ìœ ì‚¬ì‚¬ë¡€ ë¬¸ì˜
   - ì˜ˆ: "ì ‘ì†ë¶ˆê°€ í˜„ìƒ ìœ ì‚¬ì‚¬ë¡€", "ì‘ë‹µì§€ì—° ë™ì¼í˜„ìƒ ë³µêµ¬ë°©ë²•"
   
4. **default**: ê·¸ ì™¸ì˜ ëª¨ë“  ê²½ìš° (í†µê³„, ê±´ìˆ˜, ì¼ë°˜ ë¬¸ì˜ ë“±)
   - ì˜ˆ: "ë…„ë„ë³„ ê±´ìˆ˜", "ì¥ì•  í†µê³„", "ì„œë¹„ìŠ¤ í˜„í™©"

**ì‚¬ìš©ì ì§ˆë¬¸:** {query}

**ì‘ë‹µ í˜•ì‹:** repair, cause, similar, default ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

            response = self.azure_openai_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ IT ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ì •í™•íˆ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": classification_prompt}
                ],
                temperature=0.1,
                max_tokens=50
            )
            
            query_type = response.choices[0].message.content.strip().lower()
            
            if query_type not in ['repair', 'cause', 'similar', 'default']:
                query_type = 'default'
                
            return query_type
            
        except Exception as e:
            st.warning(f"ì¿¼ë¦¬ ë¶„ë¥˜ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {str(e)}")
            return 'default'

    def validate_document_relevance_with_llm(self, query, documents):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„±ì„ ì¬ê²€ì¦ - repair/cause ì „ìš©"""
        try:
            if not documents:
                return []
            
            validation_prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ë‹¤ìŒ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì‹¤ì œë¡œ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë§Œ ì„ ë³„í•´ì£¼ì„¸ìš”.
ê° ë¬¸ì„œì— ëŒ€í•´ 0-100ì  ì‚¬ì´ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , 70ì  ì´ìƒì¸ ë¬¸ì„œë§Œ ì„ íƒí•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
1. ì„œë¹„ìŠ¤ëª… ì¼ì¹˜ë„ (ì‚¬ìš©ìê°€ íŠ¹ì • ì„œë¹„ìŠ¤ë¥¼ ì–¸ê¸‰í•œ ê²½ìš°)
2. ì¥ì• í˜„ìƒ/ì¦ìƒ ì¼ì¹˜ë„  
3. ì‚¬ìš©ìê°€ ìš”êµ¬í•œ ì •ë³´ ìœ í˜•ê³¼ì˜ ì¼ì¹˜ë„
4. ì „ì²´ì ì¸ ë§¥ë½ ì¼ì¹˜ë„

"""

            for i, doc in enumerate(documents):
                doc_info = f"""
ë¬¸ì„œ {i+1}:
- ì„œë¹„ìŠ¤ëª…: {doc.get('service_name', '')}
- ì¥ì• í˜„ìƒ: {doc.get('symptom', '')}
- ì˜í–¥ë„: {doc.get('effect', '')}
- ì¥ì• ì›ì¸: {doc.get('root_cause', '')[:100]}...
- ë³µêµ¬ë°©ë²•: {doc.get('incident_repair', '')[:100]}...
"""
                validation_prompt += doc_info

            validation_prompt += """

ì‘ë‹µ í˜•ì‹ (JSON):
{
    "validated_documents": [
        {
            "document_index": 1,
            "relevance_score": 85,
            "reason": "ì„œë¹„ìŠ¤ëª…ê³¼ ì¥ì• í˜„ìƒì´ ì •í™•íˆ ì¼ì¹˜í•¨"
        },
        {
            "document_index": 3,
            "relevance_score": 72,
            "reason": "ì¥ì• í˜„ìƒì€ ìœ ì‚¬í•˜ì§€ë§Œ ì„œë¹„ìŠ¤ëª…ì´ ë‹¤ë¦„"
        }
    ]
}

70ì  ì´ìƒì¸ ë¬¸ì„œë§Œ í¬í•¨í•˜ì„¸ìš”.
"""

            response = self.azure_openai_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ ì •í™•í•˜ê²Œ í‰ê°€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": validation_prompt}
                ],
                temperature=0.1,
                max_tokens=800
            )
            
            response_content = response.choices[0].message.content.strip()
            
            try:
                import json
                json_start = response_content.find('{')
                json_end = response_content.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_content = response_content[json_start:json_end]
                    validation_result = json.loads(json_content)
                    
                    validated_docs = []
                    for validated_doc in validation_result.get('validated_documents', []):
                        doc_index = validated_doc.get('document_index', 1) - 1
                        if 0 <= doc_index < len(documents):
                            original_doc = documents[doc_index].copy()
                            original_doc['relevance_score'] = validated_doc.get('relevance_score', 0)
                            original_doc['validation_reason'] = validated_doc.get('reason', 'ê²€ì¦ë¨')
                            validated_docs.append(original_doc)
                    
                    validated_docs.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
                    return validated_docs
                    
            except (json.JSONDecodeError, KeyError) as e:
                st.warning(f"ë¬¸ì„œ ê²€ì¦ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                return documents[:5]
                
        except Exception as e:
            st.warning(f"ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return documents[:5]
        
        return documents

    def generate_rag_response_with_adaptive_processing(self, query, documents, query_type="default"):
        """ì¿¼ë¦¬ íƒ€ì…ë³„ ì ì‘í˜• RAG ì‘ë‹µ ìƒì„±"""
        try:
            # ì¿¼ë¦¬ íƒ€ì…ë³„ ì²˜ë¦¬ ë°©ì‹ ê²°ì •
            use_llm_validation = query_type in ['repair', 'cause']
            
            if use_llm_validation:
                # repair/cause: ì •í™•ì„± ìš°ì„  ì²˜ë¦¬
                st.info("ğŸ¯ ì •í™•ì„± ìš°ì„  ì²˜ë¦¬ - ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„± ì¬ê²€ì¦ ì¤‘...")
                validated_documents = self.validate_document_relevance_with_llm(query, documents)
                
                if len(validated_documents) < len(documents):
                    removed_count = len(documents) - len(validated_documents)
                    st.success(f"âœ… ê´€ë ¨ì„± ê²€ì¦ ì™„ë£Œ: {len(validated_documents)}ê°œ ë¬¸ì„œ ì„ ë³„ (ê´€ë ¨ì„± ë‚®ì€ {removed_count}ê°œ ë¬¸ì„œ ì œì™¸)")
                else:
                    st.success(f"âœ… ê´€ë ¨ì„± ê²€ì¦ ì™„ë£Œ: ëª¨ë“  {len(validated_documents)}ê°œ ë¬¸ì„œê°€ ê´€ë ¨ì„± ê¸°ì¤€ í†µê³¼")
                
                if not validated_documents:
                    return "ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ì•„ ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë‚˜ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
                
                processing_documents = validated_documents
                processing_info = "ê´€ë ¨ì„± ê²€ì¦ ì™„ë£Œ (70ì  ì´ìƒ)"
                
            else:
                # similar/default: í¬ê´„ì„± ìš°ì„  ì²˜ë¦¬
                st.info("ğŸ“‹ í¬ê´„ì„± ìš°ì„  ì²˜ë¦¬ - ê´‘ë²”ìœ„í•œ ê²€ìƒ‰ ê²°ê³¼ í™œìš© ì¤‘...")
                processing_documents = documents
                processing_info = "í¬ê´„ì  ê²€ìƒ‰ ê²°ê³¼ í™œìš©"
                st.success(f"âœ… í¬ê´„ì  ì²˜ë¦¬ ì™„ë£Œ: {len(processing_documents)}ê°œ ë¬¸ì„œ í™œìš©")

            # ì§‘ê³„ ì •ë³´ ê³„ì‚°
            total_count = len(processing_documents)
            yearly_stats = {}
            
            for doc in processing_documents:
                error_date = doc.get('error_date', '')
                year_from_date = None
                if error_date and len(error_date) >= 4:
                    try:
                        year_from_date = int(error_date[:4])
                    except:
                        pass
                
                year_from_field = doc.get('year', '')
                if year_from_field:
                    try:
                        year_from_field = int(year_from_field)
                    except:
                        year_from_field = None
                
                final_year = year_from_date or year_from_field
                if final_year:
                    yearly_stats[final_year] = yearly_stats.get(final_year, 0) + 1
            
            yearly_total = sum(yearly_stats.values())
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            
            stats_info = f"""
=== ì •í™•í•œ ì§‘ê³„ ì •ë³´ ({processing_info}) ===
ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_count}ê±´
ë…„ë„ë³„ ë¶„í¬: {dict(sorted(yearly_stats.items()))}
ë…„ë„ë³„ í•©ê³„: {yearly_total}ê±´
ì§‘ê³„ ê²€ì¦: {'ì¼ì¹˜' if yearly_total == total_count else 'ë¶ˆì¼ì¹˜ - ì¬ê³„ì‚° í•„ìš”'}
ì²˜ë¦¬ ë°©ì‹: {'ì •í™•ì„± ìš°ì„  (LLM ê²€ì¦)' if use_llm_validation else 'í¬ê´„ì„± ìš°ì„  (ê´‘ë²”ìœ„ ê²€ìƒ‰)'}
===========================
"""
            context_parts.append(stats_info)
            
            for i, doc in enumerate(processing_documents):
                final_score = doc.get('final_score', 0)
                quality_tier = doc.get('quality_tier', 'Standard')
                filter_reason = doc.get('filter_reason', 'ê¸°ë³¸ ì„ ë³„')
                service_match_type = doc.get('service_match_type', 'unknown')
                relevance_score = doc.get('relevance_score', 0) if use_llm_validation else "N/A"
                validation_reason = doc.get('validation_reason', 'ê²€ì¦ë¨') if use_llm_validation else "í¬ê´„ì  ì²˜ë¦¬"
                
                validation_info = f" - ê´€ë ¨ì„±: {relevance_score}ì  ({validation_reason})" if use_llm_validation else " - í¬ê´„ì  ê²€ìƒ‰"
                
                context_part = f"""ë¬¸ì„œ {i+1} [{quality_tier}ê¸‰ - {filter_reason} - {service_match_type} ë§¤ì¹­{validation_info}]:
ì¥ì•  ID: {doc['incident_id']}
ì„œë¹„ìŠ¤ëª…: {doc['service_name']}
ì¥ì• ì‹œê°„: {doc['error_time']}
ì˜í–¥ë„: {doc['effect']}
í˜„ìƒ: {doc['symptom']}
ë³µêµ¬ê³µì§€: {doc['repair_notice']}
ë°œìƒì¼ì: {doc['error_date']}
ìš”ì¼: {doc['week']}
ì‹œê°„ëŒ€: {doc['daynight']}
ì¥ì• ì›ì¸: {doc['root_cause']}
ë³µêµ¬ë°©ë²•: {doc['incident_repair']}
ê°œì„ ê³„íš: {doc['incident_plan']}
ì›ì¸ìœ í˜•: {doc['cause_type']}
ì²˜ë¦¬ìœ í˜•: {doc['done_type']}
ì¥ì• ë“±ê¸‰: {doc['incident_grade']}
ë‹´ë‹¹ë¶€ì„œ: {doc['owner_depart']}
ë…„ë„: {doc['year']}
ì›”: {doc['month']}
í’ˆì§ˆì ìˆ˜: {final_score:.2f}
"""
                if use_llm_validation:
                    context_part += f"ê´€ë ¨ì„±ì ìˆ˜: {relevance_score}ì \n"
                
                context_parts.append(context_part)
            
            context = "\n\n".join(context_parts)
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            system_prompt = SystemPrompts.get_prompt(query_type)

            user_prompt = f"""
ë‹¤ìŒ ì¥ì•  ì´ë ¥ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
(ì²˜ë¦¬ ë°©ì‹: {'ì •í™•ì„± ìš°ì„  - LLM ê´€ë ¨ì„± ê²€ì¦ ì ìš©' if use_llm_validation else 'í¬ê´„ì„± ìš°ì„  - ê´‘ë²”ìœ„í•œ ê²€ìƒ‰ ê²°ê³¼ í™œìš©'}):

ì¤‘ìš”! ì§‘ê³„ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° ìœ„ì˜ "ì •í™•í•œ ì§‘ê³„ ì •ë³´" ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì—¬ ì •í™•í•œ ìˆ«ìë¥¼ ì œê³µí•˜ì„¸ìš”.
- ì „ì²´ ê±´ìˆ˜: {total_count}ê±´
- ë…„ë„ë³„ ê±´ìˆ˜: {dict(sorted(yearly_stats.items()))}
- ë°˜ë“œì‹œ ë…„ë„ë³„ í•©ê³„ê°€ ì „ì²´ ê±´ìˆ˜ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""

            # Azure OpenAI API í˜¸ì¶œ
            response = self.azure_openai_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1 if use_llm_validation else 0.2,  # ì •í™•ì„± vs ì°½ì˜ì„± ì¡°ì ˆ
                max_tokens=1500
            )
            
            return response.choices[0].message.content
        
        except Exception as e:
            st.error(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def process_query(self, query, query_type=None):
        """ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì í™”ëœ ì²˜ë¦¬ ë¡œì§ì„ ì ìš©í•œ ë©”ì¸ ì¿¼ë¦¬ ì²˜ë¦¬"""
        with st.chat_message("assistant"):
            # 1ë‹¨ê³„: LLM ê¸°ë°˜ ì¿¼ë¦¬ íƒ€ì… ìë™ ë¶„ë¥˜
            if query_type is None:
                with st.spinner("ğŸ” ì§ˆë¬¸ ìœ í˜• ë¶„ì„ ì¤‘..."):
                    query_type = self.classify_query_type_with_llm(query)
                    
                # ì²˜ë¦¬ ë°©ì‹ ì•ˆë‚´
                if query_type in ['repair', 'cause']:
                    st.info(f"ğŸ“ ì§ˆë¬¸ ìœ í˜•: **{query_type.upper()}** (ğŸ¯ ì •í™•ì„± ìš°ì„  ì²˜ë¦¬ - LLM ê²€ì¦ ì ìš©)")
                else:
                    st.info(f"ğŸ“ ì§ˆë¬¸ ìœ í˜•: **{query_type.upper()}** (ğŸ“‹ í¬ê´„ì„± ìš°ì„  ì²˜ë¦¬ - ê´‘ë²”ìœ„í•œ ê²€ìƒ‰)")
            
            # 2ë‹¨ê³„: ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ
            target_service_name = self.search_manager.extract_service_name_from_query(query)
            if target_service_name:
                st.info(f"ğŸ·ï¸ ì¶”ì¶œëœ ì„œë¹„ìŠ¤ëª…: **{target_service_name}**")
            
            # 3ë‹¨ê³„: ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì í™”ëœ ê²€ìƒ‰ ìˆ˜í–‰
            with st.spinner("ğŸ”„ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
                documents = self.search_manager.semantic_search_with_adaptive_filtering(
                    query, target_service_name, query_type
                )
                
                if documents:
                    # ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ë¶„ì„
                    premium_count = sum(1 for doc in documents if doc.get('quality_tier') == 'Premium')
                    standard_count = sum(1 for doc in documents if doc.get('quality_tier') == 'Standard')
                    basic_count = sum(1 for doc in documents if doc.get('quality_tier') == 'Basic')
                    
                    # ì§‘ê³„ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
                    is_count_query = any(keyword in query.lower() for keyword in ['ê±´ìˆ˜', 'ê°œìˆ˜', 'ëª‡ê±´', 'ë…„ë„ë³„', 'ì›”ë³„', 'í†µê³„', 'í˜„í™©'])
                    
                    # ì§‘ê³„ ë¯¸ë¦¬ë³´ê¸° (ì§‘ê³„ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°)
                    if is_count_query:
                        yearly_stats = {}
                        for doc in documents:
                            error_date = doc.get('error_date', '')
                            year_from_date = None
                            if error_date and len(error_date) >= 4:
                                try:
                                    year_from_date = int(error_date[:4])
                                except:
                                    pass
                            
                            year_from_field = doc.get('year', '')
                            if year_from_field:
                                try:
                                    year_from_field = int(year_from_field)
                                except:
                                    year_from_field = None
                            
                            final_year = year_from_date or year_from_field
                            if final_year:
                                yearly_stats[final_year] = yearly_stats.get(final_year, 0) + 1
                        
                        yearly_total = sum(yearly_stats.values())
                        processing_method = "ì •í™•ì„± ìš°ì„ " if query_type in ['repair', 'cause'] else "í¬ê´„ì„± ìš°ì„ "
                        st.info(f"""
                        ğŸ“Š ì§‘ê³„ ë¯¸ë¦¬ë³´ê¸° ({processing_method} ì²˜ë¦¬ ì˜ˆì •)
                        - ì „ì²´ ê±´ìˆ˜: {len(documents)}ê±´
                        - ë…„ë„ë³„ ë¶„í¬: {dict(sorted(yearly_stats.items()))}
                        - ë…„ë„ë³„ í•©ê³„: {yearly_total}ê±´
                        - ê²€ì¦ ìƒíƒœ: {'ì¼ì¹˜' if yearly_total == len(documents) else 'ë¶ˆì¼ì¹˜'}
                        """)
                    
                    st.success(f"âœ… {len(documents)}ê°œì˜ ë§¤ì¹­ ë¬¸ì„œ ì„ ë³„ ì™„ë£Œ! (ğŸ† Premium: {premium_count}ê°œ, ğŸ¯ Standard: {standard_count}ê°œ, ğŸ“‹ Basic: {basic_count}ê°œ)")
                    
                    # ê²€ìƒ‰ëœ ë¬¸ì„œ ìƒì„¸ í‘œì‹œ
                    with st.expander("ğŸ“„ ë§¤ì¹­ëœ ë¬¸ì„œ ìƒì„¸ ë³´ê¸°"):
                        self.ui_components.display_documents_with_quality_info(documents)
                    
                    # 4ë‹¨ê³„: ì ì‘í˜• RAG ì‘ë‹µ ìƒì„±
                    with st.spinner("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘..."):
                        response = self.generate_rag_response_with_adaptive_processing(
                            query, documents, query_type
                        )
                        
                        processing_type = "ğŸ¯ ì •í™•ì„± ìš°ì„  ì²˜ë¦¬" if query_type in ['repair', 'cause'] else "ğŸ“‹ í¬ê´„ì„± ìš°ì„  ì²˜ë¦¬"
                        with st.expander(f"ğŸ¤– AI ë‹µë³€ ({processing_type})", expanded=True):
                            st.write(response)
                        
                        st.session_state.messages.append({"role": "assistant", "content": response})
                        
                else:
                    # 5ë‹¨ê³„: ëŒ€ì²´ ê²€ìƒ‰ ì‹œë„
                    st.warning("âš ï¸ í¬í•¨ ë§¤ì¹­ìœ¼ë¡œë„ ê²°ê³¼ê°€ ì—†ì–´ ë” ê´€ëŒ€í•œ ê¸°ì¤€ìœ¼ë¡œ ì¬ê²€ìƒ‰ ì¤‘...")
                    
                    fallback_documents = self.search_manager.search_documents_fallback(query, target_service_name)
                    
                    if fallback_documents:
                        st.info(f"ğŸ”„ ëŒ€ì²´ ê²€ìƒ‰ìœ¼ë¡œ {len(fallback_documents)}ê°œ ë¬¸ì„œ ë°œê²¬")
                        
                        response = self.generate_rag_response_with_adaptive_processing(
                            query, fallback_documents, query_type
                        )
                        with st.expander("ğŸ¤– AI ë‹µë³€ (ëŒ€ì²´ ê²€ìƒ‰)", expanded=True):
                            st.write(response)
                        
                        st.session_state.messages.append({"role": "assistant", "content": response})
                    else:
                        self._show_no_results_message(target_service_name, query_type)
    
    def _show_no_results_message(self, target_service_name, query_type):
        """ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê°œì„  ë°©ì•ˆ ì œì‹œ"""
        error_msg = f"""
        '{target_service_name or 'í•´ë‹¹ ì¡°ê±´'}'ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
        
        **ğŸ”§ ê°œì„  ë°©ì•ˆ:**
        - ì„œë¹„ìŠ¤ëª…ì˜ ì¼ë¶€ë§Œ ì…ë ¥í•´ë³´ì„¸ìš” (ì˜ˆ: 'API' ëŒ€ì‹  'API_Link')
        - ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”
        - ì „ì²´ ê²€ìƒ‰ì„ ì›í•˜ì‹œë©´ ì„œë¹„ìŠ¤ëª…ì„ ì œì™¸í•˜ê³  ê²€ìƒ‰í•´ì£¼ì„¸ìš”
        - ë” ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”
        
        **ğŸ’¡ {query_type.upper()} ì¿¼ë¦¬ ìµœì í™” íŒ:**
        """
        
        # ì¿¼ë¦¬ íƒ€ì…ë³„ ê°œì„  íŒ ì¶”ê°€
        if query_type == 'repair':
            error_msg += """
        - ì„œë¹„ìŠ¤ëª…ê³¼ ì¥ì• í˜„ìƒì„ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”
        - 'ë³µêµ¬ë°©ë²•', 'í•´ê²°ë°©ë²•' í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”
        """
        elif query_type == 'cause':
            error_msg += """
        - 'ì›ì¸', 'ì´ìœ ', 'ì™œ' ë“±ì˜ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”
        - ì¥ì•  í˜„ìƒì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
        """
        elif query_type == 'similar':
            error_msg += """
        - 'ìœ ì‚¬', 'ë¹„ìŠ·í•œ', 'ë™ì¼í•œ' í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”
        - í•µì‹¬ ì¥ì•  í˜„ìƒë§Œ ê°„ê²°í•˜ê²Œ ê¸°ìˆ í•˜ì„¸ìš”
        """
        else:
            error_msg += """
        - í†µê³„ë‚˜ í˜„í™© ì¡°íšŒ ì‹œ ê¸°ê°„ì„ ëª…ì‹œí•˜ì„¸ìš”
        - 'ê±´ìˆ˜', 'í†µê³„', 'í˜„í™©' ë“±ì˜ í‚¤ì›Œë“œë¥¼ í™œìš©í•˜ì„¸ìš”
        """
        
        with st.expander("ğŸ¤– AI ë‹µë³€", expanded=True):
            st.write(error_msg)
        
        st.session_state.messages.append({"role": "assistant", "content": error_msg})