import streamlit as st
import re
import os
from config.settings_local import AppConfigLocal

class SearchManagerLocal:
    """ê²€ìƒ‰ ê´€ë ¨ ê¸°ëŠ¥ ê´€ë¦¬ í´ë˜ìŠ¤ - effect í•„ë“œ ê¸°ë°˜ ì˜ë¯¸ì  ìœ ì‚¬ì„± ê²€ìƒ‰ ìµœì í™”"""
    
    def __init__(self, search_client, config=None):
        self.search_client = search_client
        self.config = config if config else AppConfigLocal()
        self._service_names_cache = None
        self._cache_loaded = False
        # effect íŒ¨í„´ ìºì‹œ
        self._effect_patterns_cache = None
        self._effect_cache_loaded = False
        
        # config í´ë” ê²½ë¡œ ì„¤ì •
        self.config_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config')
        self.service_names_file = os.path.join(self.config_dir, 'service_names.txt')
    
    @st.cache_data(ttl=3600)
    def _load_effect_patterns_from_rag(_self):
        """RAG ë°ì´í„°ì—ì„œ effect í•„ë“œì˜ íŒ¨í„´ë“¤ì„ ë¶„ì„í•˜ì—¬ ìºì‹œ"""
        try:
            results = _self.search_client.search(
                search_text="*",
                top=1000,
                select=["effect", "symptom", "service_name"],
                include_total_count=True
            )
            
            effect_patterns = {}
            
            for result in results:
                effect = result.get("effect", "").strip()
                symptom = result.get("symptom", "").strip()
                service_name = result.get("service_name", "").strip()
                
                if effect:
                    # effectë¥¼ ì •ê·œí™”í•˜ì—¬ í‚¤ì›Œë“œ ê·¸ë£¹ ìƒì„±
                    normalized_effect = _self._normalize_text_for_similarity(effect)
                    keywords = _self._extract_semantic_keywords(effect)
                    
                    if keywords:
                        for keyword in keywords:
                            if keyword not in effect_patterns:
                                effect_patterns[keyword] = []
                            effect_patterns[keyword].append({
                                'original_effect': effect,
                                'normalized_effect': normalized_effect,
                                'symptom': symptom,
                                'service_name': service_name,
                                'keywords': keywords
                            })
            
            return effect_patterns
            
        except Exception as e:
            return {}
    
    def _normalize_text_for_similarity(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¹„êµë¥¼ ìœ„í•´ ì •ê·œí™”"""
        if not text:
            return ""
        
        # ë„ì–´ì“°ê¸° ì œê±°
        normalized = re.sub(r'\s+', '', text.lower())
        
        # ì˜ë¯¸ê°€ ê°™ì€ í‘œí˜„ë“¤ì„ í†µì¼
        replacements = {
            # ë¶ˆê°€/ì‹¤íŒ¨ ê´€ë ¨
            'ë¶ˆê°€ëŠ¥': 'ë¶ˆê°€',
            'ì‹¤íŒ¨': 'ë¶ˆê°€',
            'ì•ˆë¨': 'ë¶ˆê°€',
            'ë˜ì§€ì•ŠìŒ': 'ë¶ˆê°€',
            'í• ìˆ˜ì—†ìŒ': 'ë¶ˆê°€',
            
            # ì ‘ì†/ì—°ê²° ê´€ë ¨
            'ì ‘ì†': 'ì—°ê²°',
            'ë¡œê·¸ì¸': 'ì ‘ì†',
            'ì•¡ì„¸ìŠ¤': 'ì ‘ì†',
            
            # ì˜¤ë¥˜/ì—ëŸ¬ ê´€ë ¨
            'ì˜¤ë¥˜': 'ì—ëŸ¬',
            'ì¥ì• ': 'ì—ëŸ¬',
            'ë¬¸ì œ': 'ì—ëŸ¬',
            'ì´ìŠˆ': 'ì—ëŸ¬',
            
            # ì§€ì—°/ëŠë¦¼ ê´€ë ¨
            'ì§€ì—°': 'ëŠë¦¼',
            'ëŠ¦ìŒ': 'ëŠë¦¼',
            'ì‘ë‹µì—†ìŒ': 'ëŠë¦¼',
            
            # ì„œë¹„ìŠ¤/ê¸°ëŠ¥ ê´€ë ¨
            'ì„œë¹„ìŠ¤': 'ê¸°ëŠ¥',
            'ì‹œìŠ¤í…œ': 'ì„œë¹„ìŠ¤',
            'í”Œë«í¼': 'ì„œë¹„ìŠ¤',
            
            # ê°€ì…/ë“±ë¡ ê´€ë ¨
            'ê°€ì…': 'ë“±ë¡',
            'ì‹ ì²­': 'ë“±ë¡',
            'íšŒì›ê°€ì…': 'ë“±ë¡',
            
            # ê²°ì œ/êµ¬ë§¤ ê´€ë ¨
            'ê²°ì œ': 'êµ¬ë§¤',
            'êµ¬ë§¤': 'ê²°ì œ',
            'ì£¼ë¬¸': 'ê²°ì œ',
            'ê±°ë˜': 'ê²°ì œ'
        }
        
        for old, new in replacements.items():
            normalized = normalized.replace(old, new)
        
        return normalized
    
    def _extract_semantic_keywords(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ì  í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not text:
            return []
        
        # í•µì‹¬ í‚¤ì›Œë“œ íŒ¨í„´ë“¤
        keyword_patterns = [
            # ë™ì‘ + ëŒ€ìƒ íŒ¨í„´
            r'(\w+)(ë¶ˆê°€|ì‹¤íŒ¨|ì—ëŸ¬|ì˜¤ë¥˜|ì§€ì—°|ëŠë¦¼)',
            r'(\w+)(ê°€ì…|ë“±ë¡|ì‹ ì²­)',
            r'(\w+)(ê²°ì œ|êµ¬ë§¤|ì£¼ë¬¸)',
            r'(\w+)(ì ‘ì†|ì—°ê²°|ë¡œê·¸ì¸)',
            r'(\w+)(ì¡°íšŒ|ê²€ìƒ‰|í™•ì¸)',
            
            # ëŒ€ìƒ + ìƒíƒœ íŒ¨í„´
            r'(ë³´í—˜|ê°€ì…|ê²°ì œ|ì ‘ì†|ë¡œê·¸ì¸|ì¡°íšŒ|ê²€ìƒ‰|ì£¼ë¬¸|êµ¬ë§¤)(\w*)',
            
            # ì„œë¹„ìŠ¤ëª… ê´€ë ¨
            r'(ì•±|ì›¹|ì‚¬ì´íŠ¸|í˜ì´ì§€|ì‹œìŠ¤í…œ|ì„œë¹„ìŠ¤)(\w*)',
            
            # ë‹¨ë… ì¤‘ìš” í‚¤ì›Œë“œ
            r'\b(ë³´í—˜|ê°€ì…|ë¶ˆê°€|ì‹¤íŒ¨|ì—ëŸ¬|ì˜¤ë¥˜|ì§€ì—°|ì ‘ì†|ë¡œê·¸ì¸|ê²°ì œ|êµ¬ë§¤|ì£¼ë¬¸|ì¡°íšŒ|ê²€ìƒ‰)\b'
        ]
        
        keywords = set()
        text_normalized = self._normalize_text_for_similarity(text)
        
        for pattern in keyword_patterns:
            matches = re.findall(pattern, text_normalized, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    keywords.update([m for m in match if m and len(m) >= 2])
                elif match and len(match) >= 2:
                    keywords.add(match)
        
        # ì¶”ê°€ë¡œ 2ê¸€ì ì´ìƒì˜ ëª…ì‚¬ë“¤ ì¶”ì¶œ
        noun_pattern = r'[ê°€-í£]{2,}'
        nouns = re.findall(noun_pattern, text)
        keywords.update([self._normalize_text_for_similarity(noun) for noun in nouns if len(noun) >= 2])
        
        return list(keywords)
    
    def get_effect_patterns_from_rag(self):
        """RAG ë°ì´í„°ì—ì„œ effect íŒ¨í„´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ í™œìš©)"""
        if not self._effect_cache_loaded:
            self._effect_patterns_cache = self._load_effect_patterns_from_rag()
            self._effect_cache_loaded = True
        return self._effect_patterns_cache or {}
    
    def _expand_query_with_semantic_similarity(self, query):
        """ì¿¼ë¦¬ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í‘œí˜„ë“¤ë¡œ í™•ì¥"""
        effect_patterns = self.get_effect_patterns_from_rag()
        
        if not effect_patterns:
            return query
        
        # ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        query_keywords = self._extract_semantic_keywords(query)
        query_normalized = self._normalize_text_for_similarity(query)
        
        # ìœ ì‚¬í•œ effect íŒ¨í„´ ì°¾ê¸°
        similar_effects = set()
        semantic_expansions = set()
        
        for keyword in query_keywords:
            if keyword in effect_patterns:
                for pattern_info in effect_patterns[keyword]:
                    # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
                    similarity = self._calculate_text_similarity(
                        query_normalized, 
                        pattern_info['normalized_effect']
                    )
                    
                    if similarity > 0.3:  # 30% ì´ìƒ ìœ ì‚¬í•˜ë©´ í¬í•¨
                        similar_effects.add(pattern_info['original_effect'])
                        # í•´ë‹¹ íŒ¨í„´ì˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë“¤ë„ ì¶”ê°€
                        semantic_expansions.update(pattern_info['keywords'])
        
        # ì¿¼ë¦¬ í™•ì¥
        if similar_effects or semantic_expansions:
            expanded_terms = []
            
            # ì›ë³¸ ì¿¼ë¦¬
            expanded_terms.append(f'({query})')
            
            # ìœ ì‚¬í•œ effectë“¤
            for effect in list(similar_effects)[:5]:  # ìµœëŒ€ 5ê°œê¹Œì§€
                expanded_terms.append(f'(effect:"{effect}")')
            
            # ì˜ë¯¸ì ìœ¼ë¡œ í™•ì¥ëœ í‚¤ì›Œë“œë“¤
            if semantic_expansions:
                semantic_query_parts = []
                for expansion in list(semantic_expansions)[:10]:  # ìµœëŒ€ 10ê°œê¹Œì§€
                    semantic_query_parts.append(expansion)
                if semantic_query_parts:
                    expanded_terms.append(f'({" OR ".join(semantic_query_parts)})')
            
            expanded_query = ' OR '.join(expanded_terms)
            return expanded_query
        
        return query
    
    def _calculate_text_similarity(self, text1, text2):
        """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„ ê¸°ë°˜)"""
        if not text1 or not text2:
            return 0
        
        # 2-gram ê¸°ë°˜ ìœ ì‚¬ë„ (ë” ì •í™•í•œ ìœ ì‚¬ë„ ì¸¡ì •)
        def get_bigrams(text):
            return set([text[i:i+2] for i in range(len(text)-1)])
        
        bigrams1 = get_bigrams(text1)
        bigrams2 = get_bigrams(text2)
        
        if not bigrams1 or not bigrams2:
            return 0
        
        intersection = len(bigrams1.intersection(bigrams2))
        union = len(bigrams1.union(bigrams2))
        
        return intersection / union if union > 0 else 0
    
    def _boost_semantic_documents(self, documents, query):
        """ì˜ë¯¸ì  ìœ ì‚¬ì„±ì´ ë†’ì€ ë¬¸ì„œë“¤ì˜ ì ìˆ˜ ë¶€ìŠ¤íŒ…"""
        query_normalized = self._normalize_text_for_similarity(query)
        query_keywords = set(self._extract_semantic_keywords(query))
        
        for doc in documents:
            effect = doc.get('effect', '')
            symptom = doc.get('symptom', '')
            
            # effectì™€ symptom ëª¨ë‘ì—ì„œ ìœ ì‚¬ë„ ê³„ì‚°
            effect_similarity = 0
            symptom_similarity = 0
            
            if effect:
                effect_normalized = self._normalize_text_for_similarity(effect)
                effect_similarity = self._calculate_text_similarity(query_normalized, effect_normalized)
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
                effect_keywords = set(self._extract_semantic_keywords(effect))
                keyword_overlap = len(query_keywords.intersection(effect_keywords))
                if keyword_overlap > 0:
                    effect_similarity += (keyword_overlap * 0.1)  # í‚¤ì›Œë“œ ë§¤ì¹­ë‹¹ 10% ë³´ë„ˆìŠ¤
            
            if symptom:
                symptom_normalized = self._normalize_text_for_similarity(symptom)
                symptom_similarity = self._calculate_text_similarity(query_normalized, symptom_normalized)
            
            # ìµœê³  ìœ ì‚¬ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ìˆ˜ ë¶€ìŠ¤íŒ…
            max_similarity = max(effect_similarity, symptom_similarity)
            
            if max_similarity > 0.3:  # 30% ì´ìƒ ìœ ì‚¬í•˜ë©´ ë¶€ìŠ¤íŒ…
                original_score = doc.get('final_score', doc.get('score', 0))
                boost_factor = 1 + (max_similarity * 0.5)  # ìµœëŒ€ 50% ë¶€ìŠ¤íŒ…
                doc['final_score'] = original_score * boost_factor
                doc['semantic_similarity'] = max_similarity
                
                # ë¶€ìŠ¤íŒ… ì´ìœ  í‘œì‹œ
                if 'filter_reason' in doc:
                    doc['filter_reason'] += f" + ì˜ë¯¸ì  ìœ ì‚¬ë„ ë¶€ìŠ¤íŒ… ({max_similarity:.2f})"
        
        return documents

    @st.cache_data(ttl=3600)
    def _load_service_names_from_file(_self):
        """config/service_names.txt íŒŒì¼ì—ì„œ ì„œë¹„ìŠ¤ëª… ëª©ë¡ì„ ë¡œë“œí•˜ì—¬ ìºì‹œ"""
        try:
            # config í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not os.path.exists(_self.config_dir):
                os.makedirs(_self.config_dir)
            
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            if not os.path.exists(_self.service_names_file):
                st.warning(f"ì„œë¹„ìŠ¤ëª… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {_self.service_names_file}")
                return []
            
            # íŒŒì¼ ì½ê¸°
            service_names = []
            with open(_self.service_names_file, 'r', encoding='utf-8') as f:
                for line in f:
                    service_name = line.strip()
                    if service_name:  # ë¹ˆ ì¤„ ì œì™¸
                        service_names.append(service_name)
            
            # ê¸¸ì´ìˆœìœ¼ë¡œ ì •ë ¬ (ê¸´ ê²ƒë¶€í„°) - ë” êµ¬ì²´ì ì¸ ë§¤ì¹­ ìš°ì„ 
            sorted_service_names = sorted(service_names, key=len, reverse=True)
            
            st.success(f"âœ… config/service_names.txtì—ì„œ {len(sorted_service_names)}ê°œì˜ ì„œë¹„ìŠ¤ëª…ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return sorted_service_names
            
        except Exception as e:
            st.error(f"ì„œë¹„ìŠ¤ëª… íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_service_names_from_file(self):
        """config/service_names.txt íŒŒì¼ì—ì„œ ì„œë¹„ìŠ¤ëª… ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ í™œìš©)"""
        if not self._cache_loaded:
            self._service_names_cache = self._load_service_names_from_file()
            self._cache_loaded = True
        return self._service_names_cache or []
    
    def _normalize_for_matching(self, text):
        """ë§¤ì¹­ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •ê·œí™” (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)"""
        if not text:
            return ""
        
        # ëª¨ë“  ê³µë°±, í•˜ì´í”ˆ, ì–¸ë”ìŠ¤ì½”ì–´, ìŠ¬ë˜ì‹œ ë“±ì„ ì œê±°í•˜ê³  ì†Œë¬¸ìë¡œ ë³€í™˜
        normalized = re.sub(r'[\s\-_/\(\)\+]', '', text.lower())
        return normalized
    
    def extract_service_name_from_query(self, query):
        """config/service_names.txt íŒŒì¼ ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ (ê³µë°± ë¬´ì‹œ ë§¤ì¹­)"""
        service_names = self.get_service_names_from_file()
        
        if not service_names:
            st.warning("ì„œë¹„ìŠ¤ëª… ëª©ë¡ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # ì¿¼ë¦¬ ì •ê·œí™”
        query_normalized = self._normalize_for_matching(query)
        
        # 1ë‹¨ê³„: ì™„ì „ ì¼ì¹˜ ê²€ìƒ‰ (ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ ê¸°ì¤€)
        for service_name in service_names:
            service_normalized = self._normalize_for_matching(service_name)
            if service_normalized in query_normalized:
                return service_name
        
        # 2ë‹¨ê³„: ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰ (ì–‘ë°©í–¥)
        for service_name in service_names:
            service_normalized = self._normalize_for_matching(service_name)
            # ì„œë¹„ìŠ¤ëª…ì´ ì¿¼ë¦¬ì— í¬í•¨ë˜ì–´ ìˆê±°ë‚˜, ì¿¼ë¦¬ê°€ ì„œë¹„ìŠ¤ëª…ì— í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš°
            if (service_normalized in query_normalized) or (query_normalized in service_normalized):
                # ë‹¨, ë„ˆë¬´ ì§§ì€ ë§¤ì¹­ì€ ì œì™¸ (ìµœì†Œ 3ê¸€ì)
                if len(service_normalized) >= 3:
                    return service_name
        
        # 3ë‹¨ê³„: ë‹¨ì–´ë³„ ë§¤ì¹­ (ì˜ë¬¸/í•œê¸€ ë‹¨ì–´ ë¶„ë¦¬)
        query_words = re.findall(r'[A-Za-z]+|[ê°€-í£]+', query)
        if query_words:
            for service_name in service_names:
                service_words = re.findall(r'[A-Za-z]+|[ê°€-í£]+', service_name)
                for query_word in query_words:
                    if len(query_word) >= 3:  # 3ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ ë§¤ì¹­
                        query_word_normalized = self._normalize_for_matching(query_word)
                        for service_word in service_words:
                            service_word_normalized = self._normalize_for_matching(service_word)
                            if query_word_normalized == service_word_normalized:
                                return service_name
                            # ë¶€ë¶„ ë§¤ì¹­ë„ í—ˆìš© (ë” ê¸´ ë‹¨ì–´ ì•ˆì— í¬í•¨)
                            elif (len(query_word_normalized) >= 4 and 
                                  (query_word_normalized in service_word_normalized or 
                                   service_word_normalized in query_word_normalized)):
                                return service_name
        
        # 4ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        best_match = None
        best_similarity = 0.0
        
        for service_name in service_names:
            service_normalized = self._normalize_for_matching(service_name)
            similarity = self._calculate_text_similarity(query_normalized, service_normalized)
            
            # 60% ì´ìƒ ìœ ì‚¬í•˜ê³ , ì„œë¹„ìŠ¤ëª…ì´ 3ê¸€ì ì´ìƒì¸ ê²½ìš°ë§Œ ê³ ë ¤
            if similarity > 0.6 and len(service_normalized) >= 3 and similarity > best_similarity:
                best_similarity = similarity
                best_match = service_name
        
        return best_match

    def calculate_hybrid_score(self, search_score, reranker_score):
        """ê²€ìƒ‰ ì ìˆ˜ì™€ Reranker ì ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°"""
        if reranker_score > 0:
            normalized_reranker = min(reranker_score / 4.0, 1.0)
            normalized_search = min(search_score, 1.0)
            hybrid_score = (normalized_reranker * 0.8) + (normalized_search * 0.2)
        else:
            hybrid_score = min(search_score, 1.0)
        
        return hybrid_score

    def advanced_filter_documents_v3(self, documents, query_type="default", query_text="", target_service_name=None):
        """ì„œë¹„ìŠ¤ëª… í¬í•¨ ë§¤ì¹­ì„ ì§€ì›í•˜ëŠ” ê°œì„ ëœ í•„í„°ë§ (ì˜ë¯¸ì  ìœ ì‚¬ì„± ìµœì í™”)"""
        
        # ë™ì  ì„ê³„ê°’ íšë“
        thresholds = self.config.get_dynamic_thresholds(query_type, query_text)
        
        # ì˜ë¯¸ì  ìœ ì‚¬ì„±ì´ ë†’ì€ ë¬¸ì„œë“¤ì— ì ìˆ˜ ë¶€ìŠ¤íŒ… ì ìš©
        documents = self._boost_semantic_documents(documents, query_text)
        
        filtered_docs = []
        filter_stats = {
            'total': len(documents),
            'search_filtered': 0,
            'service_exact_match': 0,
            'service_partial_match': 0,
            'service_filtered': 0,
            'reranker_qualified': 0,
            'hybrid_qualified': 0,
            'semantic_boosted': 0,
            'final_selected': 0
        }
        
        for doc in documents:
            search_score = doc.get('score', 0)
            reranker_score = doc.get('reranker_score', 0)
            
            # ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¶€ìŠ¤íŒ…ì´ ì ìš©ëœ ê²½ìš° í†µê³„ì— ë°˜ì˜
            if 'semantic_similarity' in doc:
                filter_stats['semantic_boosted'] += 1
            
            # 1ë‹¨ê³„: ê¸°ë³¸ ê²€ìƒ‰ ì ìˆ˜ í•„í„°ë§
            if search_score < thresholds['search_threshold']:
                continue
            filter_stats['search_filtered'] += 1
            
            # 2ë‹¨ê³„: ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ (ê³µë°± ë¬´ì‹œ ë§¤ì¹­ ì ìš©)
            if target_service_name:
                doc_service_name = doc.get('service_name', '').strip()
                
                # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¡œ ë§¤ì¹­
                doc_service_normalized = self._normalize_for_matching(doc_service_name)
                target_service_normalized = self._normalize_for_matching(target_service_name)
                
                if doc_service_normalized == target_service_normalized:
                    filter_stats['service_exact_match'] += 1
                    doc['service_match_type'] = 'exact'
                elif (target_service_normalized in doc_service_normalized or 
                      doc_service_normalized in target_service_normalized):
                    filter_stats['service_partial_match'] += 1
                    doc['service_match_type'] = 'partial'
                else:
                    continue
            else:
                doc['service_match_type'] = 'all'
                
            filter_stats['service_filtered'] += 1
            
            # 3ë‹¨ê³„: Reranker ì ìˆ˜ ìš°ì„  í‰ê°€
            if reranker_score >= thresholds['reranker_threshold']:
                filter_stats['reranker_qualified'] += 1
                match_type = doc.get('service_match_type', 'unknown')
                doc['filter_reason'] = f"íŒŒì¼ ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… {match_type} ë§¤ì¹­ + Reranker ê³ í’ˆì§ˆ (ì ìˆ˜: {reranker_score:.2f})"
                doc['final_score'] = reranker_score
                doc['quality_tier'] = 'Premium'
                filtered_docs.append(doc)
                filter_stats['final_selected'] += 1
                continue
            
            # 4ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ í‰ê°€ (ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¶€ìŠ¤íŒ… ë°˜ì˜)
            hybrid_score = self.calculate_hybrid_score(search_score, reranker_score)
            final_score = doc.get('final_score', hybrid_score)  # ë¶€ìŠ¤íŒ…ëœ ì ìˆ˜ ì‚¬ìš©
            
            if final_score >= thresholds['hybrid_threshold']:
                filter_stats['hybrid_qualified'] += 1
                match_type = doc.get('service_match_type', 'unknown')
                doc['filter_reason'] = f"íŒŒì¼ ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… {match_type} ë§¤ì¹­ + í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ í†µê³¼ (ì ìˆ˜: {final_score:.2f})"
                doc['quality_tier'] = 'Standard'
                filtered_docs.append(doc)
                filter_stats['final_selected'] += 1
        
        # ì˜ë¯¸ì  ìœ ì‚¬ë„ì™€ ì ìˆ˜ë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ì •ë ¬
        def sort_key(doc):
            match_priority = {'exact': 3, 'partial': 2, 'all': 1}
            semantic_boost = doc.get('semantic_similarity', 0) * 0.1  # ì˜ë¯¸ì  ìœ ì‚¬ë„ ë³´ë„ˆìŠ¤
            return (
                match_priority.get(doc.get('service_match_type', 'all'), 0), 
                doc.get('final_score', 0) + semantic_boost
            )
        
        filtered_docs.sort(key=sort_key, reverse=True)
        
        # ìµœì¢… ê²°ê³¼ ìˆ˜ ì œí•œ
        final_docs = filtered_docs[:thresholds['max_results']]
       
        # ê°œì„ ëœ í†µê³„ í‘œì‹œ
        st.info(f"""
        ğŸ“Š **íŒŒì¼ ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ + ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¬¸ì„œ í•„í„°ë§ ê²°ê³¼**
        - ğŸ” ì „ì²´ ê²€ìƒ‰ ê²°ê³¼: {filter_stats['total']}ê°œ
        - âœ… ê¸°ë³¸ ì ìˆ˜ í†µê³¼: {filter_stats['search_filtered']}ê°œ
        - âœ… ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ ì™„ë£Œ: {filter_stats['service_filtered']}ê°œ (ì •í™•: {filter_stats['service_exact_match']}, ë¶€ë¶„: {filter_stats['service_partial_match']})
        - ğŸ† Reranker ê³ í’ˆì§ˆ: {filter_stats['reranker_qualified']}ê°œ
        - ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ í†µê³¼: {filter_stats['hybrid_qualified']}ê°œ
        - ğŸ§  ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¶€ìŠ¤íŒ…: {filter_stats['semantic_boosted']}ê°œ
        - ğŸ“‹ ìµœì¢… ì„ ë³„: {len(final_docs)}ê°œ
        """)
        
        return final_docs

    def semantic_search_with_service_filter(self, query, target_service_name=None, query_type="default", top_k=20):
        """ì„œë¹„ìŠ¤ëª… í¬í•¨ ê²€ìƒ‰ì„ ì§€ì›í•˜ëŠ” ê°œì„ ëœ ì‹œë§¨í‹± ê²€ìƒ‰ - ì˜ë¯¸ì  ìœ ì‚¬ì„± ìµœì í™”"""
        try:
            # ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜ ì¿¼ë¦¬ í™•ì¥
            expanded_query = self._expand_query_with_semantic_similarity(query)
            
            # í™•ì¥ëœ ì¿¼ë¦¬ê°€ ì›ë³¸ê³¼ ë‹¤ë¥´ë©´ ë” ë§ì€ ê²°ê³¼ ìš”ì²­
            if expanded_query != query:
                top_k = max(top_k, 30)
                st.info(f"ğŸ“„ 1ë‹¨ê³„: {top_k}ê°œ ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘... (ì˜ë¯¸ì  ìœ ì‚¬ì„± í™•ì¥ ì ìš©)")
            else:
                st.info(f"ğŸ“„ 1ë‹¨ê³„: {top_k}ê°œ ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘... (ë¡œì»¬ ê²€ìƒ‰)")
            
            # íŒŒì¼ ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… í¬í•¨ ê²€ìƒ‰ì„ ìœ„í•œ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            if target_service_name:
                enhanced_query = f'(service_name:"{target_service_name}" OR service_name:*{target_service_name}*)'
                if expanded_query != target_service_name:
                    enhanced_query += f" AND ({expanded_query})"
            else:
                enhanced_query = expanded_query
            
            # ì‹œë§¨í‹± ê²€ìƒ‰ ì‹¤í–‰
            results = self.search_client.search(
                search_text=enhanced_query,
                top=top_k,
                query_type="semantic",
                semantic_configuration_name="iap-incident-rebuild-meaning",
                include_total_count=True,
                select=[
                    "incident_id", "service_name", "error_time", "effect", "symptom", "repair_notice", 
                    "error_date", "week", "daynight", "root_cause", "incident_repair", 
                    "incident_plan", "cause_type", "done_type", "incident_grade", 
                    "owner_depart", "year", "month"
                ]
            )
            
            documents = []
            for result in results:
                documents.append({
                    "incident_id": result.get("incident_id", ""),
                    "service_name": result.get("service_name", ""),
                    "error_time": result.get("error_time", 0),
                    "effect": result.get("effect", ""),
                    "symptom": result.get("symptom", ""),
                    "repair_notice": result.get("repair_notice", ""),
                    "error_date": result.get("error_date", ""),
                    "week": result.get("week", ""),
                    "daynight": result.get("daynight", ""),
                    "root_cause": result.get("root_cause", ""),
                    "incident_repair": result.get("incident_repair", ""),
                    "incident_plan": result.get("incident_plan", ""),
                    "cause_type": result.get("cause_type", ""),
                    "done_type": result.get("done_type", ""),
                    "incident_grade": result.get("incident_grade", ""),
                    "owner_depart": result.get("owner_depart", ""),
                    "year": result.get("year", ""),
                    "month": result.get("month", ""),
                    "score": result.get("@search.score", 0),
                    "reranker_score": result.get("@search.reranker_score", 0)
                })
            
            st.info(f"ğŸ¯ 2ë‹¨ê³„: íŒŒì¼ ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ + ì˜ë¯¸ì  ìœ ì‚¬ì„± ê³ í’ˆì§ˆ ë¬¸ì„œ ì„ ë³„ ì¤‘...")
            
            # ê°œì„ ëœ í•„í„°ë§ ì ìš©
            filtered_documents = self.advanced_filter_documents_v3(documents, query_type, query, target_service_name)
            
            return filtered_documents
            
        except Exception as e:
            st.warning(f"ì‹œë§¨í‹± ê²€ìƒ‰ ì‹¤íŒ¨, ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´: {str(e)}")
            return self.search_documents_fallback(query, target_service_name)

    def search_documents_fallback(self, query, target_service_name=None):
        """ë§¤ìš° ê´€ëŒ€í•œ ê¸°ì¤€ìœ¼ë¡œ fallback ê²€ìƒ‰"""
        try:
            # ë§¤ìš° ê´€ëŒ€í•œ ê²€ìƒ‰ (ì„œë¹„ìŠ¤ëª… í•„í„°ë§ì€ ìœ ì§€)
            if target_service_name:
                enhanced_query = f'(service_name:"{target_service_name}" OR service_name:*{target_service_name}*)'
                if query != target_service_name:
                    enhanced_query += f" AND ({query})"
            else:
                enhanced_query = query
            
            results = self.search_client.search(
                search_text=enhanced_query,
                top=15,  # ë” ì ì€ ê²°ê³¼
                include_total_count=True,
                select=[
                    "incident_id", "service_name", "error_time", "effect", "symptom", "repair_notice", 
                    "error_date", "week", "daynight", "root_cause", "incident_repair", 
                    "incident_plan", "cause_type", "done_type", "incident_grade", 
                    "owner_depart", "year", "month"
                ]
            )
            
            documents = []
            for result in results:
                documents.append({
                    "incident_id": result.get("incident_id", ""),
                    "service_name": result.get("service_name", ""),
                    "error_time": result.get("error_time", 0),
                    "effect": result.get("effect", ""),
                    "symptom": result.get("symptom", ""),
                    "repair_notice": result.get("repair_notice", ""),
                    "error_date": result.get("error_date", ""),
                    "week": result.get("week", ""),
                    "daynight": result.get("daynight", ""),
                    "root_cause": result.get("root_cause", ""),
                    "incident_repair": result.get("incident_repair", ""),
                    "incident_plan": result.get("incident_plan", ""),
                    "cause_type": result.get("cause_type", ""),
                    "done_type": result.get("done_type", ""),
                    "incident_grade": result.get("incident_grade", ""),
                    "owner_depart": result.get("owner_depart", ""),
                    "year": result.get("year", ""),
                    "month": result.get("month", ""),
                    "score": result.get("@search.score", 0),
                    "reranker_score": result.get("@search.reranker_score", 0)
                })
            
            # ë§¤ìš° ê´€ëŒ€í•œ í•„í„°ë§ ì ìš©
            fallback_docs = []
            for doc in documents:
                search_score = doc.get('score', 0)
                
                # ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ (íŒŒì¼ ê¸°ë°˜)
                if target_service_name:
                    doc_service_name = doc.get('service_name', '').strip()
                    doc_service_normalized = self._normalize_for_matching(doc_service_name)
                    target_service_normalized = self._normalize_for_matching(target_service_name)
                    
                    if (doc_service_normalized == target_service_normalized or 
                        target_service_normalized in doc_service_normalized or 
                        doc_service_normalized in target_service_normalized):
                        doc['service_match_type'] = 'fallback'
                        doc['filter_reason'] = f"íŒŒì¼ ê¸°ë°˜ fallback ë§¤ì¹­ (ì ìˆ˜: {search_score:.2f})"
                        doc['quality_tier'] = 'Basic'
                        doc['final_score'] = search_score
                        fallback_docs.append(doc)
                else:
                    doc['service_match_type'] = 'fallback'
                    doc['filter_reason'] = f"íŒŒì¼ ê¸°ë°˜ fallback ê²€ìƒ‰ (ì ìˆ˜: {search_score:.2f})"
                    doc['quality_tier'] = 'Basic'
                    doc['final_score'] = search_score
                    fallback_docs.append(doc)
            
            # ì ìˆ˜ìˆœ ì •ë ¬
            fallback_docs.sort(key=lambda x: x.get('final_score', 0), reverse=True)
            
            return fallback_docs[:8]  # ìµœëŒ€ 8ê°œ
            
        except Exception as e:
            st.error(f"Fallback ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            return []