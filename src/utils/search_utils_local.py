import streamlit as st
import re
import math
import os
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from config.settings_local import AppConfigLocal
from utils.filter_manager import DocumentFilterManager, FilterConditions, QueryType

class SearchManagerLocal:
    """Vector í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê´€ë¦¬ í´ë˜ìŠ¤ - ë‘ ê°œì˜ ì¸ë±ìŠ¤ ì§€ì›"""
    
    def __init__(self, search_client, search_client_2, embedding_client, config=None):
        """
        Args:
            search_client: ì¥ì• ë‚´ì—­ ì¸ë±ìŠ¤ í´ë¼ì´ì–¸íŠ¸ (INDEX_REBUILD_NAME)
            search_client_2: ì´ìƒì§•í›„ë‚´ì—­ ì¸ë±ìŠ¤ í´ë¼ì´ì–¸íŠ¸ (INDEX_REBUILD_NAME2)
            embedding_client: ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸
            config: ì„¤ì • ê°ì²´
        """
        self.search_client = search_client  # ì¥ì• ë‚´ì—­ ì¸ë±ìŠ¤
        self.search_client_2 = search_client_2  # ì´ìƒì§•í›„ë‚´ì—­ ì¸ë±ìŠ¤
        self.embedding_client = embedding_client
        self.config = config or AppConfigLocal()
        self.debug_mode = False
        
        # í†µí•© í•„í„°ë§ ë§¤ë‹ˆì €
        self.filter_manager = DocumentFilterManager(
            debug_mode=self.debug_mode, 
            search_manager=self, 
            config=self.config
        )
        
        # ìºì‹œ ë³€ìˆ˜ë“¤
        self._service_names_cache = None
        self._cache_loaded = False
        self._effect_patterns_cache = None
        self._effect_cache_loaded = False
        self._service_names_file_cache = None
        self._service_file_cache_loaded = False
        
        # RRF íŒŒë¼ë¯¸í„°
        self.rrf_k = getattr(config, 'rrf_k', 60)
        
        # í†µê³„ ì¿¼ë¦¬ ë™ì˜ì–´ ë§¤í•‘
        self.statistics_synonyms = {
            'ëª‡ê±´ì´ì•¼': 'ëª‡ê±´', 'ëª‡ê±´ì´ë‹ˆ': 'ëª‡ê±´', 'ëª‡ê±´ì¸ê°€': 'ëª‡ê±´',
            'ì•Œë ¤ì¤˜': '', 'ë³´ì—¬ì¤˜': '', 'ë§í•´ì¤˜': ''
        }
        
        # í…ìŠ¤íŠ¸ ì •ê·œí™” ë§¤í•‘
        self.text_replacements = {
            'ã„±': 'ã„±', 'ã„´': 'ã„´', 'ã„·': 'ã„·', 'ã„¹': 'ã„¹', 'ã…': 'ã…',
            'ã…‚': 'ã…‚', 'ã……': 'ã……', 'ã…‡': 'ã…‡', 'ã…ˆ': 'ã…ˆ', 'ã…Š': 'ã…Š',
            'ã…‹': 'ã…‹', 'ã…Œ': 'ã…Œ', 'ã…': 'ã…', 'ã…': 'ã…'
        }
        
        # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ ì •ì˜
        self.COMMON_TERM_SERVICES = {
            'OTP': ['otp', 'ì¼íšŒìš©ë¹„ë°€ë²ˆí˜¸', 'ì›íƒ€ì„íŒ¨ìŠ¤ì›Œë“œ', '2ì°¨ì¸ì¦', 'ì´ì¤‘ì¸ì¦'],           
            'ë³¸ì¸ì¸ì¦': ['ì‹¤ëª…ì¸ì¦', 'ì‹ ì›í™•ì¸'],
            'API': ['api', 'Application Programming Interface', 'REST API', 'APIí˜¸ì¶œ'],
            'SMS': ['sms', 'ë¬¸ì', 'ë‹¨ë¬¸', 'Short Message Service', 'ë¬¸ìë©”ì‹œì§€'],
            'VPN': ['vpn', 'Virtual Private Network', 'ê°€ìƒì‚¬ì„¤ë§'],
            'DNS': ['dns', 'Domain Name System', 'ë„ë©”ì¸ë„¤ì„ì‹œìŠ¤í…œ'],
            'SSL': ['ssl', 'https', 'Secure Sockets Layer', 'ë³´ì•ˆì†Œì¼“ê³„ì¸µ'],
            'URL': ['url', 'link', 'ë§í¬', 'Uniform Resource Locator']
        }

    def semantic_search_with_adaptive_filtering_dual_index(self, query, target_service_name=None, query_type="default"):
        """
        ë‘ ê°œì˜ ì¸ë±ìŠ¤(ì¥ì• ë‚´ì—­ + ì´ìƒì§•í›„ë‚´ì—­)ë¥¼ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ë¥¼ ë³‘í•©
        
        Returns:
            dict: {
                'incidents': [...],  # ì¥ì• ë‚´ì—­ ë¦¬ìŠ¤íŠ¸
                'anomalies': [...]   # ì´ìƒì§•í›„ë‚´ì—­ ë¦¬ìŠ¤íŠ¸
            }
        """
        try:
            # ì¥ì• ë‚´ì—­ ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
            incidents = self.semantic_search_with_adaptive_filtering(
                query, target_service_name, query_type
            ) or []
            
            # ì´ìƒì§•í›„ë‚´ì—­ ê²€ìƒ‰ (ë™ì¼ ë¡œì§ìœ¼ë¡œ search_client_2 ì‚¬ìš©)
            anomalies = self._search_from_client(
                self.search_client_2, query, target_service_name, query_type
            ) or []
            
            # ë¬¸ì„œì— ì¶œì²˜ í‘œì‹œ ì¶”ê°€
            for doc in incidents:
                doc['_source_type'] = 'incident'  # ì¥ì• ë‚´ì—­
            
            for doc in anomalies:
                doc['_source_type'] = 'anomaly'  # ì´ìƒì§•í›„ë‚´ì—­
            
            return {
                'incidents': incidents,
                'anomalies': anomalies
            }
            
        except Exception as e:
            print(f"ERROR: dual index search failed: {e}")
            import traceback
            traceback.print_exc()
            return {'incidents': [], 'anomalies': []}
    
    def _search_from_client(self, client, query, target_service_name=None, query_type="default", top_k=15):
        """
        íŠ¹ì • search clientë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            client: ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸ (search_client ë˜ëŠ” search_client_2)
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            target_service_name: ëŒ€ìƒ ì„œë¹„ìŠ¤ëª…
            query_type: ì¿¼ë¦¬ íƒ€ì…
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
        """
        try:
            # â˜…â˜…â˜… ì´ìƒì§•í›„ ì¸ë±ìŠ¤ ì—¬ë¶€ íŒë‹¨ â˜…â˜…â˜…
            is_anomaly = (client == self.search_client_2)
            
            if is_anomaly:
                print(f"DEBUG: [ANOMALY] Using strict thresholds for anomaly index search")
            
            # ê¸°ë³¸ ê²€ìƒ‰ ë¡œì§
            grade_info = self.extract_incident_grade_from_query(query)
            time_info = self._extract_year_month_from_query_unified(query)
            
            enhanced_query = (self.build_grade_search_query(query, grade_info) 
                            if grade_info['has_grade_query'] else query)
            
            # ì‹œê°„ ì¡°ê±´ ì¶”ê°€
            enhanced_query = self._add_time_conditions(enhanced_query, time_info)
            
            # ì„œë¹„ìŠ¤ëª… ì¡°ê±´ ì¶”ê°€  
            if target_service_name:
                enhanced_query = self._add_service_conditions(enhanced_query, target_service_name)
            
            # â˜…â˜…â˜… ì´ìƒì§•í›„ëŠ” ë” ì ì€ ìˆ˜ì˜ ê²°ê³¼ë§Œ ê°€ì ¸ì˜´ â˜…â˜…â˜…
            actual_top_k = 10 if is_anomaly else top_k
            
            results = client.search(
                search_text=enhanced_query, top=actual_top_k, include_total_count=True,
                select=["incident_id", "service_name", "error_time", "effect", "symptom", "repair_notice",
                       "error_date", "week", "daynight", "root_cause", "incident_repair", "incident_plan",
                       "cause_type", "done_type", "incident_grade", "owner_depart", "year", "month"]
            )
            
            documents = [self._convert_search_result_to_document(result) for result in results]
            
            # â˜…â˜…â˜… ì´ìƒì§•í›„ëŠ” ì—„ê²©í•œ ìŠ¤ì½”ì–´ í•„í„°ë§ â˜…â˜…â˜…
            if is_anomaly:
                # ì„ê³„ê°’ ê°€ì ¸ì˜¤ê¸°
                thresholds = self.config.get_dynamic_thresholds(query_type, query, is_anomaly=True)
                search_threshold = thresholds.get('search_threshold', 0.35)
                reranker_threshold = thresholds.get('reranker_threshold', 2.5)
                
                print(f"DEBUG: [ANOMALY] Applying strict thresholds - search: {search_threshold}, reranker: {reranker_threshold}")
                
                # ì—„ê²©í•œ í•„í„°ë§ ì ìš©
                filtered_by_score = []
                for doc in documents:
                    search_score = doc.get('score', 0) or 0
                    reranker_score = doc.get('reranker_score', 0) or 0
                    
                    # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì„ê³„ê°’ì„ ë„˜ì–´ì•¼ í•¨
                    if search_score >= search_threshold or reranker_score >= reranker_threshold:
                        filtered_by_score.append(doc)
                        if self.debug_mode:
                            print(f"DEBUG: [ANOMALY] Kept doc - search_score: {search_score:.3f}, reranker_score: {reranker_score:.3f}")
                    else:
                        if self.debug_mode:
                            print(f"DEBUG: [ANOMALY] Filtered out - search_score: {search_score:.3f}, reranker_score: {reranker_score:.3f}")
                
                documents = filtered_by_score
                print(f"DEBUG: [ANOMALY] After score filtering: {len(documents)} documents")
            
            # í†µí•© í•„í„°ë§ ì‹œìŠ¤í…œ ì ìš©
            query_type_enum = self._convert_to_query_type_enum(query_type)
            
            # â˜…â˜…â˜… ì´ìƒì§•í›„ëŠ” LLM ê²€ì¦ë„ í™œì„±í™” (ë” ì—„ê²©) â˜…â˜…â˜…
            enable_llm = is_anomaly
            
            filtered_docs, _ = self.filter_manager.apply_comprehensive_filtering(
                documents, query, query_type_enum, enable_llm_validation=enable_llm
            )
            
            if is_anomaly:
                print(f"DEBUG: [ANOMALY] After comprehensive filtering: {len(filtered_docs)} documents")
            
            return filtered_docs
        except Exception as e:
            print(f"ERROR: _search_from_client failed: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _load_service_names_from_file(self):
        """config/service_names.txt íŒŒì¼ì—ì„œ ì„œë¹„ìŠ¤ëª… ëª©ë¡ ë¡œë“œ"""
        try:
            # search_utils_local.pyëŠ” src/utils/ì— ìœ„ì¹˜
            # service_names.txtëŠ” src/config/ì— ìœ„ì¹˜
            # ë”°ë¼ì„œ ìƒëŒ€ ê²½ë¡œ: ../config/service_names.txt
            current_dir = Path(__file__).parent
            config_path = current_dir.parent / "config" / "service_names.txt"
            
            if not config_path.exists() or not config_path.is_file():
                print(f"WARNING: service_names.txt file not found at: {config_path}")
                return []
            
            print(f"DEBUG: Found service_names.txt at: {config_path}")
            
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    service_names = []
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        # ë¹ˆ ì¤„ì´ë‚˜ ì£¼ì„(#ìœ¼ë¡œ ì‹œì‘) ì œì™¸
                        if line and not line.startswith('#'):
                            service_names.append(line)
                    
                    if service_names:
                        print(f"DEBUG: Successfully loaded {len(service_names)} service names from file")
                        # ê¸¸ì´ìˆœìœ¼ë¡œ ì •ë ¬ (ê¸´ ê²ƒë¶€í„°) - ë§¤ì¹­ ì •í™•ë„ í–¥ìƒ
                        return sorted(set(service_names), key=len, reverse=True)
                    else:
                        print(f"WARNING: service_names.txt is empty: {config_path}")
                        return []
                        
            except UnicodeDecodeError:
                print(f"ERROR: UTF-8 encoding error, trying EUC-KR encoding")
                try:
                    with open(config_path, 'r', encoding='euc-kr') as f:
                        service_names = [line.strip() for line in f 
                                    if line.strip() and not line.startswith('#')]
                        if service_names:
                            print(f"DEBUG: Successfully loaded {len(service_names)} service names with EUC-KR encoding")
                            return sorted(set(service_names), key=len, reverse=True)
                        return []
                except Exception as e:
                    print(f"ERROR: Failed to read with EUC-KR encoding: {e}")
                    return []
                    
            except Exception as e:
                print(f"ERROR: Failed to read {config_path}: {e}")
                return []
            
        except Exception as e:
            print(f"ERROR: Critical failure in _load_service_names_from_file: {e}")
            return []

    
    def get_service_names_from_file(self):
        """íŒŒì¼ ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ í™œìš©)"""
        if not self._service_file_cache_loaded:
            self._service_names_file_cache = self._load_service_names_from_file()
            self._service_file_cache_loaded = True
        return self._service_names_file_cache or []
    
    def _find_service_name_in_file(self, query: str) -> Optional[str]:
        """
        conf/service_names.txtì—ì„œ ì„œë¹„ìŠ¤ëª… ì°¾ê¸° - í•œê¸€ ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ ê°•í™” (ê°œì„  ë²„ì „)
        
        ê°œì„  ì‚¬í•­:
        1. í•œê¸€ ì¡°ì‚¬ ì²˜ë¦¬ ì¶”ê°€ (ê°€/ì´, ì„/ë¥¼, ì˜, ì—, ì—ì„œ ë“±)
        2. ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•œ ë” ê²¬ê³ í•œ ë‹¨ì–´ ê²½ê³„ ì²´í¬
        3. ë§¤ì¹­ ìš°ì„ ìˆœìœ„ ëª…í™•í™”
        4. ì„±ëŠ¥ ìµœì í™” ë° ì¤‘ë³µ ì½”ë“œ ì œê±°
        """
        file_service_names = self.get_service_names_from_file()
        if not file_service_names:
            if self.debug_mode:
                print("DEBUG: [SERVICE_FILE] No service names loaded from file")
            return None
        
        query_stripped = query.strip()
        query_lower = query_stripped.lower()
        query_tokens = self._extract_service_tokens(query)
        
        if self.debug_mode:
            print(f"DEBUG: [SERVICE_FILE] Searching in {len(file_service_names)} file service names")
            print(f"DEBUG: [SERVICE_FILE] Query: '{query}'")
        
        candidates = []
        
        # í•œê¸€ ì¡°ì‚¬ íŒ¨í„´ (ë‹¨ì–´ ë’¤ì— ë¶™ì„ ìˆ˜ ìˆëŠ” ì¡°ì‚¬ë“¤)
        korean_particles = r'(?:[ì´ê°€ì„ë¥¼ì˜ì—ì„œì™€ê³¼ë„ë§Œë¶€í„°ê¹Œì§€ë¡œìœ¼ë¡œëŠ”]|ì—ê²Œ|ì—ì„œ|ìœ¼ë¡œ|ë¡œì„œ|ë¶€í„°|ê¹Œì§€|ì²˜ëŸ¼)?'
        
        for service_name in file_service_names:
            service_lower = service_name.lower()
            
            # ========================================
            # 1ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ (ìµœìš°ì„ )
            # ========================================
            if service_name == query_stripped or service_lower == query_lower:
                if self.debug_mode:
                    print(f"DEBUG: [SERVICE_FILE] âœ… EXACT MATCH: '{service_name}'")
                return service_name
            
            # ========================================
            # 2ë‹¨ê³„: ë‹¨ì–´ ê²½ê³„ ë§¤ì¹­ (ì¦‰ì‹œ ë°˜í™˜) - í•µì‹¬ ê°œì„ !
            # ========================================
            if service_lower in query_lower:
                # ë°©ë²• 1: ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•œ ë‹¨ì–´ ê²½ê³„ ì²´í¬
                # \bëŠ” ì˜ë¬¸ì—ë§Œ ì‘ë™í•˜ë¯€ë¡œ, ì§ì ‘ ê²½ê³„ ì²´í¬
                escaped_service = re.escape(service_lower)
                
                # í•œê¸€ ì¡°ì‚¬ë¥¼ í¬í•¨í•œ íŒ¨í„´ ë§¤ì¹­
                pattern = r'(?:^|[\s\t\n,.;:!?()\[\]{}\"\'\-/])' + escaped_service + korean_particles + r'(?:[\s\t\n,.;:!?()\[\]{}\"\'\-/]|$)'
                
                if re.search(pattern, query_lower):
                    if self.debug_mode:
                        print(f"DEBUG: [SERVICE_FILE] âœ… WORD BOUNDARY MATCH (regex): '{service_name}' in '{query}'")
                    return service_name  # ì¦‰ì‹œ ë°˜í™˜ - ì´ê²Œ í•µì‹¬!
                
                # ë°©ë²• 2: ì¸ë±ìŠ¤ ê¸°ë°˜ ë‹¨ì–´ ê²½ê³„ ì²´í¬ (fallback)
                start_idx = query_lower.find(service_lower)
                if start_idx != -1:
                    end_idx = start_idx + len(service_lower)
                    
                    # ì•ìª½ ê²½ê³„ ì²´í¬
                    is_start_valid = (
                        start_idx == 0 or 
                        query_lower[start_idx - 1] in ' \t\n,.:;!?()[]{}"\'-/'
                    )
                    
                    # ë’·ìª½ ê²½ê³„ ì²´í¬ (í•œê¸€ ì¡°ì‚¬ ê³ ë ¤)
                    is_end_valid = (
                        end_idx == len(query_lower) or 
                        query_lower[end_idx] in ' \t\n,.:;!?()[]{}"\'-/' or
                        self._is_korean_particle(query_lower[end_idx:end_idx+2])  # 2ê¸€ì ì¡°ì‚¬ ì²´í¬
                    )
                    
                    if is_start_valid and is_end_valid:
                        if self.debug_mode:
                            print(f"DEBUG: [SERVICE_FILE] âœ… WORD BOUNDARY MATCH (index): '{service_name}' in '{query}'")
                        return service_name  # ì¦‰ì‹œ ë°˜í™˜
                
                # ë‹¨ì–´ ê²½ê³„ê°€ ì•„ë‹ˆì§€ë§Œ í¬í•¨ëœ ê²½ìš° - ë†’ì€ ì ìˆ˜ë¡œ í›„ë³´ ë“±ë¡
                candidates.append((service_name, 0.95, 'file_service_substring'))
                if self.debug_mode:
                    print(f"DEBUG: [SERVICE_FILE] Service substring match: '{service_name}' in '{query}'")
                continue
            
            # ========================================
            # 3ë‹¨ê³„: ì¿¼ë¦¬ê°€ ì„œë¹„ìŠ¤ëª…ì— í¬í•¨ëœ ê²½ìš°
            # ========================================
            if query_lower in service_lower:
                candidates.append((service_name, 0.90, 'file_query_in_service'))
                if self.debug_mode:
                    print(f"DEBUG: [SERVICE_FILE] Query in service match: '{query}' in '{service_name}'")
                continue
            
            # ========================================
            # 4ë‹¨ê³„: ê³µë°± ë¬´ì‹œí•œ ë§¤ì¹­
            # ========================================
            service_no_space = re.sub(r'\s+', '', service_lower)
            query_no_space = re.sub(r'\s+', '', query_lower)
            
            if service_no_space in query_no_space:
                candidates.append((service_name, 0.85, 'file_no_space_service_in_query'))
                continue
            
            if query_no_space in service_no_space:
                candidates.append((service_name, 0.80, 'file_no_space_query_in_service'))
                continue
            
            # ========================================
            # 5ë‹¨ê³„: í† í° ê¸°ë°˜ ìœ ì‚¬ë„ ë§¤ì¹­ (ë” ì—„ê²©í•œ ê¸°ì¤€)
            # ========================================
            service_tokens = self._extract_service_tokens(service_name)
            if query_tokens and service_tokens:
                similarity = self._calculate_service_similarity(query_tokens, service_tokens)
                if similarity >= 0.7:  # 0.6ì—ì„œ 0.7ë¡œ ìƒí–¥ ì¡°ì •
                    candidates.append((service_name, similarity, 'file_token_similarity'))
                    if self.debug_mode:
                        print(f"DEBUG: [SERVICE_FILE] Token similarity: '{service_name}' (score: {similarity:.2f})")
        
        # ========================================
        # í›„ë³´ ì¤‘ ìµœê³  ì ìˆ˜ ì„ íƒ
        # ========================================
        if candidates:
            candidates.sort(key=lambda x: x[1], reverse=True)
            best_match = candidates[0]
            if self.debug_mode:
                print(f"DEBUG: [SERVICE_FILE] ğŸ¯ Best match: '{best_match[0]}' (score: {best_match[1]:.2f}, method: {best_match[2]})")
            return best_match[0]
        
        if self.debug_mode:
            print(f"DEBUG: [SERVICE_FILE] âŒ No match found in file")
        return None

    def _is_korean_particle(self, text: str) -> bool:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ê°€ í•œê¸€ ì¡°ì‚¬ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
        
        Args:
            text: í™•ì¸í•  í…ìŠ¤íŠ¸ (ìµœì†Œ 1~2ê¸€ì)
        
        Returns:
            bool: ì¡°ì‚¬ë¡œ ì‹œì‘í•˜ë©´ True
        """
        if not text:
            return False
        
        # 1ê¸€ì ì¡°ì‚¬
        single_char_particles = ['ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ë¡œ', 'ëŠ”']
        
        # 2ê¸€ì ì¡°ì‚¬
        double_char_particles = ['ì—ê²Œ', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œì„œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì²˜ëŸ¼', 'ë§Œí¼', 'ë°–ì—']
        
        # 2ê¸€ì ì¡°ì‚¬ ë¨¼ì € ì²´í¬
        if len(text) >= 2 and text[:2] in double_char_particles:
            return True
        
        # 1ê¸€ì ì¡°ì‚¬ ì²´í¬
        if len(text) >= 1 and text[0] in single_char_particles:
            return True
        
        return False

    def semantic_search_with_adaptive_filtering(self, query, target_service_name=None, query_type="default", top_k=50):
        """ë©”ì¸ ê²€ìƒ‰ ì§„ì…ì  - RAG ë°ì´í„° ë¬´ê²°ì„± ì ˆëŒ€ ë³´ì¥"""
        try:
            print(f"DEBUG: Vector hybrid search: '{query}', service: {target_service_name}")
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
            documents = self._execute_vector_hybrid_search(query, target_service_name, query_type, top_k)
            
            if not documents:
                return []
            
            # í†µí•© í•„í„°ë§ ì‹œìŠ¤í…œ ì ìš©
            query_type_enum = self._convert_to_query_type_enum(query_type)
            conditions = self.filter_manager.extract_all_conditions(query, query_type_enum)
            
            # ë²¡í„° ê²€ìƒ‰ ì„¤ì • ì ìš©
            vector_config = self.config.get_vector_search_config(query_type)
            conditions.search_threshold = vector_config.get('vector_similarity_threshold', 0.5)
            
            # ì„œë¹„ìŠ¤ëª… ì •ë³´ ì„¤ì •
            if target_service_name:
                conditions.target_service_name = target_service_name
                conditions.service_name = target_service_name
                conditions.is_common_service = self.is_common_term_service(target_service_name)[0]
            
            # í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ ì ìš©
            filtered_documents, filter_history = self.filter_manager.apply_comprehensive_filtering(
                documents, query, query_type_enum, conditions=conditions
            )
            
            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ fallback
            if len(filtered_documents) == 0 and len(documents) > 0:
                print(f"WARNING: Vector filtering removed all documents! Returning top results")
                sorted_docs = sorted(documents, key=lambda d: d.get('hybrid_score', 0) or 0, reverse=True)
                filtered_documents = sorted_docs[:15]
            
            return filtered_documents
            
        except Exception as e:
            print(f"DEBUG: Vector hybrid search error: {e}")
            return self._fallback_to_original_search(query, target_service_name, query_type, top_k//2)

    def _execute_vector_hybrid_search(self, query, target_service_name, query_type, top_k):
        """ë²¡í„° í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ - RAG ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥"""
        try:
            vector_config = self.config.get_vector_search_config(query_type)
            search_mode = self.config.get_search_mode_for_query(query_type, query)
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_vector = self.embedding_client.get_embedding(query)
            if not query_vector:
                return self._execute_text_only_search(query, target_service_name, query_type, top_k)
            
            # ê²€ìƒ‰ ëª¨ë“œì— ë”°ë¥¸ ì‹¤í–‰
            search_methods = {
                "vector_primary": self._execute_vector_primary_search,
                "text_primary": self._execute_text_primary_search,
                "hybrid_balanced": self._execute_balanced_hybrid_search
            }
            
            search_method = search_methods.get(search_mode, self._execute_balanced_hybrid_search)
            documents = search_method(query, query_vector, target_service_name, vector_config, top_k)
            
            # RRF ìŠ¤ì½”ì–´ë§ ë° ì •ê·œí™” ì ìš©
            documents = self._apply_rrf_scoring_and_normalization(documents, vector_config)
            
            return documents
            
        except Exception as e:
            print(f"ERROR: Vector hybrid search execution failed: {e}")
            return self._fallback_to_original_search(query, target_service_name, query_type, top_k)

    def _execute_balanced_hybrid_search(self, query, query_vector, target_service_name, vector_config, top_k):
        """ê· í˜•ì¡íŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        try:
            enhanced_query = self._build_enhanced_query(query, target_service_name)
            
            vector_queries = [{
                "kind": "vector",
                "vector": query_vector,
                "k_nearest_neighbors": min(self.config.vector_top_k, 50),
                "fields": "contentVector"
            }]
            
            results = self._execute_search_with_params(
                enhanced_query, vector_queries, 
                "semantic" if vector_config.get('use_semantic_reranker', True) else "simple",
                top_k, "any"
            )
            
            return self._process_search_results(results, "Hybrid")
            
        except Exception as e:
            print(f"ERROR: Balanced hybrid search failed: {e}")
            return []
    
    def _execute_vector_primary_search(self, query, query_vector, target_service_name, vector_config, top_k):
        """ë²¡í„° ê²€ìƒ‰ ìš°ì„  ëª¨ë“œ"""
        try:
            vector_queries = [{
                "kind": "vector",
                "vector": query_vector,
                "k_nearest_neighbors": min(top_k * 2, 100),
                "fields": "contentVector"
            }]
            
            basic_query = self._build_basic_query(query, target_service_name)
            
            results = self._execute_search_with_params(
                basic_query if basic_query else "*", vector_queries, "semantic", top_k, "any"
            )
            
            return self._process_search_results(results, "Vector Primary")
            
        except Exception as e:
            print(f"ERROR: Vector primary search failed: {e}")
            return []
    
    def _execute_text_primary_search(self, query, query_vector, target_service_name, vector_config, top_k):
        """í…ìŠ¤íŠ¸ ê²€ìƒ‰ ìš°ì„  ëª¨ë“œ"""
        try:
            enhanced_query = self._build_enhanced_query(query, target_service_name)
            
            vector_queries = [{
                "kind": "vector",
                "vector": query_vector,
                "k_nearest_neighbors": min(top_k // 2, 25),
                "fields": "contentVector"
            }] if query_vector else None
            
            results = self._execute_search_with_params(
                enhanced_query, vector_queries, "simple", top_k, 
                "any" if vector_queries else "all"
            )
            
            return self._process_search_results(results, "Text Primary")
            
        except Exception as e:
            print(f"ERROR: Text primary search failed: {e}")
            return []
    
    def _execute_text_only_search(self, query, target_service_name, query_type, top_k):
        """í…ìŠ¤íŠ¸ ì „ìš© ê²€ìƒ‰"""
        try:
            enhanced_query = self._build_enhanced_query(query, target_service_name)
            
            results = self._execute_search_with_params(
                enhanced_query, None, "semantic", top_k, "all"
            )
            
            return self._process_search_results(results, "Text-only fallback")
            
        except Exception as e:
            print(f"ERROR: Text-only search failed: {e}")
            return []
    
    def _execute_search_with_params(self, search_text, vector_queries, query_type, top_k, search_mode="any"):
        """ê³µí†µ ê²€ìƒ‰ ì‹¤í–‰ ë¡œì§"""
        search_params = {
            "search_text": search_text,
            "top": top_k,
            "search_mode": search_mode,
            "include_total_count": True,
            "select": [
                "incident_id", "service_name", "error_time", "effect", "symptom", 
                "repair_notice", "error_date", "week", "daynight", "root_cause", 
                "incident_repair", "incident_plan", "cause_type", "done_type", 
                "incident_grade", "owner_depart", "year", "month"
            ]
        }
        
        if vector_queries:
            search_params["vector_queries"] = vector_queries
            
        if query_type == "semantic":
            search_params.update({
                "query_type": "semantic",
                "semantic_configuration_name": "iap-incident-semantic-config"
            })
        elif query_type == "simple":
            search_params["query_type"] = "simple"
        
        return self.search_client.search(**search_params)
    
    def _process_search_results(self, results, search_type):
        """ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬"""
        documents = []
        for i, result in enumerate(results):
            if i < 5:  # ë””ë²„ê·¸ìš© ìƒìœ„ 5ê°œ ë¡œê·¸
                print(f"DEBUG: {search_type} Result {i+1}: ID={result.get('incident_id')}, "
                      f"search_score={result.get('@search.score')}, "
                      f"reranker_score={result.get('@search.reranker_score')}")
            
            doc = self._convert_search_result_to_document(result)
            documents.append(doc)
        
        return documents
    
    def _convert_search_result_to_document(self, result):
        """RAG ì›ë³¸ ë°ì´í„° ì ˆëŒ€ ë³´ì¡´ - ë‹¨ì¼ êµ¬í˜„"""
        base_fields = [
            "incident_id", "service_name", "effect", "symptom", "repair_notice",
            "error_date", "week", "daynight", "root_cause", "incident_repair",
            "incident_plan", "cause_type", "done_type", "incident_grade",
            "owner_depart", "year", "month"
        ]
        
        doc = {}
        
        # ê° í•„ë“œë¥¼ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³´ì¡´
        for field in base_fields:
            original_value = result.get(field)
            doc[field] = original_value if original_value is not None else ""
        
        # error_timeì€ ìˆ«ì ë³€í™˜ë§Œ ìˆ˜í–‰
        doc["error_time"] = self._parse_error_time(result.get("error_time", 0))
        
        # ê²€ìƒ‰ ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        doc.update({
            "score": result.get("@search.score") or 0.0,
            "reranker_score": result.get("@search.reranker_score") or 0.0,
            "captions": result.get("@search.captions", []),
            "highlights": result.get("@search.highlights", {}),
            "_data_integrity_preserved": True,
            "_original_search_result": True
        })
        
        return doc

    def _parse_error_time(self, error_time_raw):
        """error_time íŒŒì‹±"""
        try:
            if error_time_raw is None:
                return 0
            if isinstance(error_time_raw, str):
                cleaned = error_time_raw.strip()
                if not cleaned or cleaned.lower() in ['null', 'none', 'n/a', '']:
                    return 0
                return int(float(cleaned))
            return int(error_time_raw)
        except (ValueError, TypeError):
            print(f"WARNING: Failed to parse error_time: {error_time_raw}, using 0")
            return 0
    
    def _apply_rrf_scoring_and_normalization(self, documents, vector_config):
        """RRF ìŠ¤ì½”ì–´ë§ ë° ì •ê·œí™”"""
        if not documents:
            return documents
        
        try:
            for i, doc in enumerate(documents):
                search_score = doc.get('score', 0) or 0
                reranker_score = doc.get('reranker_score', 0) or 0
                
                # RRF ìŠ¤ì½”ì–´ ê³„ì‚°
                rrf_score = 1.0 / (self.rrf_k + i + 1)
                
                # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ìƒì„±
                vector_weight = vector_config.get('vector_weight', 0.5)
                text_weight = vector_config.get('text_weight', 0.5)
                
                # ì •ê·œí™”ëœ ìŠ¤ì½”ì–´ë“¤
                normalized_search = min(search_score, 1.0) if search_score else 0
                normalized_reranker = min(reranker_score / 4.0, 1.0) if reranker_score else 0
                
                # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚°
                hybrid_score = (
                    (normalized_search * text_weight) + 
                    (normalized_reranker * vector_weight) + 
                    (rrf_score * 0.1)
                )
                
                # ìŠ¤ì½”ì–´ ì •ë³´ ì €ì¥
                doc.update({
                    'hybrid_score': hybrid_score,
                    'rrf_score': rrf_score,
                    'normalized_search_score': normalized_search,
                    'normalized_reranker_score': normalized_reranker,
                    'vector_weight_used': vector_weight,
                    'text_weight_used': text_weight,
                    '_scoring_applied': True
                })
            
            # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë¡œ ì¬ì •ë ¬
            documents.sort(key=lambda d: d.get('hybrid_score', 0), reverse=True)
            
            print(f"DEBUG: Applied RRF scoring to {len(documents)} documents")
            return documents
            
        except Exception as e:
            print(f"ERROR: RRF scoring failed: {e}")
            return documents
    
    def _build_enhanced_query(self, query, target_service_name):
        """í–¥ìƒëœ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±"""
        try:
            # ë“±ê¸‰ ì •ë³´ ì¶”ì¶œ
            grade_info = self.extract_incident_grade_from_query(query)
            
            # ì‹œê°„ ì •ë³´ ì¶”ì¶œ
            time_info = self._extract_year_month_from_query_unified(query)
            
            # ì˜ë¯¸ì  í™•ì¥
            expanded_query = self._expand_query_with_semantic_similarity(query)
            
            # ë“±ê¸‰ ì¡°ê±´ ì¶”ê°€
            if grade_info['has_grade_query']:
                expanded_query = self.build_grade_search_query(expanded_query, grade_info)
            
            # ì‹œê°„ ì¡°ê±´ ì¶”ê°€
            enhanced_query = self._add_time_conditions(expanded_query, time_info)
            
            # ì„œë¹„ìŠ¤ëª… ì¡°ê±´
            if target_service_name:
                enhanced_query = self._add_service_conditions(enhanced_query, target_service_name)
            
            return enhanced_query
            
        except Exception as e:
            print(f"ERROR: Enhanced query building failed: {e}")
            return query
    
    def _add_time_conditions(self, query, time_info):
        """ì‹œê°„ ì¡°ê±´ì„ ì¿¼ë¦¬ì— ì¶”ê°€"""
        if not (time_info['year'] or time_info['months']):
            return query
            
        time_conditions = []
        
        if time_info['year']:
            time_conditions.append(f'(year:"{time_info["year"]}" OR error_date:{time_info["year"]}-*)')
        
        if time_info['months']:
            month_conditions = []
            for month_num in time_info['months']:
                month_conditions.append(f'month:"{month_num}"')
                month_str = f"{month_num:02d}"
                if time_info['year']:
                    month_conditions.append(f'error_date:{time_info["year"]}-{month_str}-*')
                else:
                    month_conditions.append(f'error_date:*-{month_str}-*')
            time_conditions.append(f'({" OR ".join(month_conditions)})')
        
        if time_conditions:
            time_filter = " AND ".join(time_conditions)
            return f'({query}) AND {time_filter}' if query.strip() else time_filter
        
        return query
    
    def _add_service_conditions(self, query, target_service_name):
        """ì„œë¹„ìŠ¤ëª… ì¡°ê±´ì„ ì¿¼ë¦¬ì— ì¶”ê°€"""
        is_common = self.is_common_term_service(target_service_name)[0]
        service_query = (f'service_name:"{target_service_name}"' if is_common 
                        else f'(service_name:"{target_service_name}" OR service_name:*{target_service_name}*)')
        return (f'{service_query} AND ({query})' if query != target_service_name 
                else f'{service_query} AND (*)')
    
    def _build_basic_query(self, query, target_service_name):
        """ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±"""
        basic_query = query
        
        if target_service_name:
            is_common = self.is_common_term_service(target_service_name)[0]
            service_query = (f'service_name:"{target_service_name}"' if is_common 
                           else f'service_name:*{target_service_name}*')
            basic_query = f'({service_query}) AND ({query})' if query else service_query
        
        return basic_query

    def _convert_to_query_type_enum(self, query_type_str):
        """ë¬¸ìì—´ ì¿¼ë¦¬ íƒ€ì…ì„ QueryType enumìœ¼ë¡œ ë³€í™˜"""
        mapping = {
            'repair': QueryType.REPAIR,
            'inquiry': QueryType.INQUIRY, 
            'statistics': QueryType.STATISTICS,
            'default': QueryType.DEFAULT
        }
        return mapping.get(query_type_str, QueryType.DEFAULT)
    
    def _fallback_to_original_search(self, query, target_service_name, query_type, top_k):
        """ì›ë˜ ê²€ìƒ‰ ë°©ì‹ìœ¼ë¡œ fallback"""
        print("DEBUG: Falling back to original search method")
        try:
            return self.search_documents_with_service_filter(query, target_service_name, query_type, top_k)
        except:
            return []
    
    def _safe_execute(self, func, default_value=None, error_msg=""):
        """ì•ˆì „í•œ ì‹¤í–‰ í—¬í¼"""
        try:
            return func()
        except Exception as e:
            if error_msg and self.debug_mode:
                print(f"DEBUG: {error_msg}: {e}")
            return default_value
    
    def extract_incident_grade_from_query(self, query):
        """ì¿¼ë¦¬ì—ì„œ ì¥ì• ë“±ê¸‰ ì •ë³´ ì¶”ì¶œ"""
        grade_info = {'has_grade_query': False, 'specific_grade': None, 'grade_keywords': []}
        
        if not query:
            return grade_info
        
        query_lower = query.lower()
        
        # ì¼ë°˜ ë“±ê¸‰ í‚¤ì›Œë“œ
        grade_general_keywords = ['ë“±ê¸‰', 'ì¥ì• ë“±ê¸‰', 'ì „íŒŒë“±ê¸‰', 'grade', 'ì‹¬ê°ë„']
        if any(k in query_lower for k in grade_general_keywords):
            grade_info['has_grade_query'] = True
            grade_info['grade_keywords'].extend([k for k in grade_general_keywords if k in query_lower])
        
        # êµ¬ì²´ì ì¸ ë“±ê¸‰ ì¶”ì¶œ
        grade_patterns = [r'(\d+)ë“±ê¸‰', r'(\d+)ê¸‰', r'(\d+)grade', r'ë“±ê¸‰.*?(\d+)', r'(\d+).*?ë“±ê¸‰']
        for pattern in grade_patterns:
            if matches := re.findall(pattern, query_lower):
                grade_info['specific_grade'] = f"{matches[0]}ë“±ê¸‰"
                grade_info['has_grade_query'] = True
                grade_info['grade_keywords'].append(grade_info['specific_grade'])
                break
        
        return grade_info
    
    def build_grade_search_query(self, query, grade_info):
        """ì¥ì• ë“±ê¸‰ ê¸°ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±"""
        if not grade_info['has_grade_query']:
            return query
        
        if grade_info['specific_grade']:
            grade_query = f'incident_grade:"{grade_info["specific_grade"]}"'
            cleaned_query = query
            for keyword in grade_info['grade_keywords']:
                cleaned_query = cleaned_query.replace(keyword, '').strip()
            return (grade_query if not cleaned_query or len(cleaned_query.strip()) < 2 
                   else f'({grade_query}) AND ({cleaned_query})')
        
        return query
    
    def is_common_term_service(self, service_name):
        """ì¼ë°˜ ìš©ì–´ë¡œ ì‚¬ìš©ë˜ëŠ” ì„œë¹„ìŠ¤ëª…ì¸ì§€ í™•ì¸"""
        if not service_name:
            return False, None
        
        service_lower = service_name.lower().strip()
        for common_service, aliases in self.COMMON_TERM_SERVICES.items():
            if (service_lower == common_service.lower() or 
                service_lower in [a.lower() for a in aliases]):
                return True, common_service
        return False, None
    
    def get_common_term_search_patterns(self, service_name):
        """ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª…ì— ëŒ€í•œ ê²€ìƒ‰ íŒ¨í„´ ìƒì„±"""
        is_common, main_service = self.is_common_term_service(service_name)
        if not is_common:
            return []
        
        patterns = [f'service_name:"{main_service}"']
        aliases = self.COMMON_TERM_SERVICES.get(main_service, [])
        
        # ë³„ì¹­ íŒ¨í„´ ì¶”ê°€
        for alias in aliases:
            patterns.append(f'service_name:"{alias}"')
        
        # í•„ë“œë³„ íŒ¨í„´ ì¶”ê°€
        fields = ['effect', 'symptom', 'root_cause', 'incident_repair', 'repair_notice']
        for term in [main_service] + aliases:
            patterns.extend([f'({field}:"{term}")' for field in fields])
        
        return patterns

    def extract_query_keywords(self, query):
        """ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = {'service_keywords': [], 'symptom_keywords': [], 'action_keywords': [], 'time_keywords': []}
        
        normalized_query = self._normalize_statistics_query(query)
        
        # íŒ¨í„´ ì •ì˜ í†µí•©
        all_patterns = {
            'service_keywords': [
                r'\b(ê´€ë¦¬ì|admin)\s*(ì›¹|web|í˜ì´ì§€|page)', r'\b(API|api)\s*(ë§í¬|link|ì„œë¹„ìŠ¤)',
                r'\b(ERP|erp)\b', r'\b(ë§ˆì´í˜ì´ì§€|mypage)', r'\b(ë³´í—˜|insurance)', r'\b(ì»¤ë®¤ë‹ˆí‹°|community)',
                r'\b(ë¸”ë¡ì²´ì¸|blockchain)', r'\b(OTP|otp|ì¼íšŒìš©ë¹„ë°€ë²ˆí˜¸)\b', r'\b(SMS|sms|ë¬¸ì|ë‹¨ë¬¸)\b',
                r'\b(VPN|vpn|ê°€ìƒì‚¬ì„¤ë§)\b', r'\b(DNS|dns|ë„ë©”ì¸)\b', r'\b(SSL|ssl|https|ë³´ì•ˆ)\b',
                r'\b([A-Z]{2,10})\s*(?:ì„œë¹„ìŠ¤|ì‹œìŠ¤í…œ)?\s*(?:ê±´ìˆ˜|í†µê³„|í˜„í™©|ëª‡|ê°œìˆ˜)',
                r'(?:ê±´ìˆ˜|í†µê³„|í˜„í™©|ëª‡|ê°œìˆ˜)\s*.*?\b([A-Z]{2,10})\b',
            ],
            'symptom_keywords': [
                r'\b(ë¡œê·¸ì¸|login)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨|ì˜¤ë¥˜)', r'\b(ì ‘ì†|ì—°ê²°)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨|ì˜¤ë¥˜)',
                r'\b(ê°€ì…|íšŒì›ê°€ì…)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨)', r'\b(ê²°ì œ|êµ¬ë§¤|ì£¼ë¬¸)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì˜¤ë¥˜)',
                r'\b(ì‘ë‹µ|response)\s*(ì§€ì—°|ëŠ¦ë¦¼|ì—†ìŒ)', r'\b(í˜ì´ì§€|page)\s*(ë¡œë”©|loading)\s*(ë¶ˆê°€|ì‹¤íŒ¨)',
                r'\b(ë¬¸ì|SMS)\s*(ë°œì†¡|ì „ì†¡)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨)', r'\b(ë°œì†¡|ì „ì†¡|ì†¡ì‹ )\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨|ì˜¤ë¥˜)',
                r'\b(OTP|otp|ì¼íšŒìš©ë¹„ë°€ë²ˆí˜¸)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨|ì˜¤ë¥˜|ì§€ì—°)', r'\b(ì¸ì¦|2ì°¨ì¸ì¦|ì´ì¤‘ì¸ì¦)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨|ì˜¤ë¥˜)',
                r'\b(ì¥ì• |ì˜¤ë¥˜|ì—ëŸ¬|ë¬¸ì œ)\s*(?:ê±´ìˆ˜|í†µê³„|í˜„í™©|ëª‡|ê°œìˆ˜)',
                r'(?:ê±´ìˆ˜|í†µê³„|í˜„í™©|ëª‡|ê°œìˆ˜)\s*.*?\b(ì¥ì• |ì˜¤ë¥˜|ì—ëŸ¬|ë¬¸ì œ)\b',
            ],
            'time_keywords': [
                r'\b(202[0-9]|201[0-9])ë…„?\b', r'\b(\d{1,2})ì›”\b',
                r'\b(ì•¼ê°„|ì£¼ê°„|ë°¤|ë‚®|ìƒˆë²½|ì‹¬ì•¼|ì˜¤ì „|ì˜¤í›„)\b',
                r'\b(ì›”ìš”ì¼|í™”ìš”ì¼|ìˆ˜ìš”ì¼|ëª©ìš”ì¼|ê¸ˆìš”ì¼|í† ìš”ì¼|ì¼ìš”ì¼|í‰ì¼|ì£¼ë§)\b',
            ]
        }
        
        # ëª¨ë“  íŒ¨í„´ì— ëŒ€í•´ ë§¤ì¹­ ìˆ˜í–‰
        for key, pattern_list in all_patterns.items():
            for pattern in pattern_list:
                if matches := re.findall(pattern, normalized_query, re.IGNORECASE):
                    keywords[key].extend([m if isinstance(m, str) else ' '.join(m) for m in matches])
        
        return keywords
    
    def calculate_keyword_relevance_score(self, query, document):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        query_keywords = self.extract_query_keywords(query)
        doc_text = ' '.join([document.get(f, '') for f in 
                           ['service_name', 'symptom', 'effect', 'root_cause', 'incident_repair']]).lower()
        
        score = 0
        keyword_weights = [('service_keywords', 40), ('symptom_keywords', 35), 
                          ('action_keywords', 15), ('time_keywords', 10)]
        
        for key, weight in keyword_weights:
            if any(k.lower() in doc_text for k in query_keywords[key]):
                score += weight
        
        return min(score, 100)

    @st.cache_data(ttl=3600)
    def _load_effect_patterns_from_rag(_self):
        """RAG ë°ì´í„°ì—ì„œ effect í•„ë“œì˜ íŒ¨í„´ë“¤ì„ ë¶„ì„í•˜ì—¬ ìºì‹œ"""
        return _self._safe_execute(lambda: {
            keyword: [{
                'original_effect': effect,
                'normalized_effect': _self._normalize_text_for_similarity(effect),
                'symptom': result.get("symptom", "").strip(),
                'service_name': result.get("service_name", "").strip(),
                'keywords': _self._extract_semantic_keywords(effect)
            } for result in _self.search_client.search(
                search_text="*", top=1000,
                select=["effect", "symptom", "service_name"],
                include_total_count=True
            ) if (effect := result.get("effect", "").strip()) 
               and keyword in (keywords := _self._extract_semantic_keywords(effect))]
            for keyword in set().union(*[_self._extract_semantic_keywords(r.get("effect", "")) 
                                       for r in _self.search_client.search(
                search_text="*", top=1000, select=["effect"], include_total_count=True
            ) if r.get("effect", "").strip()])
        }, {})
    
    def _normalize_text_for_similarity(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¹„êµë¥¼ ìœ„í•´ ì •ê·œí™”"""
        if not text:
            return ""
        
        normalized = re.sub(r'\s+', '', text.lower())
        
        for old, new in self.text_replacements.items():
            normalized = normalized.replace(old, new)
        
        return normalized
    
    def _extract_semantic_keywords(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ì  í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not text:
            return []
        
        keyword_patterns = [
            r'(\w+)(ë¶ˆê°€|ì‹¤íŒ¨|ì—ëŸ¬|ì˜¤ë¥˜|ì§€ì—°|ëŠë¦¼)', r'(\w+)(ê°€ì…|ë“±ë¡|ì‹ ì²­)',
            r'(\w+)(ê²°ì œ|êµ¬ë§¤|ì£¼ë¬¸)', r'(\w+)(ì ‘ì†|ì—°ê²°|ë¡œê·¸ì¸)',
            r'(\w+)(ì¡°íšŒ|ê²€ìƒ‰|í™•ì¸)', r'(\w+)(ë°œì†¡|ì „ì†¡|ì†¡ì‹ )',
            r'(ë³´í—˜|ê°€ì…|ê²°ì œ|ì ‘ì†|ë¡œê·¸ì¸|ì¡°íšŒ|ê²€ìƒ‰|ì£¼ë¬¸|êµ¬ë§¤|ë°œì†¡|ì „ì†¡|ë¬¸ì|SMS|OTP|API)(\w*)',
            r'(ì•±|ì›¹|ì‚¬ì´íŠ¸|í˜ì´ì§€|ì‹œìŠ¤í…œ|ì„œë¹„ìŠ¤)(\w*)',
            r'\b(ë³´í—˜|ê°€ì…|ë¶ˆê°€|ì‹¤íŒ¨|ì—ëŸ¬|ì˜¤ë¥˜|ì§€ì—°|ì ‘ì†|ë¡œê·¸ì¸|ê²°ì œ|êµ¬ë§¤|ì£¼ë¬¸|ì¡°íšŒ|ê²€ìƒ‰|ë°œì†¡|ì „ì†¡|ë¬¸ì|SMS|OTP|API)\b'
        ]
        
        keywords = set()
        text_normalized = self._normalize_text_for_similarity(text)
        
        for pattern in keyword_patterns:
            matches = re.findall(pattern, text_normalized, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    keywords.update([m for m in match if m and len(m) >= 2])
                elif match and len(match) >= 2:
                    keywords.add(match)
        
        # ëª…ì‚¬ ì¶”ì¶œ
        nouns = re.findall(r'[ê°€-í£]{2,}', text)
        keywords.update([self._normalize_text_for_similarity(n) for n in nouns if len(n) >= 2])
        
        return list(keywords)
    
    def get_effect_patterns_from_rag(self):
        """RAG ë°ì´í„°ì—ì„œ effect íŒ¨í„´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ í™œìš©)"""
        if not self._effect_cache_loaded:
            self._effect_patterns_cache = self._load_effect_patterns_from_rag()
            self._effect_cache_loaded = True
        return self._effect_patterns_cache or {}
    
    def _expand_query_with_semantic_similarity(self, query):
        """ì¿¼ë¦¬ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í‘œí˜„ë“¤ë¡œ í™•ì¥"""
        effect_patterns = self.get_effect_patterns_from_rag()
        if not effect_patterns:
            return query
        
        query_keywords = self._extract_semantic_keywords(query)
        query_normalized = self._normalize_text_for_similarity(query)
        
        expanded_query_keywords = set(query_keywords)
        
        # ë™ì˜ì–´ í™•ì¥
        synonym_mappings = [
            (['ë¶ˆê°€', 'ì‹¤íŒ¨', 'ì•ˆë¨', 'ì—ëŸ¬', 'ì˜¤ë¥˜'], ['ë¶ˆê°€', 'ì‹¤íŒ¨', 'ì•ˆë¨', 'ì—ëŸ¬', 'ì˜¤ë¥˜', 'ì¥ì• ']),
            (['ë°œì†¡', 'ì „ì†¡', 'ë¬¸ì', 'sms'], ['ë°œì†¡', 'ì „ì†¡', 'ì†¡ì‹ ', 'ë¬¸ì', 'sms']),
        ]
        
        for source_keywords, target_keywords in synonym_mappings:
            if any(k in query.lower() for k in source_keywords):
                expanded_query_keywords.update(target_keywords)
        
        # ê³µí†µ ì„œë¹„ìŠ¤ í™•ì¥
        for common_service in self.COMMON_TERM_SERVICES:
            if common_service.lower() in query.lower():
                expanded_query_keywords.update([common_service] + self.COMMON_TERM_SERVICES[common_service])
        
        similar_effects = set()
        semantic_expansions = set()
        
        for keyword in expanded_query_keywords:
            if keyword in effect_patterns:
                for pattern_info in effect_patterns[keyword]:
                    similarity = self._calculate_text_similarity(query_normalized, pattern_info['normalized_effect'])
                    if similarity > 0.2:
                        similar_effects.add(pattern_info['original_effect'])
                        semantic_expansions.update(pattern_info['keywords'])
        
        if similar_effects or semantic_expansions:
            expanded_terms = [f'({query})']
            
            # ë™ì˜ì–´ ì¶”ê°€
            synonyms = []
            if any(k in query for k in ['ë¶ˆê°€', 'ì‹¤íŒ¨']):
                synonyms.extend(['ë¶ˆê°€', 'ì‹¤íŒ¨', 'ì•ˆë¨', 'ì—ëŸ¬', 'ì˜¤ë¥˜'])
            if any(k in query for k in ['ë°œì†¡', 'ì „ì†¡']):
                synonyms.extend(['ë°œì†¡', 'ì „ì†¡', 'ì†¡ì‹ '])
            
            if synonyms:
                synonym_query = query
                for synonym in synonyms:
                    if synonym not in query:
                        for old in ['ë¶ˆê°€', 'ì‹¤íŒ¨', 'ë°œì†¡', 'ì „ì†¡']:
                            synonym_query = synonym_query.replace(old, synonym)
                        expanded_terms.append(f'({synonym_query})')
            
            # ìœ ì‚¬ íš¨ê³¼ ì¶”ê°€
            for effect in list(similar_effects)[:5]:
                expanded_terms.append(f'(effect:"{effect}")')
            
            # ì˜ë¯¸ì  í™•ì¥ ì¶”ê°€
            if semantic_expansions:
                expanded_terms.append(f'({" OR ".join(list(semantic_expansions)[:10])})')
            
            return ' OR '.join(expanded_terms)
        
        return query
    
    def _calculate_text_similarity(self, text1, text2):
        """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„ ê¸°ë°˜)"""
        if not text1 or not text2:
            return 0
        
        bigrams1 = set([text1[i:i+2] for i in range(len(text1)-1)])
        bigrams2 = set([text2[i:i+2] for i in range(len(text2)-1)])
        
        if not bigrams1 or not bigrams2:
            return 0
        
        intersection = len(bigrams1.intersection(bigrams2))
        union = len(bigrams1.union(bigrams2))
        
        return intersection / union if union > 0 else 0
    
    def _boost_semantic_documents(self, documents, query):
        """ì˜ë¯¸ì  ìœ ì‚¬ì„±ì´ ë†’ì€ ë¬¸ì„œë“¤ì˜ ì ìˆ˜ ë¶€ìŠ¤íŒ…"""
        query_normalized = self._normalize_text_for_similarity(query)
        query_keywords = set(self._extract_semantic_keywords(query))
        
        for doc in documents:
            effect = doc.get('effect', '')
            symptom = doc.get('symptom', '')
            
            max_similarity = 0
            
            # effect ìœ ì‚¬ì„± ê³„ì‚°
            if effect:
                effect_normalized = self._normalize_text_for_similarity(effect)
                effect_similarity = self._calculate_text_similarity(query_normalized, effect_normalized)
                effect_keywords = set(self._extract_semantic_keywords(effect))
                keyword_overlap = len(query_keywords.intersection(effect_keywords))
                effect_similarity += keyword_overlap * 0.1
                max_similarity = max(max_similarity, effect_similarity)
            
            # symptom ìœ ì‚¬ì„± ê³„ì‚°
            if symptom:
                symptom_normalized = self._normalize_text_for_similarity(symptom)
                symptom_similarity = self._calculate_text_similarity(query_normalized, symptom_normalized)
                max_similarity = max(max_similarity, symptom_similarity)
            
            # ë¶€ìŠ¤íŒ… ì ìš©
            if max_similarity > 0.3:
                original_score = doc.get('final_score', doc.get('score', 0))
                doc['final_score'] = original_score * (1 + max_similarity * 0.5)
                doc['semantic_similarity'] = max_similarity
                if 'filter_reason' in doc:
                    doc['filter_reason'] += f" + ì˜ë¯¸ì  ìœ ì‚¬ë„ ë¶€ìŠ¤íŒ… ({max_similarity:.2f})"
        
        return documents

    @st.cache_data(ttl=3600)
    def _load_service_names_from_rag(_self):
        """RAG ë°ì´í„°ì—ì„œ ì‹¤ì œ ì„œë¹„ìŠ¤ëª… ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ ìºì‹œ"""
        return _self._safe_execute(lambda: sorted(list({
            r.get("service_name", "").strip() 
            for r in _self.search_client.search(
                search_text="*", top=1000, select=["service_name"], include_total_count=True
            ) if r.get("service_name", "").strip()
        }), key=len, reverse=True), [])
    
    def get_service_names_from_rag(self):
        """RAG ë°ì´í„°ì—ì„œ ì„œë¹„ìŠ¤ëª… ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ í™œìš©)"""
        if not self._cache_loaded:
            self._service_names_cache = self._load_service_names_from_rag()
            self._cache_loaded = True
        return self._service_names_cache or []
    
    def _normalize_service_name(self, service_name):
        """ì„œë¹„ìŠ¤ëª…ì„ ì •ê·œí™”"""
        if not service_name:
            return ""
        normalized = re.sub(r'[^\w\sê°€-í£]', ' ', service_name)
        return re.sub(r'\s+', ' ', normalized).strip().lower()
    
    def _extract_service_tokens(self, service_name):
        """ì„œë¹„ìŠ¤ëª…ì—ì„œ ì˜ë¯¸ìˆëŠ” í† í°ë“¤ì„ ì¶”ì¶œ"""
        if not service_name:
            return []
        tokens = re.findall(r'[A-Za-zê°€-í£0-9]+', service_name)
        return [t.lower() for t in tokens if len(t) >= 2]
    
    def _calculate_service_similarity(self, query_tokens, service_tokens):
        """í† í° ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… ìœ ì‚¬ë„ ê³„ì‚°"""
        if not query_tokens or not service_tokens:
            return 0.0
        
        query_set = set(query_tokens)
        service_set = set(service_tokens)
        
        intersection = len(query_set.intersection(service_set))
        union = len(query_set.union(service_set))
        
        jaccard_score = intersection / union if union > 0 else 0
        inclusion_score = intersection / len(query_set) if len(query_tokens) > 0 else 0
        
        return jaccard_score * 0.3 + inclusion_score * 0.7
    
    def extract_service_name_from_query(self, query):
        """ê°œì„ ëœ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ - conf/service_names.txt ìš°ì„ ìˆœìœ„ ê°•í™”"""
        if not query:
            return None
            
        print(f"DEBUG: [SERVICE_EXTRACTION] Starting enhanced service name extraction")
        print(f"DEBUG: [SERVICE_EXTRACTION] Input query: '{query}'")
        
        # ì›ì¸ìœ í˜• ì¿¼ë¦¬ì¸ì§€ ë¨¼ì € í™•ì¸
        is_cause_type_query = self._is_cause_type_query(query)
        if is_cause_type_query:
            print(f"DEBUG: [SERVICE_EXTRACTION] Cause type query detected - limiting service extraction")
            return self._extract_service_name_for_cause_type_query(query)
        
        # 1ë‹¨ê³„: ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ ìš°ì„  ì²´í¬ (ë³€ê²½ ì—†ìŒ)
        is_common, common_service = self.is_common_term_service(query)
        if is_common:
            print(f"DEBUG: [SERVICE_EXTRACTION] Common term service found: '{common_service}'")
            return common_service
        
        # â­ 2ë‹¨ê³„: conf/service_names.txt íŒŒì¼ì—ì„œ ìµœìš°ì„  ê²€ìƒ‰ (ê°•í™”)
        print(f"DEBUG: [SERVICE_EXTRACTION] Checking conf/service_names.txt file first...")
        file_service_name = self._find_service_name_in_file(query)
        if file_service_name:
            print(f"DEBUG: [SERVICE_EXTRACTION] âœ… Service found in FILE: '{file_service_name}'")
            return file_service_name
        
        # 3ë‹¨ê³„: í†µê³„ ì¿¼ë¦¬ì— íŠ¹í™”ëœ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ (íŒŒì¼ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš°ë§Œ)
        print(f"DEBUG: [SERVICE_EXTRACTION] File search failed, trying statistics pattern...")
        stats_service_name = self._extract_service_name_for_statistics(query)
        if stats_service_name:
            # í†µê³„ì—ì„œ ì¶”ì¶œëœ ì„œë¹„ìŠ¤ëª…ë„ íŒŒì¼ì— ìˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸
            file_service_names = self.get_service_names_from_file()
            if file_service_names and stats_service_name in file_service_names:
                print(f"DEBUG: [SERVICE_EXTRACTION] Statistics service found in file: '{stats_service_name}'")
                return stats_service_name
            elif not file_service_names:  # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°ë§Œ í†µê³„ ê²°ê³¼ ì‚¬ìš©
                print(f"DEBUG: [SERVICE_EXTRACTION] Statistics service found (no file available): '{stats_service_name}'")
                return stats_service_name
        
        # â­ 4ë‹¨ê³„: Azure AI Search ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ (ìµœì¢… fallback)
        print(f"DEBUG: [SERVICE_EXTRACTION] Fallback to Azure AI Search data...")
        
        normalized_query = self._normalize_statistics_query(query)
        rag_service_names = self.get_service_names_from_rag()
        
        if not rag_service_names:
            print(f"DEBUG: [SERVICE_EXTRACTION] No RAG service names available, using legacy extraction")
            return self._extract_service_name_legacy(normalized_query)
        
        query_lower = normalized_query.lower()
        query_tokens = self._extract_service_tokens(normalized_query)
        
        if not query_tokens:
            print(f"DEBUG: [SERVICE_EXTRACTION] No valid tokens extracted from query")
            return None
        
        candidates = []
        for service_name in rag_service_names:
            service_tokens = self._extract_service_tokens(service_name)
            if not service_tokens:
                continue
            
            # ì •í™•í•œ ë§¤ì¹­
            if service_name.lower() in query_lower:
                candidates.append((service_name, 1.0, 'rag_exact_match'))
                continue
            
            # ì •ê·œí™”ëœ ë§¤ì¹­
            normalized_query_clean = self._normalize_service_name(normalized_query)
            normalized_service = self._normalize_service_name(service_name)
            
            if (normalized_service in normalized_query_clean or 
                normalized_query_clean in normalized_service):
                candidates.append((service_name, 0.9, 'rag_normalized_inclusion'))
                continue
            
            # í† í° ìœ ì‚¬ë„ ë§¤ì¹­ (ë” ì—„ê²©í•œ ê¸°ì¤€)
            similarity = self._calculate_service_similarity(query_tokens, service_tokens)
            if similarity >= 0.5:  # ê¸°ì¡´ 0.4ì—ì„œ 0.5ë¡œ ìƒí–¥ ì¡°ì •
                candidates.append((service_name, similarity, 'rag_token_similarity'))
        
        if not candidates:
            print(f"DEBUG: [SERVICE_EXTRACTION] No candidates found in RAG data")
            return None
        
        candidates.sort(key=lambda x: x[1], reverse=True)
        best_match = candidates[0]
        print(f"DEBUG: [SERVICE_EXTRACTION] âš ï¸  Best RAG match: '{best_match[0]}' (Score: {best_match[1]:.2f})")
        print(f"DEBUG: [SERVICE_EXTRACTION] Note: This service was NOT found in conf/service_names.txt")
        return best_match[0]

    def _is_cause_type_query(self, query):
        """ì›ì¸ìœ í˜• ì¿¼ë¦¬ì¸ì§€ ì—„ê²©í•˜ê²Œ íŒë³„ - í†µê³„/ë¶„ë¥˜ ê´€ë ¨ ì¿¼ë¦¬ë§Œ"""
        if not query:
            return False
        
        query_lower = query.lower()
        
        # 1ë‹¨ê³„: ì›ì¸ìœ í˜•/ë¶„ë¥˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë¬»ëŠ” í‚¤ì›Œë“œ
        explicit_cause_type_keywords = [
            'ì›ì¸ìœ í˜•', 'ì›ì¸ë³„', 'ì›ì¸ìœ í˜•ë³„', 'ì›ì¸íƒ€ì…', 'cause_type',
            'ì›ì¸ë¶„ì„', 'ì›ì¸í˜„í™©', 'ì›ì¸í†µê³„', 'ì›ì¸ë¶„í¬',
            'ì›ì¸ë¶„ë¥˜', 'ì›ì¸ì¢…ë¥˜', 'ì›ì¸ì¹´í…Œê³ ë¦¬'
        ]
        
        for keyword in explicit_cause_type_keywords:
            if keyword in query_lower:
                return True
        
        # 2ë‹¨ê³„: "ì›ì¸" + "í†µê³„/ë¶„ë¥˜" ì¡°í•© ì²´í¬
        has_cause = any(word in query_lower for word in ['ì›ì¸', 'cause'])
        has_stats_or_category = any(word in query_lower for word in 
            ['í†µê³„', 'ë¶„ë¥˜', 'ìœ í˜•', 'íƒ€ì…', 'ë³„', 'í˜„í™©', 'ë¶„í¬', 'ë¶„ì„', 'ì¹´í…Œê³ ë¦¬', 'ì¢…ë¥˜'])
        
        if has_cause and has_stats_or_category:
            # í•˜ì§€ë§Œ "ì›ì¸ì´ ë­ì•¼", "ì›ì¸ ì•Œë ¤ì¤˜" ê°™ì€ ì¼ë°˜ ì§ˆë¬¸ì€ ì œì™¸
            exclude_patterns = [
                r'ì›ì¸[ì´ê°€]?\s*(ë­|ë¬´ì—‡|ì–´ë–»ê²Œ|ì™œ)',
                r'ì›ì¸.*?(ì•Œë ¤|ì„¤ëª…|ë§í•´|ë³´ì—¬)',
                r'(ë­|ë¬´ì—‡|ì–´ë–¤|ì–´ë–»ê²Œ).*?ì›ì¸'
            ]
            
            for pattern in exclude_patterns:
                if re.search(pattern, query_lower):
                    return False
            
            return True
        
        return False

    def _extract_service_name_for_cause_type_query(self, query):
        """ì›ì¸ìœ í˜• ì¿¼ë¦¬ì—ì„œ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ (ë§¤ìš° ì œí•œì )"""
        if not query:
            return None
        
        # ì›ì¸ìœ í˜• ì¿¼ë¦¬ì—ì„œëŠ” ëª…ì‹œì ìœ¼ë¡œ ë”°ì˜´í‘œë‚˜ ê´„í˜¸ë¡œ ê°ì‹¸ì§„ ì„œë¹„ìŠ¤ëª…ë§Œ ì¶”ì¶œ
        service_patterns = [
            r'["\']([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£\s]{2,20})["\']',
            r'\(([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£\s]{2,20})\)',
            r'\b([A-Z]{3,10})\s+(?:ì„œë¹„ìŠ¤|ì‹œìŠ¤í…œ).*?ì›ì¸ìœ í˜•',
            r'ì›ì¸ìœ í˜•.*?\b([A-Z]{3,10})\s+(?:ì„œë¹„ìŠ¤|ì‹œìŠ¤í…œ)'
        ]
        
        # ì œì™¸í•  ì›ì¸ìœ í˜• ê´€ë ¨ í‚¤ì›Œë“œë“¤
        exclude_keywords = [
            'ì›ì¸', 'ì›ì¸ìœ í˜•', 'ì›ì¸ë³„', 'ìœ í˜•', 'íƒ€ì…', 'type', 'ì›ì¸ìœ í˜•ë³„',
            'ì›ì¸íƒ€ì…ë³„', 'ë¬¸ì œì›ì¸', 'ì¥ì• ì›ì¸', 'ë°œìƒì›ì¸', 'ê·¼ë³¸ì›ì¸',
            'ì›ì¸ë¶„ì„', 'ì›ì¸í˜„í™©', 'ì›ì¸í†µê³„', 'ì›ì¸ë¶„í¬', 'ì›ì¸ë¶„ë¥˜'
        ]
        
        for pattern in service_patterns:
            try:
                matches = re.findall(pattern, query, re.IGNORECASE)
                if matches:
                    for match in matches:
                        service_name = match.strip() if isinstance(match, str) else match[0].strip()
                        
                        if (len(service_name) >= 2 and 
                            service_name.lower() not in [k.lower() for k in exclude_keywords] and
                            not service_name.isdigit()):
                            
                            print(f"DEBUG: [CAUSE_TYPE_QUERY] Service name found: '{service_name}'")
                            return service_name
                            
            except Exception as e:
                continue
        
        print(f"DEBUG: [CAUSE_TYPE_QUERY] No service name found in cause type query")
        return None

    def _extract_service_name_for_statistics(self, query):
        """í†µê³„ ì¿¼ë¦¬ì— íŠ¹í™”ëœ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ"""
        if not query:
            return None
        
        # í†µê³„ ì¿¼ë¦¬ì— íŠ¹í™”ëœ íŒ¨í„´ë“¤
        stats_service_patterns = [
            # "ìƒì²´ì¸ì¦í”Œë«í¼ ë…„ë„ë³„ ì¥ì• ê±´ìˆ˜" í˜•íƒœ
            r'^([ê°€-í£]{4,20}(?:í”Œë«í¼|ì‹œìŠ¤í…œ|ì„œë¹„ìŠ¤|í¬í„¸|ì•±|APP|ê´€ë¦¬|ì„¼í„°))\s+(?:ë…„ë„ë³„|ì—°ë„ë³„|ì›”ë³„|ì¥ì• |ê±´ìˆ˜|í†µê³„|í˜„í™©)',
            
            # "ë„¤íŠ¸ì›Œí¬ë³´ì•ˆë²”ìœ„ê´€ë¦¬ ì„œë¹„ìŠ¤ í†µê³„" í˜•íƒœ  
            r'^([ê°€-í£]{4,30})\s+(?:ì„œë¹„ìŠ¤|ì‹œìŠ¤í…œ)?\s*(?:ë…„ë„ë³„|ì—°ë„ë³„|ì›”ë³„|ì¥ì• |ê±´ìˆ˜|í†µê³„|í˜„í™©)',
            
            # "ERP ë…„ë„ë³„" í˜•íƒœ
            r'^([A-Zê°€-í£][A-Za-z0-9ê°€-í£\-_]{1,20})\s+(?:ë…„ë„ë³„|ì—°ë„ë³„|ì›”ë³„|ì¥ì• |ê±´ìˆ˜|í†µê³„|í˜„í™©)',
            
            # ì¤‘ê°„ì— ìˆëŠ” ì„œë¹„ìŠ¤ëª… "ì•Œë ¤ì¤˜ ìƒì²´ì¸ì¦í”Œë«í¼ í†µê³„"
            r'(?:ì•Œë ¤|ë³´ì—¬|í™•ì¸).*?([ê°€-í£]{4,20}(?:í”Œë«í¼|ì‹œìŠ¤í…œ|ì„œë¹„ìŠ¤|í¬í„¸|ì•±|ê´€ë¦¬|ì„¼í„°)).*?(?:ë…„ë„ë³„|ì—°ë„ë³„|ì›”ë³„|ì¥ì• |ê±´ìˆ˜|í†µê³„|í˜„í™©)',
            
            # ë”°ì˜´í‘œë‚˜ íŠ¹ìˆ˜ë¬¸ìë¡œ ê°ì‹¸ì§„ ì„œë¹„ìŠ¤ëª…
            r'["\']([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£\s\-_]{2,30})["\'].*?(?:ë…„ë„ë³„|ì—°ë„ë³„|ì›”ë³„|ì¥ì• |ê±´ìˆ˜|í†µê³„|í˜„í™©)',
            
            # ì„œë¹„ìŠ¤ëª…ì´ ì¿¼ë¦¬ì˜ í•µì‹¬ì¸ ê²½ìš°
            r'\b([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£\s\-_]{3,20})\b.*?(?:ë…„ë„ë³„|ì—°ë„ë³„|ì›”ë³„|ì¥ì• |ê±´ìˆ˜|í†µê³„|í˜„í™©|ëª‡ê±´|ê°œìˆ˜)',
        ]
        
        for pattern in stats_service_patterns:
            try:
                matches = re.findall(pattern, query, re.IGNORECASE)
                if matches:
                    for match in matches:
                        service_name = match[0].strip() if isinstance(match, tuple) else match.strip()
                        
                        # ì œì™¸í•  í†µê³„ ê´€ë ¨ í‚¤ì›Œë“œë“¤
                        exclude_stats_keywords = [
                            'ë…„ë„ë³„', 'ì—°ë„ë³„', 'ì›”ë³„', 'ìš”ì¼ë³„', 'ì‹œê°„ëŒ€ë³„', 'ë¶€ì„œë³„', 'ë“±ê¸‰ë³„',
                            'ì¥ì• ', 'ê±´ìˆ˜', 'í†µê³„', 'í˜„í™©', 'ëª‡', 'ê°œìˆ˜', 'ë°œìƒ', 'ì•Œë ¤', 'ë³´ì—¬',
                            'í™•ì¸', 'ì²´í¬', 'ë¶„ì„', 'ì¡°íšŒ', 'ê²€ìƒ‰', 'ì •ë³´', 'ë°ì´í„°', 'SELECT',
                            'FROM', 'WHERE', 'AND', 'OR', 'ë…„', 'ì›”', 'ì¼'
                        ]
                        
                        if (len(service_name) >= 3 and 
                            service_name not in exclude_stats_keywords and
                            not service_name.isdigit() and
                            not re.match(r'^[0-9]+$', service_name)):
                            
                            return service_name
            except Exception as e:
                continue
        
        return None

    def diagnose_service_name_matching(self, query):
        """ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ ì§„ë‹¨ ë„êµ¬ (ë””ë²„ê¹…ìš©)"""
        print(f"\n=== [DIAGNOSIS] SERVICE NAME MATCHING DIAGNOSIS ===")
        print(f"[DIAGNOSIS] Query: '{query}'")
        
        # 1. íŒŒì¼ ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… í™•ì¸
        file_services = self.get_service_names_from_file()
        print(f"\n[DIAGNOSIS] 1. File-based services ({len(file_services)}):")
        if file_services:
            for i, service in enumerate(file_services[:10]):
                print(f"   {i+1}. {service}")
            if len(file_services) > 10:
                print(f"   ... and {len(file_services) - 10} more")
        else:
            print(f"   No services loaded from conf/service_names.txt")
        
        # 2. RAG ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… í™•ì¸  
        rag_services = self.get_service_names_from_rag()
        print(f"\n[DIAGNOSIS] 2. RAG-based services ({len(rag_services)}):")
        for i, service in enumerate(rag_services[:10]):
            print(f"   {i+1}. {service}")
        if len(rag_services) > 10:
            print(f"   ... and {len(rag_services) - 10} more")
        
        # 3. ë§¤ì¹­ ê²°ê³¼
        final_result = self.extract_service_name_from_query(query)
        print(f"\n[DIAGNOSIS] 3. Final matching result: {final_result}")
        
        # 4. ê²°ê³¼ ìš”ì•½
        print(f"\n[DIAGNOSIS] 4. Summary:")
        print(f"   File services available: {len(file_services) > 0}")
        print(f"   RAG services available: {len(rag_services) > 0}")
        print(f"   Final result: {'SUCCESS' if final_result else 'NO MATCH'}")
        
        if final_result:
            is_in_file = final_result in file_services
            is_in_rag = final_result in rag_services
            print(f"   Source: {'FILE' if is_in_file else 'RAG' if is_in_rag else 'COMMON_TERM'}")
        
        print(f"=== [DIAGNOSIS] END DIAGNOSIS ===\n")
        return {
            'file_services_count': len(file_services),
            'rag_services_count': len(rag_services),
            'final_result': final_result,
            'file_available': len(file_services) > 0,
            'rag_available': len(rag_services) > 0
        }

    def calculate_hybrid_score(self, search_score, reranker_score):
        """ê²€ìƒ‰ ì ìˆ˜ì™€ Reranker ì ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°"""
        search_score = search_score or 0.0
        reranker_score = reranker_score or 0.0
        
        if reranker_score > 0:
            normalized_reranker = min(reranker_score / 4.0, 1.0)
            normalized_search = min(search_score, 1.0)
            return normalized_reranker * 0.8 + normalized_search * 0.2
        
        return min(search_score, 1.0)

    def _extract_year_month_from_query_unified(self, query):
        """í†µí•©ëœ ì—°ë„ì™€ ì›” ì¡°ê±´ ì¶”ì¶œ"""
        time_info = {'year': None, 'months': []}
        if not query:
            return time_info
        
        # ì—°ë„ ì¶”ì¶œ
        year_patterns = [r'\b(\d{4})ë…„\b', r'\b(\d{4})\s*ë…„ë„\b', r'\b(\d{4})ë…„ë„\b']
        for pattern in year_patterns:
            if matches := re.findall(pattern, query, re.IGNORECASE):
                time_info['year'] = matches[-1]
                break
        
        # ëª¨ë“  ì›” ê´€ë ¨ íŒ¨í„´ì„ í†µí•©
        months_set = set()
        
        # ì›” ë²”ìœ„ íŒ¨í„´
        range_patterns = [r'\b(\d+)\s*~\s*(\d+)ì›”\b', r'\b(\d+)ì›”\s*~\s*(\d+)ì›”\b', 
                         r'\b(\d+)\s*-\s*(\d+)ì›”\b', r'\b(\d+)ì›”\s*-\s*(\d+)ì›”\b']
        for pattern in range_patterns:
            if matches := re.findall(pattern, query, re.IGNORECASE):
                for match in matches:
                    start_month, end_month = int(match[0]), int(match[1])
                    if 1 <= start_month <= 12 and 1 <= end_month <= 12 and start_month <= end_month:
                        months_set.update(range(start_month, end_month + 1))
        
        # ê°œë³„ ì›” ë° ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ì›”
        if comma_matches := re.findall(r'(\d{1,2})ì›”', query):
            for match in comma_matches:
                month_num = int(match)
                if 1 <= month_num <= 12:
                    months_set.add(month_num)
        
        time_info['months'] = sorted(list(months_set))
        
        return time_info

    def search_documents_with_service_filter(self, query, target_service_name=None, query_type="default", top_k=15):
        """ì„œë¹„ìŠ¤ëª… í•„í„°ë§ì„ ì§€ì›í•˜ëŠ” ì¼ë°˜ ê²€ìƒ‰ (fallbackìš©)"""
        try:
            # ê¸°ë³¸ ê²€ìƒ‰ ë¡œì§
            grade_info = self.extract_incident_grade_from_query(query)
            time_info = self._extract_year_month_from_query_unified(query)
            
            enhanced_query = (self.build_grade_search_query(query, grade_info) 
                            if grade_info['has_grade_query'] else query)
            
            # ì‹œê°„ ì¡°ê±´ ì¶”ê°€
            enhanced_query = self._add_time_conditions(enhanced_query, time_info)
            
            # ì„œë¹„ìŠ¤ëª… ì¡°ê±´ ì¶”ê°€  
            if target_service_name:
                enhanced_query = self._add_service_conditions(enhanced_query, target_service_name)
            
            results = self.search_client.search(
                search_text=enhanced_query, top=top_k, include_total_count=True,
                select=["incident_id", "service_name", "error_time", "effect", "symptom", "repair_notice",
                       "error_date", "week", "daynight", "root_cause", "incident_repair", "incident_plan",
                       "cause_type", "done_type", "incident_grade", "owner_depart", "year", "month"]
            )
            
            documents = [self._convert_search_result_to_document(result) for result in results]
            
            # í†µí•© í•„í„°ë§ ì‹œìŠ¤í…œ ì ìš©
            query_type_enum = self._convert_to_query_type_enum(query_type)
            filtered_docs, _ = self.filter_manager.apply_comprehensive_filtering(
                documents, query, query_type_enum, enable_llm_validation=False
            )
            
            return filtered_docs
        except:
            return []

    def search_documents_fallback(self, query, target_service_name=None, top_k=25):
        """ë§¤ìš° ê´€ëŒ€í•œ ê¸°ì¤€ì˜ ëŒ€ì²´ ê²€ìƒ‰"""
        try:
            grade_info = self.extract_incident_grade_from_query(query)
            
            search_query = (f'service_name:*{target_service_name}*' if target_service_name 
                          else query)
            
            if grade_info['has_grade_query'] and grade_info['specific_grade']:
                search_query += f' AND incident_grade:"{grade_info["specific_grade"]}"'
            
            results = self.search_client.search(
                search_text=search_query, top=top_k, include_total_count=True,
                select=["incident_id", "service_name", "error_time", "effect", "symptom", "repair_notice",
                       "error_date", "week", "daynight", "root_cause", "incident_repair", "incident_plan",
                       "cause_type", "done_type", "incident_grade", "owner_depart", "year", "month"]
            )
            
            documents = []
            for result in results:
                score = result.get("@search.score") or 0.0
                if score >= 0.05:  # ë§¤ìš° ë‚®ì€ ì„ê³„ê°’
                    doc = self._convert_search_result_to_document(result)
                    doc.update({
                        "final_score": score,
                        "quality_tier": "Basic",
                        "filter_reason": f"ëŒ€ì²´ ê²€ìƒ‰ (ê´€ëŒ€í•œ ê¸°ì¤€, ì ìˆ˜: {score:.2f})",
                        "service_match_type": "fallback"
                    })
                    
                    if grade_info['has_grade_query']:
                        doc_grade = result.get("incident_grade", "")
                        if grade_info['specific_grade'] and doc_grade == grade_info['specific_grade']:
                            doc['grade_match_type'] = 'exact'
                            doc['filter_reason'] += f" (ë“±ê¸‰: {doc_grade})"
                        elif doc_grade:
                            doc['grade_match_type'] = 'general'
                            doc['filter_reason'] += f" (ë“±ê¸‰: {doc_grade})"
                    
                    documents.append(doc)
            
            documents.sort(key=lambda x: x.get('final_score', 0) or 0, reverse=True)
            return documents[:15]
        except:
            return []

    def _extract_service_name_legacy(self, query):
        """ê¸°ì¡´ íŒ¨í„´ ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ (fallback)"""
        service_patterns = [
            r'([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\(\)\s]*[A-Za-z0-9ê°€-í£_\-/\+\)])\s+(?:ì—°ë„ë³„|ì›”ë³„|ê±´ìˆ˜|ì¥ì• |í˜„ìƒ|ë³µêµ¬|ì„œë¹„ìŠ¤|í†µê³„|ë°œìƒ|ë°œìƒì¼ì|ì–¸ì œ)',
            r'ì„œë¹„ìŠ¤.*?([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\(\)\s]*[A-Za-z0-9ê°€-í£_\-/\+\)])',
            r'^([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\(\)\s]*[A-Za-z0-9ê°€-í£_\-/\+\)])\s+(?!ìœ¼ë¡œ|ì—ì„œ|ì—ê²Œ|ì—|ì„|ë¥¼|ì´|ê°€)',
            r'["\']([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\(\)\s]*[A-Za-z0-9ê°€-í£_\-/\+\)])["\']',
            r'\(([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\s]*[A-Za-z0-9ê°€-í£_\-/\+])\)',
            r'\b([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\(\)]{2,}(?:\s+[A-Za-z0-9ê°€-í£_\-/\+\(\)]+)*)\b'
        ]
        
        for pattern in service_patterns:
            if matches := re.findall(pattern, query, re.IGNORECASE):
                for match in matches:
                    if (service_name := match.strip()) and len(service_name) >= 2:
                        return service_name
        return None

    def _normalize_statistics_query(self, query):
        """í†µê³„ ì¿¼ë¦¬ì˜ ë™ì˜ì–´ë“¤ì„ ì •ê·œí™”"""
        if not query:
            return query
        
        normalized = query
        for old_term, new_term in self.statistics_synonyms.items():
            normalized = normalized.replace(old_term, new_term)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        return normalized

    def test_service_name_matching(self):
        """ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ í…ŒìŠ¤íŠ¸"""
        test_queries = [
            "ë„¤íŠ¸ì›Œí¬ë³´ì•ˆë²”ìœ„ê´€ë¦¬ ì„œë¹„ìŠ¤ì—ì„œ ë°œìƒí•œ ì‹¤ëª…ì¸ì¦ ì¥ì•  ë³µêµ¬ë°©ë²•",
            "ERP ë¡œê·¸ì¸ ë¶ˆê°€ ë¬¸ì œ",
            "ë¸”ë¡ì²´ì¸ ì§€ì—­í™”í ì ‘ì† ì¥ì• ",
            "OTP ì¸ì¦ ì‹¤íŒ¨",
            "API ì—°ë™ ì˜¤ë¥˜"
        ]
        
        print("=== SERVICE NAME MATCHING TEST ===")
        for query in test_queries:
            result = self.extract_service_name_from_query(query)
            print(f"Query: {query}")
            print(f"Result: {result}")
            print("-" * 50)

    # ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ wrapper ë©”ì„œë“œë“¤ - filter_managerë¡œ ìœ„ì„
    def filter_documents_by_time_conditions(self, documents, time_conditions):
        """ì‹œê°„ ì¡°ê±´ ê¸°ë°˜ í•„í„°ë§ (í˜¸í™˜ì„± wrapper)"""
        return self.filter_manager.filter_documents_by_time_conditions(documents, time_conditions)
    
    def filter_documents_by_department_conditions(self, documents, department_conditions):
        """ë¶€ì„œ ì¡°ê±´ ê¸°ë°˜ í•„í„°ë§ (í˜¸í™˜ì„± wrapper)"""
        return self.filter_manager.filter_documents_by_department_conditions(documents, department_conditions)
    
    def filter_documents_by_grade(self, documents, grade_info):
        """ì¥ì•  ë“±ê¸‰ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§ (í˜¸í™˜ì„± wrapper)"""
        if not grade_info['has_grade_query']:
            return documents
        
        conditions = FilterConditions()
        conditions.has_grade_query = grade_info['has_grade_query']
        conditions.specific_grade = grade_info.get('specific_grade')
        conditions.grade = grade_info.get('specific_grade')
        
        filtered_docs = []
        for doc in documents:
            is_valid, _ = self.filter_manager.validator.validate_document_conditions(doc, conditions)
            if is_valid:
                doc['grade_match_type'] = 'exact' if conditions.specific_grade else 'general'
                filtered_docs.append(doc)
        
        # ë“±ê¸‰ ìˆœì„œë¡œ ì •ë ¬
        grade_order = {'1ë“±ê¸‰': 1, '2ë“±ê¸‰': 2, '3ë“±ê¸‰': 3, '4ë“±ê¸‰': 4}
        filtered_docs.sort(key=lambda d: next((v for k, v in grade_order.items() 
                                             if k in d.get('incident_grade', '')), 999))
        return filtered_docs