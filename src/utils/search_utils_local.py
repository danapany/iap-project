import streamlit as st
import re
from config.settings_local import AppConfigLocal

class SearchManagerLocal:
    """ê²€ìƒ‰ ê´€ë ¨ ê¸°ëŠ¥ ê´€ë¦¬ í´ë˜ìŠ¤ - ì‹œê°„ëŒ€/ìš”ì¼ ê¸°ë°˜ í•„í„°ë§ ì§€ì› ì¶”ê°€"""
    
    # ì¼ë°˜ì ì¸ ìš©ì–´ë¡œ ì‚¬ìš©ë˜ëŠ” ì„œë¹„ìŠ¤ëª…ë“¤ - í•˜ë“œì½”ë”© ì˜ˆì™¸ì²˜ë¦¬
    COMMON_TERM_SERVICES = {
        'OTP': ['otp', 'ì¼íšŒìš©ë¹„ë°€ë²ˆí˜¸', 'ì›íƒ€ì„íŒ¨ìŠ¤ì›Œë“œ', '2ì°¨ì¸ì¦', 'ì´ì¤‘ì¸ì¦'],           
        'ë³¸ì¸ì¸ì¦': ['ì‹¤ëª…ì¸ì¦', 'ì‹ ì›í™•ì¸'],
        'API': ['api', 'Application Programming Interface', 'REST API', 'APIí˜¸ì¶œ'],
        'SMS': ['sms', 'ë¬¸ì', 'ë‹¨ë¬¸', 'Short Message Service', 'ë¬¸ìë©”ì‹œì§€'],
        'VPN': ['vpn', 'Virtual Private Network', 'ê°€ìƒì‚¬ì„¤ë§'],
        'DNS': ['dns', 'Domain Name System', 'ë„ë©”ì¸ë„¤ì„ì‹œìŠ¤í…œ'],
        'SSL': ['ssl', 'https', 'Secure Sockets Layer', 'ë³´ì•ˆì†Œì¼“ê³„ì¸µ'],
        'URL': ['url', 'link', 'ë§í¬', 'Uniform Resource Locator']
    }
    
    def __init__(self, search_client, config=None):
        self.search_client = search_client
        self.config = config if config else AppConfigLocal()
        self._service_names_cache = None
        self._cache_loaded = False
        # effect íŒ¨í„´ ìºì‹œ
        self._effect_patterns_cache = None
        self._effect_cache_loaded = False
    
    def filter_documents_by_time_conditions(self, documents, time_conditions):
        """ì‹œê°„ ì¡°ê±´ì— ë”°ë¥¸ ë¬¸ì„œ í•„í„°ë§"""
        if not time_conditions or not time_conditions.get('is_time_query'):
            return documents
        
        filtered_docs = []
        filter_stats = {
            'total': len(documents),
            'daynight_filtered': 0,
            'week_filtered': 0,
            'final_count': 0
        }
        
        for doc in documents:
            # ì‹œê°„ëŒ€ í•„í„°ë§
            if time_conditions.get('daynight'):
                doc_daynight = doc.get('daynight', '').strip()
                required_daynight = time_conditions['daynight']
                
                if not doc_daynight or doc_daynight != required_daynight:
                    continue
                filter_stats['daynight_filtered'] += 1
            
            # ìš”ì¼ í•„í„°ë§
            if time_conditions.get('week'):
                doc_week = doc.get('week', '').strip()
                required_week = time_conditions['week']
                
                # í‰ì¼/ì£¼ë§ ì²˜ë¦¬
                if required_week == 'í‰ì¼':
                    if doc_week not in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ']:
                        continue
                elif required_week == 'ì£¼ë§':
                    if doc_week not in ['í† ', 'ì¼']:
                        continue
                else:
                    if not doc_week or doc_week != required_week:
                        continue
                filter_stats['week_filtered'] += 1
            
            filtered_docs.append(doc)
            filter_stats['final_count'] += 1
        
        # í•„í„°ë§ ê²°ê³¼ ë¡œê·¸
        time_desc = []
        if time_conditions.get('daynight'):
            time_desc.append(f"ì‹œê°„ëŒ€: {time_conditions['daynight']}")
        if time_conditions.get('week'):
            time_desc.append(f"ìš”ì¼: {time_conditions['week']}")
        
        st.info(f"""
        â° ì‹œê°„ ì¡°ê±´ í•„í„°ë§ ê²°ê³¼ ({', '.join(time_desc)})
        - ì „ì²´ ê²€ìƒ‰ ê²°ê³¼: {filter_stats['total']}ê±´
        - ì‹œê°„ ì¡°ê±´ ë§¤ì¹­: {filter_stats['final_count']}ê±´
        """)
        
        return filtered_docs
    
    def filter_documents_by_department_conditions(self, documents, department_conditions):
        """ë¶€ì„œ ì¡°ê±´ì— ë”°ë¥¸ ë¬¸ì„œ í•„í„°ë§"""
        if not department_conditions or not department_conditions.get('is_department_query'):
            return documents
        
        filtered_docs = []
        filter_stats = {
            'total': len(documents),
            'department_filtered': 0,
            'final_count': 0
        }
        
        for doc in documents:
            # ë¶€ì„œ í•„í„°ë§
            if department_conditions.get('owner_depart'):
                doc_owner_depart = doc.get('owner_depart', '').strip()
                required_department = department_conditions['owner_depart']
                
                # ë¶€ë¶„ ë§¤ì¹­ë„ í—ˆìš© (ì˜ˆ: "ê°œë°œ" ê²€ìƒ‰ì‹œ "ê°œë°œíŒ€", "ê°œë°œë¶€ì„œ" ë“±ë„ í¬í•¨)
                if not doc_owner_depart or required_department.lower() not in doc_owner_depart.lower():
                    continue
                filter_stats['department_filtered'] += 1
            
            filtered_docs.append(doc)
            filter_stats['final_count'] += 1
        
        # í•„í„°ë§ ê²°ê³¼ ë¡œê·¸
        dept_desc = department_conditions.get('owner_depart', 'í•´ë‹¹ ë¶€ì„œ')
        
        st.info(f"""
        ğŸ¢ ë¶€ì„œ ì¡°ê±´ í•„í„°ë§ ê²°ê³¼ ({dept_desc})
        - ì „ì²´ ê²€ìƒ‰ ê²°ê³¼: {filter_stats['total']}ê±´
        - ë¶€ì„œ ì¡°ê±´ ë§¤ì¹­: {filter_stats['final_count']}ê±´
        """)
        
        return filtered_docs

    def is_common_term_service(self, service_name):
        """ì¼ë°˜ ìš©ì–´ë¡œ ì‚¬ìš©ë˜ëŠ” ì„œë¹„ìŠ¤ëª…ì¸ì§€ í™•ì¸"""
        if not service_name:
            return False
        
        service_lower = service_name.lower().strip()
        
        for common_service, aliases in self.COMMON_TERM_SERVICES.items():
            if service_lower == common_service.lower() or service_lower in [alias.lower() for alias in aliases]:
                return True, common_service
        
        return False, None
    
    def get_common_term_search_patterns(self, service_name):
        """ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª…ì— ëŒ€í•œ ê²€ìƒ‰ íŒ¨í„´ ìƒì„±"""
        is_common, main_service = self.is_common_term_service(service_name)
        
        if not is_common:
            return []
        
        patterns = []
        aliases = self.COMMON_TERM_SERVICES.get(main_service, [])
        
        # ë©”ì¸ ì„œë¹„ìŠ¤ëª…ê³¼ ëª¨ë“  ë³„ì¹­ë“¤ì— ëŒ€í•œ íŒ¨í„´ ìƒì„±
        for term in [main_service] + aliases:
            patterns.extend([
                f'({term})',
                f'(effect:"{term}")',
                f'(symptom:"{term}")',
                f'(root_cause:"{term}")',
                f'(incident_repair:"{term}")',
                f'(repair_notice:"{term}")'
            ])
        
        return patterns

    def extract_query_keywords(self, query):
        """ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ - ê´€ë ¨ì„± ê²€ì¦ìš© (repair/cause ì „ìš©)"""
        keywords = {
            'service_keywords': [],
            'symptom_keywords': [],
            'action_keywords': [],
            'time_keywords': []
        }
        
        # ì„œë¹„ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ - ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª… ì¶”ê°€
        service_patterns = [
            r'\b(ê´€ë¦¬ì|admin)\s*(ì›¹|web|í˜ì´ì§€|page)',
            r'\b(API|api)\s*(ë§í¬|link|ì„œë¹„ìŠ¤)',
            r'\b(ERP|erp)\b',
            r'\b(ë§ˆì´í˜ì´ì§€|mypage)',
            r'\b(ë³´í—˜|insurance)',
            r'\b(ì»¤ë®¤ë‹ˆí‹°|community)',
            r'\b(ë¸”ë¡ì²´ì¸|blockchain)',
            # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª… íŒ¨í„´ ì¶”ê°€
            r'\b(OTP|otp|ì¼íšŒìš©ë¹„ë°€ë²ˆí˜¸)\b',
            r'\b(SMS|sms|ë¬¸ì|ë‹¨ë¬¸)\b',
            r'\b(VPN|vpn|ê°€ìƒì‚¬ì„¤ë§)\b',
            r'\b(DNS|dns|ë„ë©”ì¸)\b',
            r'\b(SSL|ssl|https|ë³´ì•ˆ)\b'
        ]
        
        for pattern in service_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                keywords['service_keywords'].extend([match if isinstance(match, str) else ' '.join(match) for match in matches])
        
        # ì¦ìƒ/í˜„ìƒ ê´€ë ¨ í‚¤ì›Œë“œ - ì˜ë¯¸ì  ë™ì˜ì–´ í™•ì¥
        symptom_patterns = [
            r'\b(ë¡œê·¸ì¸|login)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨|ì˜¤ë¥˜)',
            r'\b(ì ‘ì†|ì—°ê²°)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨|ì˜¤ë¥˜)',
            r'\b(ê°€ì…|íšŒì›ê°€ì…)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨)',
            r'\b(ê²°ì œ|êµ¬ë§¤|ì£¼ë¬¸)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì˜¤ë¥˜)',
            r'\b(ì‘ë‹µ|response)\s*(ì§€ì—°|ëŠë¦¼|ì—†ìŒ)',
            r'\b(í˜ì´ì§€|page)\s*(ë¡œë”©|loading)\s*(ë¶ˆê°€|ì‹¤íŒ¨)',
            r'\b(ë¬¸ì|SMS)\s*(ë°œì†¡|ì „ì†¡)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨)',  # í™•ì¥ëœ íŒ¨í„´
            r'\b(ë°œì†¡|ì „ì†¡|ì†¡ì‹ )\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨|ì˜¤ë¥˜)',     # ì¶”ê°€ íŒ¨í„´
            # OTP ê´€ë ¨ ì¦ìƒ íŒ¨í„´ ì¶”ê°€
            r'\b(OTP|otp|ì¼íšŒìš©ë¹„ë°€ë²ˆí˜¸)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨|ì˜¤ë¥˜|ì§€ì—°)',
            r'\b(ì¸ì¦|2ì°¨ì¸ì¦|ì´ì¤‘ì¸ì¦)\s*(ë¶ˆê°€|ì‹¤íŒ¨|ì•ˆë¨|ì˜¤ë¥˜)'
        ]
        
        for pattern in symptom_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                keywords['symptom_keywords'].extend([match if isinstance(match, str) else ' '.join(match) for match in matches])
        
        # ìš”ì²­ í–‰ë™ ê´€ë ¨ í‚¤ì›Œë“œ
        action_patterns = [
            r'\b(ë³µêµ¬|í•´ê²°|ìˆ˜ë¦¬)(?:ë°©ë²•|ì¡°ì¹˜)',
            r'\b(ì›ì¸|ì´ìœ |cause)',
            r'\b(ìœ ì‚¬|ë¹„ìŠ·|similar)(?:ì‚¬ë¡€|í˜„ìƒ)',
            r'\b(ë‚´ì—­|ì´ë ¥|history)',
            r'\b(ê±´ìˆ˜|ê°œìˆ˜|í†µê³„)'
        ]
        
        for pattern in action_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                keywords['action_keywords'].extend(matches)
        
        # ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ - ì‹œê°„ëŒ€/ìš”ì¼ í‚¤ì›Œë“œ ì¶”ê°€
        time_patterns = [
            r'\b(\d{4})ë…„',
            r'\b(\d{1,2})ì›”',
            r'\b(ì•¼ê°„|ì£¼ê°„|ì˜¤ì „|ì˜¤í›„)',
            r'\b(ì›”ìš”ì¼|í™”ìš”ì¼|ìˆ˜ìš”ì¼|ëª©ìš”ì¼|ê¸ˆìš”ì¼|í† ìš”ì¼|ì¼ìš”ì¼|í‰ì¼|ì£¼ë§)',
            r'\b(ìµœê·¼|recent|ì–´ì œ|ì˜¤ëŠ˜)'
        ]
        
        for pattern in time_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                keywords['time_keywords'].extend(matches)
        
        return keywords
    
    def calculate_keyword_relevance_score(self, query, document):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° - repair/causeìš© ì •í™•ì„± í–¥ìƒ"""
        query_keywords = self.extract_query_keywords(query)
        score = 0
        max_score = 100
        
        # ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¤€ë¹„
        doc_text = f"""
        {document.get('service_name', '')} 
        {document.get('symptom', '')} 
        {document.get('effect', '')} 
        {document.get('root_cause', '')} 
        {document.get('incident_repair', '')}
        """.lower()
        
        # ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ (40ì )
        service_score = 0
        for keyword in query_keywords['service_keywords']:
            if keyword.lower() in doc_text:
                service_score = 40
                break
        score += service_score
        
        # ì¦ìƒ/í˜„ìƒ ë§¤ì¹­ (35ì )
        symptom_score = 0
        for keyword in query_keywords['symptom_keywords']:
            if keyword.lower() in doc_text:
                symptom_score = 35
                break
        score += symptom_score
        
        # ìš”ì²­ í–‰ë™ ë§¤ì¹­ (15ì )
        action_score = 0
        for keyword in query_keywords['action_keywords']:
            if keyword.lower() in doc_text:
                action_score = 15
                break
        score += action_score
        
        # ì‹œê°„ ê´€ë ¨ ë§¤ì¹­ (10ì )
        time_score = 0
        for keyword in query_keywords['time_keywords']:
            if keyword.lower() in doc_text:
                time_score = 10
                break
        score += time_score
        
        return min(score, max_score)

    @st.cache_data(ttl=3600)
    def _load_effect_patterns_from_rag(_self):
        """RAG ë°ì´í„°ì—ì„œ effect í•„ë“œì˜ íŒ¨í„´ë“¤ì„ ë¶„ì„í•˜ì—¬ ìºì‹œ"""
        try:
            results = _self.search_client.search(
                search_text="*",
                top=1000,
                select=["effect", "symptom", "service_name"],
                include_total_count=True
            )
            
            effect_patterns = {}
            
            for result in results:
                effect = result.get("effect", "").strip()
                symptom = result.get("symptom", "").strip()
                service_name = result.get("service_name", "").strip()
                
                if effect:
                    normalized_effect = _self._normalize_text_for_similarity(effect)
                    keywords = _self._extract_semantic_keywords(effect)
                    
                    if keywords:
                        for keyword in keywords:
                            if keyword not in effect_patterns:
                                effect_patterns[keyword] = []
                            effect_patterns[keyword].append({
                                'original_effect': effect,
                                'normalized_effect': normalized_effect,
                                'symptom': symptom,
                                'service_name': service_name,
                                'keywords': keywords
                            })
            
            return effect_patterns
            
        except Exception as e:
            return {}
    
    def _normalize_text_for_similarity(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¹„êµë¥¼ ìœ„í•´ ì •ê·œí™” - ì˜ë¯¸ì  ë™ì˜ì–´ í™•ì¥"""
        if not text:
            return ""
        
        # ë„ì–´ì“°ê¸° ì œê±°
        normalized = re.sub(r'\s+', '', text.lower())
        
        # ì˜ë¯¸ê°€ ê°™ì€ í‘œí˜„ë“¤ì„ í†µì¼ - í™•ì¥ëœ ë™ì˜ì–´ ì‚¬ì „
        replacements = {
            # ë¶ˆê°€/ì‹¤íŒ¨ ê´€ë ¨ í™•ì¥
            'ë¶ˆê°€ëŠ¥': 'ë¶ˆê°€', 'ì‹¤íŒ¨': 'ë¶ˆê°€', 'ì•ˆë¨': 'ë¶ˆê°€', 'ë˜ì§€ì•ŠìŒ': 'ë¶ˆê°€', 
            'í• ìˆ˜ì—†ìŒ': 'ë¶ˆê°€', 'ë¶ˆëŠ¥': 'ë¶ˆê°€', 'ì—ëŸ¬': 'ë¶ˆê°€', 'ì¥ì• ': 'ë¶ˆê°€',
            
            # ì ‘ì†/ì—°ê²° ê´€ë ¨
            'ì ‘ì†': 'ì—°ê²°', 'ë¡œê·¸ì¸': 'ì ‘ì†', 'ì•¡ì„¸ìŠ¤': 'ì ‘ì†', 'ì§„ì…': 'ì ‘ì†',
            
            # ì˜¤ë¥˜/ì—ëŸ¬ ê´€ë ¨
            'ì˜¤ë¥˜': 'ì—ëŸ¬', 'ì¥ì• ': 'ì—ëŸ¬', 'ë¬¸ì œ': 'ì—ëŸ¬', 'ì´ìŠˆ': 'ì—ëŸ¬', 'ë²„ê·¸': 'ì—ëŸ¬',
            
            # ì§€ì—°/ëŠë¦¼ ê´€ë ¨
            'ì§€ì—°': 'ëŠë¦¼', 'ëŠ¦ìŒ': 'ëŠë¦¼', 'ì‘ë‹µì—†ìŒ': 'ëŠë¦¼', 'íƒ€ì„ì•„ì›ƒ': 'ëŠë¦¼',
            
            # ì„œë¹„ìŠ¤/ê¸°ëŠ¥ ê´€ë ¨
            'ì„œë¹„ìŠ¤': 'ê¸°ëŠ¥', 'ì‹œìŠ¤í…œ': 'ì„œë¹„ìŠ¤', 'í”Œë«í¼': 'ì„œë¹„ìŠ¤',
            
            # ê°€ì…/ë“±ë¡ ê´€ë ¨
            'ê°€ì…': 'ë“±ë¡', 'ì‹ ì²­': 'ë“±ë¡', 'íšŒì›ê°€ì…': 'ë“±ë¡', 'íšŒì›ë“±ë¡': 'ë“±ë¡',
            
            # ê²°ì œ/êµ¬ë§¤ ê´€ë ¨
            'ê²°ì œ': 'êµ¬ë§¤', 'êµ¬ë§¤': 'ê²°ì œ', 'ì£¼ë¬¸': 'ê²°ì œ', 'ê±°ë˜': 'ê²°ì œ', 'êµ¬ì…': 'ê²°ì œ',
            
            # ë°œì†¡/ì „ì†¡ ê´€ë ¨ - í™•ì¥
            'ë°œì†¡': 'ì „ì†¡', 'ì†¡ì‹ ': 'ì „ì†¡', 'ì „ë‹¬': 'ì „ì†¡', 'ë³´ë‚´ê¸°': 'ì „ì†¡',
            
            # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª… ì •ê·œí™”
            'otp': 'OTP', 'ì¼íšŒìš©ë¹„ë°€ë²ˆí˜¸': 'OTP', 'ì›íƒ€ì„íŒ¨ìŠ¤ì›Œë“œ': 'OTP',
            'api': 'API', 'sms': 'SMS', 'ë¬¸ì': 'SMS', 'ë‹¨ë¬¸': 'SMS',
            'vpn': 'VPN', 'ê°€ìƒì‚¬ì„¤ë§': 'VPN', 'dns': 'DNS', 'ë„ë©”ì¸': 'DNS'
        }
        
        for old, new in replacements.items():
            normalized = normalized.replace(old, new)
        
        return normalized
    
    def _extract_semantic_keywords(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ì  í‚¤ì›Œë“œ ì¶”ì¶œ - ë°œì†¡/ì „ì†¡ ê´€ë ¨ í™•ì¥"""
        if not text:
            return []
        
        keyword_patterns = [
            # ë™ì‘ + ëŒ€ìƒ íŒ¨í„´ - ë°œì†¡/ì „ì†¡ ê´€ë ¨ ì¶”ê°€
            r'(\w+)(ë¶ˆê°€|ì‹¤íŒ¨|ì—ëŸ¬|ì˜¤ë¥˜|ì§€ì—°|ëŠë¦¼)',
            r'(\w+)(ê°€ì…|ë“±ë¡|ì‹ ì²­)',
            r'(\w+)(ê²°ì œ|êµ¬ë§¤|ì£¼ë¬¸)',
            r'(\w+)(ì ‘ì†|ì—°ê²°|ë¡œê·¸ì¸)',
            r'(\w+)(ì¡°íšŒ|ê²€ìƒ‰|í™•ì¸)',
            r'(\w+)(ë°œì†¡|ì „ì†¡|ì†¡ì‹ )',  # ìƒˆë¡œ ì¶”ê°€
            
            # ëŒ€ìƒ + ìƒíƒœ íŒ¨í„´ - í™•ì¥
            r'(ë³´í—˜|ê°€ì…|ê²°ì œ|ì ‘ì†|ë¡œê·¸ì¸|ì¡°íšŒ|ê²€ìƒ‰|ì£¼ë¬¸|êµ¬ë§¤|ë°œì†¡|ì „ì†¡|ë¬¸ì|SMS|OTP|API)(\w*)',
            
            # ì„œë¹„ìŠ¤ëª… ê´€ë ¨
            r'(ì•±|ì›¹|ì‚¬ì´íŠ¸|í˜ì´ì§€|ì‹œìŠ¤í…œ|ì„œë¹„ìŠ¤)(\w*)',
            
            # ë‹¨ë… ì¤‘ìš” í‚¤ì›Œë“œ - ë°œì†¡ ê´€ë ¨ ì¶”ê°€
            r'\b(ë³´í—˜|ê°€ì…|ë¶ˆê°€|ì‹¤íŒ¨|ì—ëŸ¬|ì˜¤ë¥˜|ì§€ì—°|ì ‘ì†|ë¡œê·¸ì¸|ê²°ì œ|êµ¬ë§¤|ì£¼ë¬¸|ì¡°íšŒ|ê²€ìƒ‰|ë°œì†¡|ì „ì†¡|ë¬¸ì|SMS|OTP|API)\b'
        ]
        
        keywords = set()
        text_normalized = self._normalize_text_for_similarity(text)
        
        for pattern in keyword_patterns:
            matches = re.findall(pattern, text_normalized, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    keywords.update([m for m in match if m and len(m) >= 2])
                elif match and len(match) >= 2:
                    keywords.add(match)
        
        noun_pattern = r'[ê°€-í£]{2,}'
        nouns = re.findall(noun_pattern, text)
        keywords.update([self._normalize_text_for_similarity(noun) for noun in nouns if len(noun) >= 2])
        
        return list(keywords)
    
    def get_effect_patterns_from_rag(self):
        """RAG ë°ì´í„°ì—ì„œ effect íŒ¨í„´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ í™œìš©)"""
        if not self._effect_cache_loaded:
            self._effect_patterns_cache = self._load_effect_patterns_from_rag()
            self._effect_cache_loaded = True
        return self._effect_patterns_cache or {}
    
    def _expand_query_with_semantic_similarity(self, query):
        """ì¿¼ë¦¬ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í‘œí˜„ë“¤ë¡œ í™•ì¥ - ë™ì˜ì–´ í™•ì¥"""
        effect_patterns = self.get_effect_patterns_from_rag()
        
        if not effect_patterns:
            return query
        
        query_keywords = self._extract_semantic_keywords(query)
        query_normalized = self._normalize_text_for_similarity(query)
        
        similar_effects = set()
        semantic_expansions = set()
        
        # ë™ì˜ì–´ í™•ì¥ì„ ìœ„í•œ ì¶”ê°€ ì²˜ë¦¬
        expanded_query_keywords = set(query_keywords)
        
        # ë¶ˆê°€/ì‹¤íŒ¨ ë™ì˜ì–´ í™•ì¥
        if any(keyword in query.lower() for keyword in ['ë¶ˆê°€', 'ì‹¤íŒ¨', 'ì•ˆë¨', 'ì—ëŸ¬', 'ì˜¤ë¥˜']):
            expanded_query_keywords.update(['ë¶ˆê°€', 'ì‹¤íŒ¨', 'ì•ˆë¨', 'ì—ëŸ¬', 'ì˜¤ë¥˜', 'ì¥ì• '])
        
        # ë°œì†¡/ì „ì†¡ ë™ì˜ì–´ í™•ì¥
        if any(keyword in query.lower() for keyword in ['ë°œì†¡', 'ì „ì†¡', 'ë¬¸ì', 'sms']):
            expanded_query_keywords.update(['ë°œì†¡', 'ì „ì†¡', 'ì†¡ì‹ ', 'ë¬¸ì', 'sms'])
        
        # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª… í™•ì¥
        for common_service in self.COMMON_TERM_SERVICES.keys():
            if common_service.lower() in query.lower():
                aliases = self.COMMON_TERM_SERVICES[common_service]
                expanded_query_keywords.update([common_service] + aliases)
        
        for keyword in expanded_query_keywords:
            if keyword in effect_patterns:
                for pattern_info in effect_patterns[keyword]:
                    similarity = self._calculate_text_similarity(
                        query_normalized, 
                        pattern_info['normalized_effect']
                    )
                    
                    if similarity > 0.2:  # ì„ê³„ê°’ì„ ë‚®ì¶°ì„œ ë” í¬ê´„ì ìœ¼ë¡œ
                        similar_effects.add(pattern_info['original_effect'])
                        semantic_expansions.update(pattern_info['keywords'])
        
        if similar_effects or semantic_expansions:
            expanded_terms = []
            expanded_terms.append(f'({query})')
            
            # ë™ì˜ì–´ ê¸°ë°˜ í™•ì¥ ì¿¼ë¦¬ ì¶”ê°€
            synonyms = []
            if 'ë¶ˆê°€' in query or 'ì‹¤íŒ¨' in query:
                synonyms.extend(['ë¶ˆê°€', 'ì‹¤íŒ¨', 'ì•ˆë¨', 'ì—ëŸ¬', 'ì˜¤ë¥˜'])
            if 'ë°œì†¡' in query or 'ì „ì†¡' in query:
                synonyms.extend(['ë°œì†¡', 'ì „ì†¡', 'ì†¡ì‹ '])
            
            if synonyms:
                synonym_query = query
                for synonym in synonyms:
                    if synonym not in query:
                        synonym_query = synonym_query.replace('ë¶ˆê°€', synonym).replace('ì‹¤íŒ¨', synonym).replace('ë°œì†¡', synonym).replace('ì „ì†¡', synonym)
                        expanded_terms.append(f'({synonym_query})')
            
            for effect in list(similar_effects)[:5]:
                expanded_terms.append(f'(effect:"{effect}")')
            
            if semantic_expansions:
                semantic_query_parts = []
                for expansion in list(semantic_expansions)[:10]:
                    semantic_query_parts.append(expansion)
                if semantic_query_parts:
                    expanded_terms.append(f'({" OR ".join(semantic_query_parts)})')
            
            expanded_query = ' OR '.join(expanded_terms)
            return expanded_query
        
        return query
    
    def _calculate_text_similarity(self, text1, text2):
        """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„ ê¸°ë°˜)"""
        if not text1 or not text2:
            return 0
        
        def get_bigrams(text):
            return set([text[i:i+2] for i in range(len(text)-1)])
        
        bigrams1 = get_bigrams(text1)
        bigrams2 = get_bigrams(text2)
        
        if not bigrams1 or not bigrams2:
            return 0
        
        intersection = len(bigrams1.intersection(bigrams2))
        union = len(bigrams1.union(bigrams2))
        
        return intersection / union if union > 0 else 0
    
    def _boost_semantic_documents(self, documents, query):
        """ì˜ë¯¸ì  ìœ ì‚¬ì„±ì´ ë†’ì€ ë¬¸ì„œë“¤ì˜ ì ìˆ˜ ë¶€ìŠ¤íŒ…"""
        query_normalized = self._normalize_text_for_similarity(query)
        query_keywords = set(self._extract_semantic_keywords(query))
        
        for doc in documents:
            effect = doc.get('effect', '')
            symptom = doc.get('symptom', '')
            
            effect_similarity = 0
            symptom_similarity = 0
            
            if effect:
                effect_normalized = self._normalize_text_for_similarity(effect)
                effect_similarity = self._calculate_text_similarity(query_normalized, effect_normalized)
                
                effect_keywords = set(self._extract_semantic_keywords(effect))
                keyword_overlap = len(query_keywords.intersection(effect_keywords))
                if keyword_overlap > 0:
                    effect_similarity += (keyword_overlap * 0.1)
            
            if symptom:
                symptom_normalized = self._normalize_text_for_similarity(symptom)
                symptom_similarity = self._calculate_text_similarity(query_normalized, symptom_normalized)
            
            max_similarity = max(effect_similarity, symptom_similarity)
            
            if max_similarity > 0.3:
                original_score = doc.get('final_score', doc.get('score', 0))
                boost_factor = 1 + (max_similarity * 0.5)
                doc['final_score'] = original_score * boost_factor
                doc['semantic_similarity'] = max_similarity
                
                if 'filter_reason' in doc:
                    doc['filter_reason'] += f" + ì˜ë¯¸ì  ìœ ì‚¬ë„ ë¶€ìŠ¤íŒ… ({max_similarity:.2f})"
        
        return documents

    @st.cache_data(ttl=3600)
    def _load_service_names_from_rag(_self):
        """RAG ë°ì´í„°ì—ì„œ ì‹¤ì œ ì„œë¹„ìŠ¤ëª… ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ ìºì‹œ"""
        try:
            results = _self.search_client.search(
                search_text="*",
                top=1000,
                select=["service_name"],
                include_total_count=True
            )
            
            service_names = set()
            for result in results:
                service_name = result.get("service_name", "").strip()
                if service_name:
                    service_names.add(service_name)
            
            sorted_service_names = sorted(list(service_names), key=len, reverse=True)
            return sorted_service_names
            
        except Exception as e:
            st.warning(f"RAG ë°ì´í„°ì—ì„œ ì„œë¹„ìŠ¤ëª… ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_service_names_from_rag(self):
        """RAG ë°ì´í„°ì—ì„œ ì„œë¹„ìŠ¤ëª… ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ í™œìš©)"""
        if not self._cache_loaded:
            self._service_names_cache = self._load_service_names_from_rag()
            self._cache_loaded = True
        return self._service_names_cache or []
    
    def _normalize_service_name(self, service_name):
        """ì„œë¹„ìŠ¤ëª…ì„ ì •ê·œí™” - íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì²˜ë¦¬"""
        if not service_name:
            return ""
        
        # íŠ¹ìˆ˜ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜ í›„ ì¤‘ë³µ ê³µë°± ì œê±°
        normalized = re.sub(r'[^\w\sê°€-í£]', ' ', service_name)
        normalized = re.sub(r'\s+', ' ', normalized).strip().lower()
        
        return normalized
    
    def _extract_service_tokens(self, service_name):
        """ì„œë¹„ìŠ¤ëª…ì—ì„œ ì˜ë¯¸ìˆëŠ” í† í°ë“¤ì„ ì¶”ì¶œ"""
        if not service_name:
            return []
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ê³  ë‹¨ì–´ í† í°ìœ¼ë¡œ ë¶„ë¦¬
        tokens = re.findall(r'[A-Za-zê°€-í£0-9]+', service_name)
        # ê¸¸ì´ 2 ì´ìƒì¸ í† í°ë§Œ ìœ íš¨í•˜ë‹¤ê³  íŒë‹¨
        valid_tokens = [token.lower() for token in tokens if len(token) >= 2]
        
        return valid_tokens
    
    def _calculate_service_similarity(self, query_tokens, service_tokens):
        """í† í° ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… ìœ ì‚¬ë„ ê³„ì‚°"""
        if not query_tokens or not service_tokens:
            return 0.0
        
        query_set = set(query_tokens)
        service_set = set(service_tokens)
        
        # Jaccard ìœ ì‚¬ë„
        intersection = len(query_set.intersection(service_set))
        union = len(query_set.union(service_set))
        
        jaccard_score = intersection / union if union > 0 else 0
        
        # í¬í•¨ ë¹„ìœ¨ (ì¿¼ë¦¬ í† í°ì´ ì„œë¹„ìŠ¤ í† í°ì— ì–¼ë§ˆë‚˜ í¬í•¨ë˜ëŠ”ì§€)
        inclusion_score = intersection / len(query_set) if len(query_set) > 0 else 0
        
        # ë‘ ì ìˆ˜ì˜ ê°€ì¤‘ í‰ê·  (í¬í•¨ ë¹„ìœ¨ì„ ë” ì¤‘ì‹œ)
        final_score = (jaccard_score * 0.3) + (inclusion_score * 0.7)
        
        return final_score
    
    def extract_service_name_from_query(self, query):
        """ê°œì„ ëœ RAG ë°ì´í„° ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ - í† í° ê¸°ë°˜ ìœ ì‚¬ë„ ë§¤ì¹­ + ì¼ë°˜ìš©ì–´ ì˜ˆì™¸ì²˜ë¦¬"""
        # 1ë‹¨ê³„: ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª… í™•ì¸
        is_common, common_service = self.is_common_term_service(query)
        if is_common:
            st.info(f"ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª… ê°ì§€: **{common_service}** (ì „ì²´ í•„ë“œ ê²€ìƒ‰ ëª¨ë“œ)")
            return common_service
        
        rag_service_names = self.get_service_names_from_rag()
        
        if not rag_service_names:
            return self._extract_service_name_legacy(query)
        
        query_lower = query.lower()
        query_tokens = self._extract_service_tokens(query)
        
        if not query_tokens:
            return None
        
        # í›„ë³´ ì„œë¹„ìŠ¤ëª…ê³¼ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        candidates = []
        
        for service_name in rag_service_names:
            service_tokens = self._extract_service_tokens(service_name)
            
            if not service_tokens:
                continue
            
            # 1ë‹¨ê³„: ì™„ì „ ì¼ì¹˜ ê²€ìƒ‰ (ìµœê³  ìš°ì„ ìˆœìœ„)
            if service_name.lower() in query_lower:
                candidates.append((service_name, 1.0, 'exact_match'))
                continue
            
            # 2ë‹¨ê³„: ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ í¬í•¨ ê´€ê³„
            normalized_query = self._normalize_service_name(query)
            normalized_service = self._normalize_service_name(service_name)
            
            if normalized_service in normalized_query or normalized_query in normalized_service:
                candidates.append((service_name, 0.9, 'normalized_inclusion'))
                continue
            
            # 3ë‹¨ê³„: í† í° ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = self._calculate_service_similarity(query_tokens, service_tokens)
            
            if similarity >= 0.5:  # 50% ì´ìƒ ìœ ì‚¬ë„
                candidates.append((service_name, similarity, 'token_similarity'))
        
        # í›„ë³´ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜
        if not candidates:
            return None
        
        # ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
        candidates.sort(key=lambda x: x[1], reverse=True)
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì„œë¹„ìŠ¤ëª… ë°˜í™˜
        best_match = candidates[0]
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì •ë³´ ì¶œë ¥ (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
        if len(candidates) > 1:
            st.info(f"ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ ê²°ê³¼: '{best_match[0]}' (ìœ ì‚¬ë„: {best_match[1]:.2f}, ë°©ì‹: {best_match[2]})")
        
        return best_match[0]

    def calculate_hybrid_score(self, search_score, reranker_score):
        """ê²€ìƒ‰ ì ìˆ˜ì™€ Reranker ì ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° - None ê°’ ì²˜ë¦¬"""
        # None ê°’ ì²˜ë¦¬
        search_score = search_score if search_score is not None else 0.0
        reranker_score = reranker_score if reranker_score is not None else 0.0
        
        if reranker_score > 0:
            normalized_reranker = min(reranker_score / 4.0, 1.0)
            normalized_search = min(search_score, 1.0)
            hybrid_score = (normalized_reranker * 0.8) + (normalized_search * 0.2)
        else:
            hybrid_score = min(search_score, 1.0)
        
        return hybrid_score

    def advanced_filter_documents_for_accuracy(self, documents, query_type="default", query_text="", target_service_name=None):
        """ì •í™•ì„± ìš°ì„  í•„í„°ë§ - repair/causeìš© - None ê°’ ì²˜ë¦¬ ê°•í™” + ì¼ë°˜ìš©ì–´ ì„œë¹„ìŠ¤ëª… ì˜ˆì™¸ì²˜ë¦¬"""
        
        thresholds = self.config.get_dynamic_thresholds(query_type, query_text)
        documents = self._boost_semantic_documents(documents, query_text)
        
        # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª…ì¸ì§€ í™•ì¸
        is_common_service = False
        if target_service_name:
            is_common, _ = self.is_common_term_service(target_service_name)
            is_common_service = is_common
        
        filtered_docs = []
        filter_stats = {
            'total': len(documents),
            'search_filtered': 0,
            'service_exact_match': 0,
            'service_partial_match': 0,
            'service_filtered': 0,
            'reranker_qualified': 0,
            'hybrid_qualified': 0,
            'semantic_boosted': 0,
            'keyword_relevant': 0,
            'final_selected': 0,
            'common_term_matches': 0
        }
        
        for doc in documents:
            # None ê°’ ì²˜ë¦¬
            search_score = doc.get('score', 0) if doc.get('score') is not None else 0.0
            reranker_score = doc.get('reranker_score', 0) if doc.get('reranker_score') is not None else 0.0
            
            if 'semantic_similarity' in doc:
                filter_stats['semantic_boosted'] += 1
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            keyword_relevance = self.calculate_keyword_relevance_score(query_text, doc)
            if keyword_relevance >= 30:
                filter_stats['keyword_relevant'] += 1
                doc['keyword_relevance_score'] = keyword_relevance
            
            # ê¸°ë³¸ ê²€ìƒ‰ ì ìˆ˜ í•„í„°ë§
            if search_score < thresholds['search_threshold']:
                continue
            filter_stats['search_filtered'] += 1
            
            # ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ - ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª… ì˜ˆì™¸ì²˜ë¦¬
            if target_service_name:
                doc_service_name = doc.get('service_name', '').strip()
                
                if is_common_service:
                    # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª…ì¸ ê²½ìš°: ëª¨ë“  í•„ë“œì—ì„œ ê²€ìƒ‰
                    all_fields_text = f"""
                    {doc.get('service_name', '')} {doc.get('symptom', '')} 
                    {doc.get('effect', '')} {doc.get('root_cause', '')} 
                    {doc.get('incident_repair', '')} {doc.get('repair_notice', '')}
                    """.lower()
                    
                    aliases = self.COMMON_TERM_SERVICES.get(target_service_name, [])
                    search_terms = [target_service_name.lower()] + [alias.lower() for alias in aliases]
                    
                    if any(term in all_fields_text for term in search_terms):
                        filter_stats['common_term_matches'] += 1
                        doc['service_match_type'] = 'common_term'
                    else:
                        continue
                else:
                    # ì¼ë°˜ì ì¸ ì„œë¹„ìŠ¤ëª… ë§¤ì¹­
                    if doc_service_name.lower() == target_service_name.lower():
                        filter_stats['service_exact_match'] += 1
                        doc['service_match_type'] = 'exact'
                    elif target_service_name.lower() in doc_service_name.lower() or doc_service_name.lower() in target_service_name.lower():
                        filter_stats['service_partial_match'] += 1
                        doc['service_match_type'] = 'partial'
                    else:
                        continue
            else:
                doc['service_match_type'] = 'all'
                
            filter_stats['service_filtered'] += 1
            
            # Reranker ì ìˆ˜ ìš°ì„  í‰ê°€
            if reranker_score >= thresholds['reranker_threshold']:
                filter_stats['reranker_qualified'] += 1
                match_type = doc.get('service_match_type', 'unknown')
                relevance_info = f" (í‚¤ì›Œë“œ ê´€ë ¨ì„±: {keyword_relevance}ì )" if keyword_relevance >= 30 else ""
                match_desc = "ì¼ë°˜ìš©ì–´ ì „ì²´í•„ë“œ" if match_type == 'common_term' else match_type
                doc['filter_reason'] = f"ì •í™•ì„± ìš°ì„  - {match_desc} ë§¤ì¹­ + Reranker ê³ í’ˆì§ˆ (ì ìˆ˜: {reranker_score:.2f}){relevance_info}"
                doc['final_score'] = reranker_score
                doc['quality_tier'] = 'Premium'
                filtered_docs.append(doc)
                filter_stats['final_selected'] += 1
                continue
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ í‰ê°€
            hybrid_score = self.calculate_hybrid_score(search_score, reranker_score)
            final_score = doc.get('final_score', hybrid_score)
            
            # final_scoreë„ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬
            final_score = final_score if final_score is not None else 0.0
            
            if final_score >= thresholds['hybrid_threshold']:
                filter_stats['hybrid_qualified'] += 1
                match_type = doc.get('service_match_type', 'unknown')
                relevance_info = f" (í‚¤ì›Œë“œ ê´€ë ¨ì„±: {keyword_relevance}ì )" if keyword_relevance >= 30 else ""
                match_desc = "ì¼ë°˜ìš©ì–´ ì „ì²´í•„ë“œ" if match_type == 'common_term' else match_type
                doc['filter_reason'] = f"ì •í™•ì„± ìš°ì„  - {match_desc} ë§¤ì¹­ + í•˜ì´ë¸Œë¦¬ë“œ í†µê³¼ (ì ìˆ˜: {final_score:.2f}){relevance_info}"
                doc['quality_tier'] = 'Standard'
                filtered_docs.append(doc)
                filter_stats['final_selected'] += 1
        
        # ì •í™•ì„± ìš°ì„  ì •ë ¬
        def sort_key(doc):
            match_priority = {'exact': 3, 'partial': 2, 'common_term': 2.5, 'all': 1}
            semantic_boost = doc.get('semantic_similarity', 0) or 0
            keyword_boost = doc.get('keyword_relevance_score', 0) or 0
            final_score = doc.get('final_score', 0) or 0
            
            return (
                match_priority.get(doc.get('service_match_type', 'all'), 0), 
                final_score + (semantic_boost * 0.1) + (keyword_boost * 0.001)
            )
        
        filtered_docs.sort(key=sort_key, reverse=True)
        final_docs = filtered_docs[:thresholds['max_results']]
       
        common_term_info = f"\n        - ì¼ë°˜ìš©ì–´ ì „ì²´í•„ë“œ ë§¤ì¹­: {filter_stats['common_term_matches']}ê°œ" if is_common_service else ""
        
        st.info(f"""
        ì •í™•ì„± ìš°ì„  í•„í„°ë§ ê²°ê³¼ (repair/cause ìµœì í™”)
        - ì „ì²´ ê²€ìƒ‰ ê²°ê³¼: {filter_stats['total']}ê°œ
        - ê¸°ë³¸ ì ìˆ˜ í†µê³¼: {filter_stats['search_filtered']}ê°œ
        - ì´ ì„œë¹„ìŠ¤ëª… ë§¤ì¹­: {filter_stats['service_filtered']}ê°œ{common_term_info}
        - Reranker ê³ í’ˆì§ˆ: {filter_stats['reranker_qualified']}ê°œ
        - í•˜ì´ë¸Œë¦¬ë“œ í†µê³¼: {filter_stats['hybrid_qualified']}ê°œ
        - ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¶€ìŠ¤íŒ…: {filter_stats['semantic_boosted']}ê°œ
        - í‚¤ì›Œë“œ ê´€ë ¨ì„± í™•ë³´: {filter_stats['keyword_relevant']}ê°œ
        - ìµœì¢… ì„ ë³„: {len(final_docs)}ê°œ
        """)
        
        return final_docs

    def simple_filter_documents_for_coverage(self, documents, query_type="default", query_text="", target_service_name=None):
        """í¬ê´„ì„± ìš°ì„  í•„í„°ë§ - similar/defaultìš© - None ê°’ ì²˜ë¦¬ ê°•í™” + ì¼ë°˜ìš©ì–´ ì„œë¹„ìŠ¤ëª… ì˜ˆì™¸ì²˜ë¦¬"""
        
        thresholds = self.config.get_dynamic_thresholds(query_type, query_text)
        documents = self._boost_semantic_documents(documents, query_text)
        
        # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª…ì¸ì§€ í™•ì¸
        is_common_service = False
        if target_service_name:
            is_common, _ = self.is_common_term_service(target_service_name)
            is_common_service = is_common
        
        filtered_docs = []
        filter_stats = {
            'total': len(documents),
            'search_filtered': 0,
            'service_filtered': 0,
            'reranker_qualified': 0,
            'hybrid_qualified': 0,
            'semantic_boosted': 0,
            'final_selected': 0,
            'common_term_matches': 0
        }
        
        for doc in documents:
            # None ê°’ ì²˜ë¦¬
            search_score = doc.get('score', 0) if doc.get('score') is not None else 0.0
            reranker_score = doc.get('reranker_score', 0) if doc.get('reranker_score') is not None else 0.0
            
            if 'semantic_similarity' in doc:
                filter_stats['semantic_boosted'] += 1
            
            # ê¸°ë³¸ ê²€ìƒ‰ ì ìˆ˜ í•„í„°ë§
            if search_score < thresholds['search_threshold']:
                continue
            filter_stats['search_filtered'] += 1
            
            # ì„œë¹„ìŠ¤ëª… ë§¤ì¹­ (ê°„ì†Œí™”) - ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª… ì˜ˆì™¸ì²˜ë¦¬
            if target_service_name:
                doc_service_name = doc.get('service_name', '').strip()
                
                if is_common_service:
                    # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª…ì¸ ê²½ìš°: ëª¨ë“  í•„ë“œì—ì„œ ê²€ìƒ‰
                    all_fields_text = f"""
                    {doc.get('service_name', '')} {doc.get('symptom', '')} 
                    {doc.get('effect', '')} {doc.get('root_cause', '')} 
                    {doc.get('incident_repair', '')} {doc.get('repair_notice', '')}
                    """.lower()
                    
                    aliases = self.COMMON_TERM_SERVICES.get(target_service_name, [])
                    search_terms = [target_service_name.lower()] + [alias.lower() for alias in aliases]
                    
                    if any(term in all_fields_text for term in search_terms):
                        filter_stats['common_term_matches'] += 1
                        doc['service_match_type'] = 'common_term'
                    else:
                        continue
                else:
                    # ì¼ë°˜ì ì¸ ì„œë¹„ìŠ¤ëª… ë§¤ì¹­
                    if doc_service_name.lower() == target_service_name.lower():
                        doc['service_match_type'] = 'exact'
                    elif target_service_name.lower() in doc_service_name.lower() or doc_service_name.lower() in target_service_name.lower():
                        doc['service_match_type'] = 'partial'
                    else:
                        continue
            else:
                doc['service_match_type'] = 'all'
                
            filter_stats['service_filtered'] += 1
            
            # Reranker ì ìˆ˜ ìš°ì„  í‰ê°€
            if reranker_score >= thresholds['reranker_threshold']:
                filter_stats['reranker_qualified'] += 1
                match_type = doc.get('service_match_type', 'unknown')
                match_desc = "ì¼ë°˜ìš©ì–´ ì „ì²´í•„ë“œ" if match_type == 'common_term' else match_type
                doc['filter_reason'] = f"í¬ê´„ì„± ìš°ì„  - {match_desc} ë§¤ì¹­ + Reranker ê³ í’ˆì§ˆ (ì ìˆ˜: {reranker_score:.2f})"
                doc['final_score'] = reranker_score
                doc['quality_tier'] = 'Premium'
                filtered_docs.append(doc)
                filter_stats['final_selected'] += 1
                continue
            
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ í‰ê°€
            hybrid_score = self.calculate_hybrid_score(search_score, reranker_score)
            final_score = doc.get('final_score', hybrid_score)
            
            # final_scoreë„ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬
            final_score = final_score if final_score is not None else 0.0
            
            if final_score >= thresholds['hybrid_threshold']:
                filter_stats['hybrid_qualified'] += 1
                match_type = doc.get('service_match_type', 'unknown')
                match_desc = "ì¼ë°˜ìš©ì–´ ì „ì²´í•„ë“œ" if match_type == 'common_term' else match_type
                doc['filter_reason'] = f"í¬ê´„ì„± ìš°ì„  - {match_desc} ë§¤ì¹­ + í•˜ì´ë¸Œë¦¬ë“œ í†µê³¼ (ì ìˆ˜: {final_score:.2f})"
                doc['quality_tier'] = 'Standard'
                filtered_docs.append(doc)
                filter_stats['final_selected'] += 1
        
        # í¬ê´„ì„± ìš°ì„  ì •ë ¬ (ì˜ë¯¸ì  ìœ ì‚¬ì„± ì¤‘ì‹œ)
        def sort_key(doc):
            match_priority = {'exact': 3, 'partial': 2, 'common_term': 2.5, 'all': 1}
            semantic_boost = doc.get('semantic_similarity', 0) or 0
            final_score = doc.get('final_score', 0) or 0
            
            return (
                match_priority.get(doc.get('service_match_type', 'all'), 0), 
                final_score + (semantic_boost * 0.1)
            )
        
        filtered_docs.sort(key=sort_key, reverse=True)
        final_docs = filtered_docs[:thresholds['max_results']]
       
        common_term_info = f"\n        - ì¼ë°˜ìš©ì–´ ì „ì²´í•„ë“œ ë§¤ì¹­: {filter_stats['common_term_matches']}ê°œ" if is_common_service else ""
        
        st.info(f"""
        í¬ê´„ì„± ìš°ì„  í•„í„°ë§ ê²°ê³¼ (similar/default ìµœì í™”)
        - ì „ì²´ ê²€ìƒ‰ ê²°ê³¼: {filter_stats['total']}ê°œ
        - ê¸°ë³¸ ì ìˆ˜ í†µê³¼: {filter_stats['search_filtered']}ê°œ
        - ì´ ì„œë¹„ìŠ¤ëª… ë§¤ì¹­: {filter_stats['service_filtered']}ê°œ{common_term_info}
        - Reranker ê³ í’ˆì§ˆ: {filter_stats['reranker_qualified']}ê°œ
        - í•˜ì´ë¸Œë¦¬ë“œ í†µê³¼: {filter_stats['hybrid_qualified']}ê°œ
        - ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¶€ìŠ¤íŒ…: {filter_stats['semantic_boosted']}ê°œ
        - ìµœì¢… ì„ ë³„: {len(final_docs)}ê°œ
        """)
        
        return final_docs

    def semantic_search_with_adaptive_filtering(self, query, target_service_name=None, query_type="default", top_k=50):
        """ì¿¼ë¦¬ íƒ€ì…ë³„ ì ì‘í˜• í•„í„°ë§ì„ ì ìš©í•œ ì‹œë§¨í‹± ê²€ìƒ‰ - ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™” + ì¼ë°˜ìš©ì–´ ì„œë¹„ìŠ¤ëª… ì˜ˆì™¸ì²˜ë¦¬"""
        try:
            # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª… í™•ì¸
            is_common_service = False
            common_service_patterns = []
            if target_service_name:
                is_common, _ = self.is_common_term_service(target_service_name)
                if is_common:
                    is_common_service = True
                    common_service_patterns = self.get_common_term_search_patterns(target_service_name)
                    st.info(f"ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª… '{target_service_name}' ê²€ìƒ‰: ëª¨ë“  í•„ë“œ ëŒ€ìƒ")
            
            # ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜ ì¿¼ë¦¬ í™•ì¥
            expanded_query = self._expand_query_with_semantic_similarity(query)
            
            # ì¿¼ë¦¬ íƒ€ì…ë³„ ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¡°ì •
            if query_type in ['repair', 'cause']:
                top_k = max(top_k, 80)
                st.info(f"ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘... (ì •í™•ì„± ìš°ì„  - LLM ê²€ì¦ ì¤€ë¹„)")
            else:
                top_k = max(top_k, 30)
                st.info(f"ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘... (í¬ê´„ì„± ìš°ì„  - ê´‘ë²”ìœ„í•œ ê²€ìƒ‰)")
            
            # RAG ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… í¬í•¨ ê²€ìƒ‰ì„ ìœ„í•œ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            if target_service_name:
                if is_common_service:
                    # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª…: ëª¨ë“  í•„ë“œì—ì„œ ê²€ìƒ‰
                    field_queries = []
                    aliases = self.COMMON_TERM_SERVICES.get(target_service_name, [])
                    search_terms = [target_service_name] + aliases
                    
                    for term in search_terms:
                        field_queries.extend([
                            f'service_name:"{term}"',
                            f'effect:"{term}"',
                            f'symptom:"{term}"',
                            f'root_cause:"{term}"',
                            f'incident_repair:"{term}"',
                            f'repair_notice:"{term}"'
                        ])
                    
                    enhanced_query = f'({" OR ".join(field_queries)})'
                    if expanded_query != target_service_name:
                        enhanced_query += f" AND ({expanded_query})"
                else:
                    # ì¼ë°˜ì ì¸ ì„œë¹„ìŠ¤ëª… ê²€ìƒ‰
                    enhanced_query = f'(service_name:"{target_service_name}" OR service_name:*{target_service_name}*)'
                    if expanded_query != target_service_name:
                        enhanced_query += f" AND ({expanded_query})"
            else:
                enhanced_query = expanded_query
            
            # ì‹œë§¨í‹± ê²€ìƒ‰ ì‹¤í–‰
            results = self.search_client.search(
                search_text=enhanced_query,
                top=top_k,
                query_type="semantic",
                semantic_configuration_name="iap-incident-rebuild-meaning",
                include_total_count=True,
                select=[
                    "incident_id", "service_name", "error_time", "effect", "symptom", "repair_notice", 
                    "error_date", "week", "daynight", "root_cause", "incident_repair", 
                    "incident_plan", "cause_type", "done_type", "incident_grade", 
                    "owner_depart", "year", "month"
                ]
            )
            
            documents = []
            for result in results:
                documents.append({
                    "incident_id": result.get("incident_id", ""),
                    "service_name": result.get("service_name", ""),
                    "error_time": result.get("error_time", 0) if result.get("error_time") is not None else 0,
                    "effect": result.get("effect", ""),
                    "symptom": result.get("symptom", ""),
                    "repair_notice": result.get("repair_notice", ""),
                    "error_date": result.get("error_date", ""),
                    "week": result.get("week", ""),
                    "daynight": result.get("daynight", ""),
                    "root_cause": result.get("root_cause", ""),
                    "incident_repair": result.get("incident_repair", ""),
                    "incident_plan": result.get("incident_plan", ""),
                    "cause_type": result.get("cause_type", ""),
                    "done_type": result.get("done_type", ""),
                    "incident_grade": result.get("incident_grade", ""),
                    "owner_depart": result.get("owner_depart", ""),
                    "year": result.get("year", ""),
                    "month": result.get("month", ""),
                    "score": result.get("@search.score", 0) if result.get("@search.score") is not None else 0.0,
                    "reranker_score": result.get("@search.reranker_score", 0) if result.get("@search.reranker_score") is not None else 0.0
                })
            
            processing_mode = "ì •í™•ì„± ìš°ì„  + ì¼ë°˜ìš©ì–´ ì „ì²´í•„ë“œ" if is_common_service and query_type in ['repair', 'cause'] else \
                             "í¬ê´„ì„± ìš°ì„  + ì¼ë°˜ìš©ì–´ ì „ì²´í•„ë“œ" if is_common_service else \
                             "ì •í™•ì„± ìš°ì„ " if query_type in ['repair', 'cause'] else "í¬ê´„ì„± ìš°ì„ "
            
            st.info(f"ì¿¼ë¦¬ íƒ€ì…ë³„ ì ì‘í˜• ë¬¸ì„œ ì„ ë³„ ì¤‘... ({processing_mode} ìµœì í™”)")
            
            # ì¿¼ë¦¬ íƒ€ì…ë³„ ì ì‘í˜• í•„í„°ë§ ì ìš©
            if query_type in ['repair', 'cause']:
                filtered_documents = self.advanced_filter_documents_for_accuracy(documents, query_type, query, target_service_name)
            else:
                filtered_documents = self.simple_filter_documents_for_coverage(documents, query_type, query, target_service_name)
            
            return filtered_documents
            
        except Exception as e:
            st.warning(f"ì‹œë§¨í‹± ê²€ìƒ‰ ì‹¤íŒ¨, ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´: {str(e)}")
            return self.search_documents_with_service_filter(query, target_service_name, query_type, top_k//2)

    def search_documents_with_service_filter(self, query, target_service_name=None, query_type="default", top_k=15):
        """ì„œë¹„ìŠ¤ëª… í•„í„°ë§ì„ ì§€ì›í•˜ëŠ” ì¼ë°˜ ê²€ìƒ‰ (fallbackìš©) - ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™” + ì¼ë°˜ìš©ì–´ ì„œë¹„ìŠ¤ëª… ì˜ˆì™¸ì²˜ë¦¬"""
        try:
            # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª… í™•ì¸ ë° ì²˜ë¦¬
            if target_service_name:
                is_common, _ = self.is_common_term_service(target_service_name)
                
                if is_common:
                    # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª…: ëª¨ë“  í•„ë“œì—ì„œ ê²€ìƒ‰
                    aliases = self.COMMON_TERM_SERVICES.get(target_service_name, [])
                    search_terms = [target_service_name] + aliases
                    
                    field_queries = []
                    for term in search_terms:
                        field_queries.extend([
                            f'service_name:"{term}"',
                            f'effect:"{term}"',
                            f'symptom:"{term}"',
                            f'root_cause:"{term}"',
                            f'incident_repair:"{term}"'
                        ])
                    
                    enhanced_query = f'({" OR ".join(field_queries)}) AND ({query})'
                else:
                    # ì¼ë°˜ì ì¸ ì„œë¹„ìŠ¤ëª… ê²€ìƒ‰
                    enhanced_query = f'(service_name:"{target_service_name}" OR service_name:*{target_service_name}*) AND ({query})'
            else:
                enhanced_query = query
            
            results = self.search_client.search(
                search_text=enhanced_query,
                top=top_k,
                include_total_count=True,
                select=[
                    "incident_id", "service_name", "error_time", "effect", "symptom", "repair_notice", 
                    "error_date", "week", "daynight", "root_cause", "incident_repair", 
                    "incident_plan", "cause_type", "done_type", "incident_grade", 
                    "owner_depart", "year", "month"
                ]
            )
            
            documents = []
            for result in results:
                documents.append({
                    "incident_id": result.get("incident_id", ""),
                    "service_name": result.get("service_name", ""),
                    "error_time": result.get("error_time", 0) if result.get("error_time") is not None else 0,
                    "effect": result.get("effect", ""),
                    "symptom": result.get("symptom", ""),
                    "repair_notice": result.get("repair_notice", ""),
                    "error_date": result.get("error_date", ""),
                    "week": result.get("week", ""),
                    "daynight": result.get("daynight", ""),
                    "root_cause": result.get("root_cause", ""),
                    "incident_repair": result.get("incident_repair", ""),
                    "incident_plan": result.get("incident_plan", ""),
                    "cause_type": result.get("cause_type", ""),
                    "done_type": result.get("done_type", ""),
                    "incident_grade": result.get("incident_grade", ""),
                    "owner_depart": result.get("owner_depart", ""),
                    "year": result.get("year", ""),
                    "month": result.get("month", ""),
                    "score": result.get("@search.score", 0) if result.get("@search.score") is not None else 0.0,
                    "reranker_score": result.get("@search.reranker_score", 0) if result.get("@search.reranker_score") is not None else 0.0
                })
            
            # ì¿¼ë¦¬ íƒ€ì…ë³„ ì ì‘í˜• í•„í„°ë§ ì ìš©
            if query_type in ['repair', 'cause']:
                filtered_documents = self.advanced_filter_documents_for_accuracy(documents, query_type, query, target_service_name)
            else:
                filtered_documents = self.simple_filter_documents_for_coverage(documents, query_type, query, target_service_name)
            
            return filtered_documents
            
        except Exception as e:
            st.error(f"ì¼ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    def search_documents_fallback(self, query, target_service_name=None, top_k=25):
        """ë§¤ìš° ê´€ëŒ€í•œ ê¸°ì¤€ì˜ ëŒ€ì²´ ê²€ìƒ‰ - ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™” + ì¼ë°˜ìš©ì–´ ì„œë¹„ìŠ¤ëª… ì˜ˆì™¸ì²˜ë¦¬"""
        try:
            fallback_thresholds = {
                'search_threshold': 0.05,
                'reranker_threshold': 0.5,
                'hybrid_threshold': 0.1,
                'semantic_threshold': 0.05,
                'max_results': 15
            }
            
            if target_service_name:
                is_common, _ = self.is_common_term_service(target_service_name)
                
                if is_common:
                    # ì¼ë°˜ ìš©ì–´ ì„œë¹„ìŠ¤ëª…: ë‹¨ìˆœ í¬í•¨ ê²€ìƒ‰
                    search_query = f'*{target_service_name}*'
                else:
                    # ì¼ë°˜ì ì¸ ì„œë¹„ìŠ¤ëª… ê²€ìƒ‰
                    search_query = f'service_name:*{target_service_name}*'
            else:
                search_query = query
            
            results = self.search_client.search(
                search_text=search_query,
                top=top_k,
                include_total_count=True,
                select=[
                    "incident_id", "service_name", "error_time", "effect", "symptom", "repair_notice", 
                    "error_date", "week", "daynight", "root_cause", "incident_repair", 
                    "incident_plan", "cause_type", "done_type", "incident_grade", 
                    "owner_depart", "year", "month"
                ]
            )
            
            documents = []
            for result in results:
                score = result.get("@search.score", 0)
                score = score if score is not None else 0.0
                
                if score >= fallback_thresholds['search_threshold']:
                    doc = {
                        "incident_id": result.get("incident_id", ""),
                        "service_name": result.get("service_name", ""),
                        "error_time": result.get("error_time", 0) if result.get("error_time") is not None else 0,
                        "effect": result.get("effect", ""),
                        "symptom": result.get("symptom", ""),
                        "repair_notice": result.get("repair_notice", ""),
                        "error_date": result.get("error_date", ""),
                        "week": result.get("week", ""),
                        "daynight": result.get("daynight", ""),
                        "root_cause": result.get("root_cause", ""),
                        "incident_repair": result.get("incident_repair", ""),
                        "incident_plan": result.get("incident_plan", ""),
                        "cause_type": result.get("cause_type", ""),
                        "done_type": result.get("done_type", ""),
                        "incident_grade": result.get("incident_grade", ""),
                        "owner_depart": result.get("owner_depart", ""),
                        "year": result.get("year", ""),
                        "month": result.get("month", ""),
                        "score": score,
                        "reranker_score": result.get("@search.reranker_score", 0) if result.get("@search.reranker_score") is not None else 0.0,
                        "final_score": score,
                        "quality_tier": "Basic",
                        "filter_reason": f"ëŒ€ì²´ ê²€ìƒ‰ (ê´€ëŒ€í•œ ê¸°ì¤€, ì ìˆ˜: {score:.2f})",
                        "service_match_type": "fallback"
                    }
                    documents.append(doc)
            
            documents.sort(key=lambda x: x.get('final_score', 0) or 0, reverse=True)
            
            return documents[:fallback_thresholds['max_results']]
            
        except Exception as e:
            st.error(f"ëŒ€ì²´ ê²€ìƒ‰ë„ ì‹¤íŒ¨: {str(e)}")
            return []

    def _extract_service_name_legacy(self, query):
        """ê¸°ì¡´ íŒ¨í„´ ê¸°ë°˜ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ (fallback)"""
        service_patterns = [
            r'([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\(\)\s]*[A-Za-z0-9ê°€-í£_\-/\+\)])\s+(?:ë…„ë„ë³„|ì›”ë³„|ê±´ìˆ˜|ì¥ì• |í˜„ìƒ|ë³µêµ¬|ì„œë¹„ìŠ¤|í†µê³„|ë°œìƒ|ë°œìƒì¼ì|ì–¸ì œ)',
            r'ì„œë¹„ìŠ¤.*?([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\(\)\s]*[A-Za-z0-9ê°€-í£_\-/\+\)])',
            r'^([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\(\)\s]*[A-Za-z0-9ê°€-í£_\-/\+\)])\s+(?!ìœ¼ë¡œ|ì—ì„œ|ì—ê²Œ|ì—|ì„|ë¥¼|ì´|ê°€)',
            r'["\']([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\(\)\s]*[A-Za-z0-9ê°€-í£_\-/\+\)])["\']',
            r'\(([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\s]*[A-Za-z0-9ê°€-í£_\-/\+])\)',
            r'\b([A-Za-zê°€-í£][A-Za-z0-9ê°€-í£_\-/\+\(\)]{2,}(?:\s+[A-Za-z0-9ê°€-í£_\-/\+\(\)]+)*)\b'
        ]
        
        for pattern in service_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for match in matches:
                service_name = match.strip()
                if len(service_name) >= 2:
                    return service_name
        
        return None