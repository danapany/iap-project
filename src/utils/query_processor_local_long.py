import streamlit as st
import re
import time
import os
import json
from datetime import datetime
from config.prompts import SystemPrompts
from config.settings_local import AppConfigLocal
from utils.search_utils_local import SearchManagerLocal
from utils.ui_components_local import UIComponentsLocal
from utils.reprompting_db_manager import RepromptingDBManager
from utils.chart_utils import ChartManager 

# MonitoringManager import ì¶”ê°€
try:
    from utils.monitoring_manager import MonitoringManager
    MONITORING_AVAILABLE = True
except ImportError as e:
    print(f"DEBUG: MonitoringManager not available: {e}")
    MONITORING_AVAILABLE = False
    # í´ë°±ì„ ìœ„í•œ ë”ë¯¸ í´ë˜ìŠ¤
    class MonitoringManager:
        def __init__(self, *args, **kwargs):
            pass
        def log_user_activity(self, *args, **kwargs):
            pass

LANGSMITH_ENABLED = os.getenv('LANGSMITH_TRACING', 'false').lower() == 'true'

if LANGSMITH_ENABLED:
    try:
        from langsmith import traceable, trace
        from langsmith.wrappers import wrap_openai
        LANGSMITH_AVAILABLE = True
    except ImportError:
        LANGSMITH_AVAILABLE = False
        LANGSMITH_ENABLED = False
        def traceable(name=None, **kwargs):
            def decorator(func):
                return func
            return decorator
        
        def trace(name=None, **kwargs):
            class DummyTrace:
                def __enter__(self):
                    return self
                def __exit__(self, *args):
                    pass
                def update(self, **kwargs):
                    pass
            return DummyTrace()
else:
    LANGSMITH_AVAILABLE = False
    def traceable(name=None, **kwargs):
        def decorator(func):
            return func
        return decorator
    
    def trace(name=None, **kwargs):
        class DummyTrace:
            def __enter__(self):
                return self
            def __exit__(self, *args):
                pass
            def update(self, **kwargs):
                pass
        return DummyTrace()

class StatisticsValidator:
    """í†µê³„ ê³„ì‚° ê²€ì¦ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.validation_errors = []
        self.validation_warnings = []
    
    def validate_document(self, doc, doc_index):
        """ê°œë³„ ë¬¸ì„œ ë°ì´í„° ê²€ì¦"""
        errors = []
        warnings = []
        
        required_fields = ['incident_id', 'service_name', 'error_date']
        for field in required_fields:
            if not doc.get(field):
                errors.append(f"ë¬¸ì„œ {doc_index}: {field} í•„ë“œê°€ ë¹„ì–´ìˆìŒ")
        
        error_time = doc.get('error_time')
        if error_time is not None:
            try:
                error_time_int = int(error_time)
                if error_time_int < 0:
                    warnings.append(f"ë¬¸ì„œ {doc_index}: error_timeì´ ìŒìˆ˜ ({error_time_int})")
                elif error_time_int > 10080:
                    warnings.append(f"ë¬¸ì„œ {doc_index}: error_timeì´ ë¹„ì •ìƒì ìœ¼ë¡œ í¼ ({error_time_int}ë¶„)")
            except (ValueError, TypeError):
                errors.append(f"ë¬¸ì„œ {doc_index}: error_time í˜•ì‹ ì˜¤ë¥˜ ({error_time})")
        
        error_date = doc.get('error_date')
        if error_date:
            try:
                if len(str(error_date)) >= 4:
                    year = int(str(error_date)[:4])
                    if year < 2000 or year > 2030:
                        warnings.append(f"ë¬¸ì„œ {doc_index}: ë¹„ì •ìƒì ì¸ ì—°ë„ ({year})")
            except (ValueError, TypeError):
                warnings.append(f"ë¬¸ì„œ {doc_index}: error_date í˜•ì‹ ê²€ì¦ ì‹¤íŒ¨ ({error_date})")
        
        return errors, warnings
    
    def validate_statistics_result(self, stats, original_doc_count):
        """í†µê³„ ê²°ê³¼ ê²€ì¦"""
        errors = []
        warnings = []
        
        total_count = stats.get('total_count', 0)
        if total_count != original_doc_count:
            errors.append(f"ì´ ê°œìˆ˜ ë¶ˆì¼ì¹˜: ê³„ì‚°ëœ ê°œìˆ˜({total_count}) != ì›ë³¸ ê°œìˆ˜({original_doc_count})")
        
        yearly_stats = stats.get('yearly_stats', {})
        if yearly_stats:
            yearly_total = sum(yearly_stats.values())
            if yearly_total > total_count:
                warnings.append(f"ì—°ë„ë³„ í•©ê³„({yearly_total})ê°€ ì´ ê°œìˆ˜({total_count})ë¥¼ ì´ˆê³¼")
        
        monthly_stats = stats.get('monthly_stats', {})
        if monthly_stats:
            monthly_total = sum(monthly_stats.values())
            if stats.get('is_error_time_query', False):
                if monthly_total < 0:
                    errors.append(f"ì›”ë³„ ì¥ì• ì‹œê°„ í•©ê³„ê°€ ìŒìˆ˜: {monthly_total}")
            else:
                if monthly_total > total_count:
                    warnings.append(f"ì›”ë³„ ê±´ìˆ˜ í•©ê³„({monthly_total})ê°€ ì´ ê°œìˆ˜({total_count})ë¥¼ ì´ˆê³¼")
        
        return errors, warnings

class DataNormalizer:
    """ë°ì´í„° ì •ê·œí™” í´ë˜ìŠ¤ - ë‚ ì§œ ì¶”ì¶œ ë¡œì§ ê°œì„ """
    
    @staticmethod
    def normalize_error_time(error_time):
        """error_time í•„ë“œ ì •ê·œí™”"""
        if error_time is None:
            return 0
        
        try:
            if isinstance(error_time, str):
                error_time = error_time.strip()
                if error_time == '' or error_time.lower() in ['null', 'none', 'n/a']:
                    return 0
                return int(float(error_time))
            
            return int(error_time)
            
        except (ValueError, TypeError):
            return 0
    
    @staticmethod
    def normalize_date_fields(doc):
        """ë‚ ì§œ ê´€ë ¨ í•„ë“œ ì •ê·œí™” - ì¶”ì¶œ ë¡œì§ ê°œì„ """
        normalized_doc = doc.copy()
        
        error_date = doc.get('error_date', '')
        print(f"DEBUG: Normalizing error_date: {error_date}")
        
        if error_date:
            try:
                error_date_str = str(error_date).strip()
                
                # YYYY-MM-DD í˜•ì‹ ì²˜ë¦¬
                if '-' in error_date_str and len(error_date_str) >= 7:
                    parts = error_date_str.split('-')
                    if len(parts) >= 2:
                        # ì—°ë„ ì¶”ì¶œ
                        if parts[0].isdigit() and len(parts[0]) == 4:
                            normalized_doc['extracted_year'] = parts[0]
                            print(f"DEBUG: Extracted year from error_date: {parts[0]}")
                        
                        # ì›” ì¶”ì¶œ
                        if parts[1].isdigit():
                            month_num = int(parts[1])
                            if 1 <= month_num <= 12:
                                normalized_doc['extracted_month'] = str(month_num)
                                print(f"DEBUG: Extracted month from error_date: {month_num}")
                
                # YYYYMMDD í˜•ì‹ ì²˜ë¦¬
                elif len(error_date_str) >= 8 and error_date_str.isdigit():
                    normalized_doc['extracted_year'] = error_date_str[:4]
                    month_str = error_date_str[4:6]
                    try:
                        month_num = int(month_str)
                        if 1 <= month_num <= 12:
                            normalized_doc['extracted_month'] = str(month_num)
                            print(f"DEBUG: Extracted year/month from YYYYMMDD: {error_date_str[:4]}/{month_num}")
                    except (ValueError, TypeError):
                        pass
                
                # YYYY í˜•ì‹ë§Œ ìˆëŠ” ê²½ìš°
                elif len(error_date_str) >= 4 and error_date_str[:4].isdigit():
                    normalized_doc['extracted_year'] = error_date_str[:4]
                    print(f"DEBUG: Extracted year only: {error_date_str[:4]}")
                    
            except (ValueError, TypeError) as e:
                print(f"DEBUG: Error parsing error_date {error_date}: {e}")
                pass
        
        # ê¸°ì¡´ year/month í•„ë“œê°€ ì—†ìœ¼ë©´ ì¶”ì¶œëœ ê°’ìœ¼ë¡œ ì„¤ì •
        if not normalized_doc.get('year') and normalized_doc.get('extracted_year'):
            normalized_doc['year'] = normalized_doc['extracted_year']
            print(f"DEBUG: Set year field: {normalized_doc['year']}")
        
        if not normalized_doc.get('month') and normalized_doc.get('extracted_month'):
            normalized_doc['month'] = normalized_doc['extracted_month']
            print(f"DEBUG: Set month field: {normalized_doc['month']}")
        
        # ê¸°ì¡´ í•„ë“œ ì •ê·œí™”
        if normalized_doc.get('year'):
            normalized_doc['year'] = str(normalized_doc['year']).strip()
        
        if normalized_doc.get('month'):
            try:
                month_val = int(normalized_doc['month'])
                if 1 <= month_val <= 12:
                    normalized_doc['month'] = str(month_val)
                else:
                    normalized_doc['month'] = ''
            except (ValueError, TypeError):
                normalized_doc['month'] = ''
        
        return normalized_doc
    
    @staticmethod
    def normalize_document(doc):
        """ë¬¸ì„œ ì „ì²´ ì •ê·œí™”"""
        print(f"DEBUG: Normalizing document with incident_id: {doc.get('incident_id', 'N/A')}")
        
        normalized_doc = DataNormalizer.normalize_date_fields(doc)
        normalized_doc['error_time'] = DataNormalizer.normalize_error_time(doc.get('error_time'))
        
        # ë¬¸ìì—´ í•„ë“œë“¤ ì •ê·œí™”
        string_fields = ['service_name', 'incident_grade', 'owner_depart', 'daynight', 'week']
        for field in string_fields:
            value = normalized_doc.get(field)
            if value:
                normalized_doc[field] = str(value).strip()
            else:
                normalized_doc[field] = ''
        
        print(f"DEBUG: Normalized document - year: {normalized_doc.get('year')}, month: {normalized_doc.get('month')}")
        return normalized_doc

class ImprovedStatisticsCalculator:
    """ê°œì„ ëœ í†µê³„ ê³„ì‚° í´ë˜ìŠ¤ - ì¤‘ë³µ ì œê±° ì˜µì…˜ ì¶”ê°€"""
    
    def __init__(self, remove_duplicates=False):
        self.validator = StatisticsValidator()
        self.normalizer = DataNormalizer()
        self.remove_duplicates = remove_duplicates  # ì¤‘ë³µ ì œê±° ì˜µì…˜
    
    def _extract_filter_conditions(self, query):
        """ì¿¼ë¦¬ì—ì„œ í•„í„°ë§ ì¡°ê±´ ì¶”ì¶œ - ì •í™•ì„± ê°œì„ """
        conditions = {
            'year': None,
            'month': None,
            'start_month': None,
            'end_month': None,
            'daynight': None,
            'week': None,
            'service_name': None,
            'department': None,
            'grade': None
        }
        
        if not query:
            return conditions
        
        query_lower = query.lower()
        print(f"DEBUG: Extracting conditions from query: '{query}'")
        
        # ì—°ë„ ì¶”ì¶œ
        year_match = re.search(r'\b(202[0-9]|201[0-9])\b', query_lower)
        if year_match:
            conditions['year'] = year_match.group(1)
            print(f"DEBUG: Extracted year condition: {conditions['year']}")
        
        # ì›” ë²”ìœ„ ì¶”ì¶œ
        month_range_patterns = [
            r'\b(\d+)\s*~\s*(\d+)ì›”\b',
            r'\b(\d+)ì›”\s*~\s*(\d+)ì›”\b',  
            r'\b(\d+)\s*-\s*(\d+)ì›”\b',
            r'\b(\d+)ì›”\s*-\s*(\d+)ì›”\b'
        ]
        
        month_range_found = False
        for pattern in month_range_patterns:
            month_range_match = re.search(pattern, query_lower)
            if month_range_match:
                start_month = int(month_range_match.group(1))
                end_month = int(month_range_match.group(2))
                if 1 <= start_month <= 12 and 1 <= end_month <= 12 and start_month <= end_month:
                    conditions['start_month'] = start_month
                    conditions['end_month'] = end_month
                    month_range_found = True
                    print(f"DEBUG: Extracted month range condition: {start_month}~{end_month}")
                    break
        
        # ë‹¨ì¼ ì›” ì¶”ì¶œ (ì›” ë²”ìœ„ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        if not month_range_found:
            month_match = re.search(r'\b(\d{1,2})ì›”\b', query_lower)
            if month_match:
                month_num = int(month_match.group(1))
                if 1 <= month_num <= 12:
                    conditions['month'] = str(month_num)
                    print(f"DEBUG: Extracted single month condition: {month_num}")
        
        # ì‹œê°„ëŒ€ ì¡°ê±´
        if any(word in query_lower for word in ['ì•¼ê°„', 'ë°¤', 'ìƒˆë²½', 'ì‹¬ì•¼']):
            conditions['daynight'] = 'ì•¼ê°„'
        elif any(word in query_lower for word in ['ì£¼ê°„', 'ë‚®', 'ì˜¤ì „', 'ì˜¤í›„']):
            conditions['daynight'] = 'ì£¼ê°„'
        
        # ìš”ì¼ ì¡°ê±´
        week_patterns = {
            'ì›”': ['ì›”ìš”ì¼', 'ì›”'],
            'í™”': ['í™”ìš”ì¼', 'í™”'],
            'ìˆ˜': ['ìˆ˜ìš”ì¼', 'ìˆ˜'],
            'ëª©': ['ëª©ìš”ì¼', 'ëª©'],
            'ê¸ˆ': ['ê¸ˆìš”ì¼', 'ê¸ˆ'],
            'í† ': ['í† ìš”ì¼', 'í† '],
            'ì¼': ['ì¼ìš”ì¼', 'ì¼']
        }
        
        for week_key, patterns in week_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                conditions['week'] = week_key
                break
        
        if 'í‰ì¼' in query_lower:
            conditions['week'] = 'í‰ì¼'
        elif 'ì£¼ë§' in query_lower:
            conditions['week'] = 'ì£¼ë§'
        
        # ë“±ê¸‰ ì¡°ê±´
        grade_match = re.search(r'(\d+)ë“±ê¸‰', query_lower)
        if grade_match:
            conditions['grade'] = f"{grade_match.group(1)}ë“±ê¸‰"
        
        return conditions
    
    def _validate_document_against_conditions(self, doc, conditions):
        """ë¬¸ì„œê°€ ì¡°ê±´ì— ì •í™•íˆ ë§ëŠ”ì§€ ì—„ê²©í•˜ê²Œ ê²€ì¦"""
        incident_id = doc.get('incident_id', 'N/A')
        
        # ì—°ë„ ì¡°ê±´ í™•ì¸
        if conditions['year']:
            doc_year = self._extract_year_from_document(doc)
            if not doc_year or doc_year != conditions['year']:
                print(f"DEBUG: Document {incident_id} filtered out - year mismatch. Expected: {conditions['year']}, Got: {doc_year}")
                return False, f"year mismatch (expected: {conditions['year']}, got: {doc_year})"
        
        # ì›” ë²”ìœ„ ì¡°ê±´ í™•ì¸
        if conditions['start_month'] and conditions['end_month']:
            doc_month = self._extract_month_from_document(doc)
            if not doc_month:
                print(f"DEBUG: Document {incident_id} filtered out - no month info")
                return False, "no month information"
            
            try:
                month_num = int(doc_month)
                if not (conditions['start_month'] <= month_num <= conditions['end_month']):
                    print(f"DEBUG: Document {incident_id} filtered out - month {month_num} not in range {conditions['start_month']}~{conditions['end_month']}")
                    return False, f"month {month_num} not in range {conditions['start_month']}~{conditions['end_month']}"
            except (ValueError, TypeError):
                print(f"DEBUG: Document {incident_id} filtered out - invalid month format: {doc_month}")
                return False, f"invalid month format: {doc_month}"
        
        # ë‹¨ì¼ ì›” ì¡°ê±´ í™•ì¸
        elif conditions['month']:
            doc_month = self._extract_month_from_document(doc)
            if not doc_month or str(doc_month) != conditions['month']:
                print(f"DEBUG: Document {incident_id} filtered out - month mismatch. Expected: {conditions['month']}, Got: {doc_month}")
                return False, f"month mismatch (expected: {conditions['month']}, got: {doc_month})"
        
        # ì‹œê°„ëŒ€ ì¡°ê±´ í™•ì¸
        if conditions['daynight']:
            doc_daynight = doc.get('daynight', '').strip()
            required_daynight = conditions['daynight']
            
            if not doc_daynight:
                return False, f"no daynight information"
            elif doc_daynight != required_daynight:
                return False, f"daynight mismatch (expected: {required_daynight}, got: {doc_daynight})"
        
        # ìš”ì¼ ì¡°ê±´ í™•ì¸
        if conditions['week']:
            doc_week = doc.get('week', '').strip()
            required_week = conditions['week']
            
            if required_week == 'í‰ì¼':
                if doc_week not in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ']:
                    return False, f"not weekday (got: {doc_week})"
            elif required_week == 'ì£¼ë§':
                if doc_week not in ['í† ', 'ì¼']:
                    return False, f"not weekend (got: {doc_week})"
            else:
                if not doc_week:
                    return False, f"no week information"
                elif doc_week != required_week:
                    return False, f"week mismatch (expected: {required_week}, got: {doc_week})"
        
        # ë“±ê¸‰ ì¡°ê±´ í™•ì¸
        if conditions['grade']:
            doc_grade = doc.get('incident_grade', '')
            if doc_grade != conditions['grade']:
                return False, f"grade mismatch (expected: {conditions['grade']}, got: {doc_grade})"
        
        print(f"DEBUG: Document {incident_id} passed all conditions")
        return True, "passed"
    
    def _extract_year_from_document(self, doc):
        """ë¬¸ì„œì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ"""
        # 1ìˆœìœ„: year í•„ë“œ
        year = doc.get('year')
        if year:
            year_str = str(year).strip()
            if len(year_str) == 4 and year_str.isdigit():
                return year_str
        
        # 2ìˆœìœ„: extracted_year í•„ë“œ  
        year = doc.get('extracted_year')
        if year:
            year_str = str(year).strip()
            if len(year_str) == 4 and year_str.isdigit():
                return year_str
        
        # 3ìˆœìœ„: error_dateì—ì„œ ì¶”ì¶œ
        error_date = doc.get('error_date', '')
        if error_date:
            error_date_str = str(error_date).strip()
            if len(error_date_str) >= 4:
                if '-' in error_date_str:
                    parts = error_date_str.split('-')
                    if len(parts) > 0 and len(parts[0]) == 4 and parts[0].isdigit():
                        return parts[0]
                elif len(error_date_str) >= 8 and error_date_str[:4].isdigit():
                    return error_date_str[:4]
                elif len(error_date_str) >= 4 and error_date_str[:4].isdigit():
                    return error_date_str[:4]
        
        return None
    
    def _extract_month_from_document(self, doc):
        """ë¬¸ì„œì—ì„œ ì›” ì •ë³´ ì¶”ì¶œ"""
        # 1ìˆœìœ„: month í•„ë“œ
        month = doc.get('month')
        if month:
            try:
                month_num = int(month)
                if 1 <= month_num <= 12:
                    return str(month_num)
            except (ValueError, TypeError):
                pass
        
        # 2ìˆœìœ„: extracted_month í•„ë“œ
        month = doc.get('extracted_month')
        if month:
            try:
                month_num = int(month)
                if 1 <= month_num <= 12:
                    return str(month_num)
            except (ValueError, TypeError):
                pass
        
        # 3ìˆœìœ„: error_dateì—ì„œ ì¶”ì¶œ
        error_date = doc.get('error_date', '')
        if error_date:
            error_date_str = str(error_date).strip()
            if '-' in error_date_str:
                parts = error_date_str.split('-')
                if len(parts) >= 2 and parts[1].isdigit():
                    try:
                        month_num = int(parts[1])
                        if 1 <= month_num <= 12:
                            return str(month_num)
                    except (ValueError, TypeError):
                        pass
            elif len(error_date_str) >= 6 and error_date_str.isdigit():
                try:
                    month_num = int(error_date_str[4:6])
                    if 1 <= month_num <= 12:
                        return str(month_num)
                except (ValueError, TypeError):
                    pass
        
        return None
    
    def _apply_filters(self, documents, conditions):
        """í•„í„° ì¡°ê±´ì— ë”°ë¥¸ ë¬¸ì„œ í•„í„°ë§ - ì—„ê²©í•œ ê²€ì¦ ì¶”ê°€"""
        filtered_docs = []
        filter_stats = {
            'total_input': len(documents),
            'passed': 0,
            'filtered_reasons': {}
        }
        
        print(f"DEBUG: Starting filtering with conditions: {conditions}")
        print(f"DEBUG: Input documents: {len(documents)}")
        
        for doc in documents:
            incident_id = doc.get('incident_id', 'N/A')
            error_date = doc.get('error_date', 'N/A')
            
            is_valid, reason = self._validate_document_against_conditions(doc, conditions)
            
            if is_valid:
                filtered_docs.append(doc)
                filter_stats['passed'] += 1
                print(f"DEBUG: âœ“ INCLUDED - ID: {incident_id}, Date: {error_date}")
            else:
                filter_stats['filtered_reasons'][reason] = filter_stats['filtered_reasons'].get(reason, 0) + 1
                print(f"DEBUG: âœ— EXCLUDED - ID: {incident_id}, Date: {error_date}, Reason: {reason}")
        
        print(f"DEBUG: Filtering complete - {filter_stats['passed']}/{filter_stats['total_input']} documents passed")
        print(f"DEBUG: Filter reasons: {filter_stats['filtered_reasons']}")
        
        return filtered_docs
    
    def _matches_conditions(self, doc, conditions):
        """ì¡°ê±´ ë§¤ì¹­ í™•ì¸ (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)"""
        is_valid, _ = self._validate_document_against_conditions(doc, conditions)
        return is_valid
    
    def _empty_statistics(self):
        """ë¹ˆ í†µê³„ ë°˜í™˜"""
        return {
            'total_count': 0,
            'yearly_stats': {},
            'monthly_stats': {},
            'time_stats': {'daynight': {}, 'week': {}},
            'department_stats': {},
            'service_stats': {},
            'grade_stats': {},
            'is_error_time_query': False,
            'validation': {'errors': [], 'warnings': [], 'is_valid': True}
        }
    
    def _is_error_time_query(self, query):
        """ì¥ì• ì‹œê°„ ê´€ë ¨ ì¿¼ë¦¬ì¸ì§€ í™•ì¸"""
        if not query:
            return False
        
        error_time_keywords = ['ì¥ì• ì‹œê°„', 'ì¥ì•  ì‹œê°„', 'error_time', 'ì‹œê°„ í†µê³„', 'ì‹œê°„ í•©ê³„', 'ì‹œê°„ í•©ì‚°', 'ë¶„']
        return any(keyword in query.lower() for keyword in error_time_keywords)
    
    def _calculate_detailed_statistics(self, documents, conditions, is_error_time_query):
        """ìƒì„¸ í†µê³„ ê³„ì‚° - ì¤‘ë³µ ì œê±° ì—†ì´ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬"""
        stats = {
            'total_count': len(documents),
            'yearly_stats': {},
            'monthly_stats': {},
            'time_stats': {'daynight': {}, 'week': {}},
            'department_stats': {},
            'service_stats': {},
            'grade_stats': {},
            'is_error_time_query': is_error_time_query,
            'filter_conditions': conditions,
            'calculation_details': {}
        }
        
        print(f"DEBUG: Calculating statistics for {len(documents)} documents (NO DEDUPLICATION)")
        print(f"DEBUG: Filter conditions: {conditions}")
        
        # ë¬¸ì„œë³„ ì²˜ë¦¬ ìƒí™©ì„ ë””ë²„ê¹…ìœ¼ë¡œ í™•ì¸
        print("DEBUG: Processing each document for statistics:")
        
        # ì—°ë„ë³„ í†µê³„ - ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬
        for i, doc in enumerate(documents):
            incident_id = doc.get('incident_id', 'N/A')
            error_date = doc.get('error_date', 'N/A')
            year = self._extract_year_from_document(doc)
            
            print(f"DEBUG: Doc {i+1}: ID={incident_id}, Date={error_date}, Extracted_Year={year}")
            
            if year:
                if is_error_time_query:
                    error_time = doc.get('error_time', 0)
                    stats['yearly_stats'][year] = stats['yearly_stats'].get(year, 0) + error_time
                    print(f"DEBUG: Added {error_time} minutes to year {year}, total now: {stats['yearly_stats'][year]}")
                else:
                    stats['yearly_stats'][year] = stats['yearly_stats'].get(year, 0) + 1
                    print(f"DEBUG: Added 1 count to year {year}, total now: {stats['yearly_stats'][year]}")
            else:
                print(f"DEBUG: No year extracted for document {incident_id}")
        
        print(f"DEBUG: Final yearly stats: {stats['yearly_stats']}")
        
        # ì›”ë³„ í†µê³„ - ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬
        monthly_data = {}
        
        # ì›” ë²”ìœ„ê°€ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ ì›”ë“¤ì„ ë¯¸ë¦¬ ì´ˆê¸°í™”
        if conditions['start_month'] and conditions['end_month']:
            for month_num in range(conditions['start_month'], conditions['end_month'] + 1):
                month_key = f"{month_num}ì›”"
                monthly_data[month_key] = 0
            print(f"DEBUG: Initialized months {conditions['start_month']}~{conditions['end_month']}")
        
        # ê° ë¬¸ì„œì— ëŒ€í•´ ì›”ë³„ í†µê³„ ê³„ì‚°
        print("DEBUG: Processing each document for monthly stats:")
        for i, doc in enumerate(documents):
            incident_id = doc.get('incident_id', 'N/A')
            month = self._extract_month_from_document(doc)
            
            print(f"DEBUG: Monthly Doc {i+1}: ID={incident_id}, Extracted_Month={month}")
            
            if month:
                try:
                    month_num = int(month)
                    if 1 <= month_num <= 12:
                        month_key = f"{month_num}ì›”"
                        
                        if is_error_time_query:
                            error_time = doc.get('error_time', 0)
                            monthly_data[month_key] = monthly_data.get(month_key, 0) + error_time
                            print(f"DEBUG: Added {error_time} minutes to {month_key}, total now: {monthly_data[month_key]}")
                        else:
                            monthly_data[month_key] = monthly_data.get(month_key, 0) + 1
                            print(f"DEBUG: Added 1 count to {month_key}, total now: {monthly_data[month_key]}")
                except (ValueError, TypeError):
                    print(f"DEBUG: Invalid month format for document {incident_id}: {month}")
                    continue
            else:
                print(f"DEBUG: No month extracted for document {incident_id}")
        
        # ì›” ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ statsì— ì €ì¥
        month_order = [f"{i}ì›”" for i in range(1, 13)]
        for month in month_order:
            if month in monthly_data:
                stats['monthly_stats'][month] = monthly_data[month]
        
        print(f"DEBUG: Final monthly stats: {stats['monthly_stats']}")
        
        # ì‹œê°„ëŒ€ë³„ í†µê³„ - ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬
        print("DEBUG: Processing each document for time stats:")
        for i, doc in enumerate(documents):
            incident_id = doc.get('incident_id', 'N/A')
            daynight = doc.get('daynight', '')
            week = doc.get('week', '')
            
            print(f"DEBUG: Time Doc {i+1}: ID={incident_id}, daynight={daynight}, week={week}")
            
            if daynight:
                if is_error_time_query:
                    error_time = doc.get('error_time', 0)
                    stats['time_stats']['daynight'][daynight] = stats['time_stats']['daynight'].get(daynight, 0) + error_time
                else:
                    stats['time_stats']['daynight'][daynight] = stats['time_stats']['daynight'].get(daynight, 0) + 1
            
            if week:
                if is_error_time_query:
                    error_time = doc.get('error_time', 0)
                    stats['time_stats']['week'][week] = stats['time_stats']['week'].get(week, 0) + error_time
                else:
                    stats['time_stats']['week'][week] = stats['time_stats']['week'].get(week, 0) + 1
        
        # ë¶€ì„œë³„ í†µê³„ - ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬
        print("DEBUG: Processing each document for department stats:")
        for i, doc in enumerate(documents):
            incident_id = doc.get('incident_id', 'N/A')
            department = doc.get('owner_depart', '')
            
            print(f"DEBUG: Dept Doc {i+1}: ID={incident_id}, department={department}")
            
            if department:
                if is_error_time_query:
                    error_time = doc.get('error_time', 0)
                    stats['department_stats'][department] = stats['department_stats'].get(department, 0) + error_time
                    print(f"DEBUG: Added {error_time} minutes to department {department}")
                else:
                    stats['department_stats'][department] = stats['department_stats'].get(department, 0) + 1
                    print(f"DEBUG: Added 1 count to department {department}")
        
        # ì„œë¹„ìŠ¤ë³„ í†µê³„ - ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬
        print("DEBUG: Processing each document for service stats:")
        for i, doc in enumerate(documents):
            incident_id = doc.get('incident_id', 'N/A')
            service = doc.get('service_name', '')
            
            print(f"DEBUG: Service Doc {i+1}: ID={incident_id}, service={service}")
            
            if service:
                if is_error_time_query:
                    error_time = doc.get('error_time', 0)
                    stats['service_stats'][service] = stats['service_stats'].get(service, 0) + error_time
                    print(f"DEBUG: Added {error_time} minutes to service {service}")
                else:
                    stats['service_stats'][service] = stats['service_stats'].get(service, 0) + 1
                    print(f"DEBUG: Added 1 count to service {service}")
        
        # ë“±ê¸‰ë³„ í†µê³„ - ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬
        print("DEBUG: Processing each document for grade stats:")
        for i, doc in enumerate(documents):
            incident_id = doc.get('incident_id', 'N/A')
            grade = doc.get('incident_grade', '')
            
            print(f"DEBUG: Grade Doc {i+1}: ID={incident_id}, grade={grade}")
            
            if grade:
                if is_error_time_query:
                    error_time = doc.get('error_time', 0)
                    stats['grade_stats'][grade] = stats['grade_stats'].get(grade, 0) + error_time
                    print(f"DEBUG: Added {error_time} minutes to grade {grade}")
                else:
                    stats['grade_stats'][grade] = stats['grade_stats'].get(grade, 0) + 1
                    print(f"DEBUG: Added 1 count to grade {grade}")
        
        # ìµœì¢… í†µê³„ ìš”ì•½ ì¶œë ¥
        print("DEBUG: === FINAL STATISTICS SUMMARY ===")
        print(f"DEBUG: Total documents processed: {len(documents)}")
        print(f"DEBUG: Yearly stats: {stats['yearly_stats']}")
        print(f"DEBUG: Monthly stats: {stats['monthly_stats']}")
        print(f"DEBUG: Service stats: {stats['service_stats']}")
        print(f"DEBUG: Grade stats: {stats['grade_stats']}")
        print(f"DEBUG: Department stats: {stats['department_stats']}")
        print(f"DEBUG: Time stats: {stats['time_stats']}")
        print("DEBUG: ===================================")
        
        # ê³„ì‚° ì„¸ë¶€ì‚¬í•­
        total_error_time = sum(doc.get('error_time', 0) for doc in documents)
        stats['calculation_details'] = {
            'total_error_time_minutes': total_error_time,
            'total_error_time_hours': round(total_error_time / 60, 2),
            'average_error_time': round(total_error_time / len(documents), 2) if documents else 0,
            'max_error_time': max((doc.get('error_time', 0) for doc in documents), default=0),
            'min_error_time': min((doc.get('error_time', 0) for doc in documents), default=0),
            'documents_with_error_time': len([doc for doc in documents if doc.get('error_time', 0) > 0])
        }
        
        return stats
    
    def calculate_comprehensive_statistics(self, documents, query, query_type="default"):
        """ì¢…í•©ì ì¸ í†µê³„ ê³„ì‚° - ì¤‘ë³µ ì œê±° ì˜µì…˜ ì ìš© ë° í•„í„°ë§ ìµœì†Œí™”"""
        if not documents:
            return self._empty_statistics()
        
        print(f"DEBUG: ============ STATISTICS CALCULATION START ============")
        print(f"DEBUG: Query: '{query}'")
        print(f"DEBUG: Input documents: {len(documents)}")
        print(f"DEBUG: Remove duplicates option: {self.remove_duplicates}")
        
        # ì…ë ¥ ë¬¸ì„œ ìƒíƒœ í™•ì¸
        for i, doc in enumerate(documents[:3]):  # ì²˜ìŒ 3ê°œë§Œ ë¡œê·¸
            incident_id = doc.get('incident_id', 'N/A')
            error_date = doc.get('error_date', 'N/A')
            year = doc.get('year', 'N/A')
            month = doc.get('month', 'N/A')
            print(f"DEBUG: Input doc {i+1}: ID={incident_id}, error_date={error_date}, year={year}, month={month}")
        
        # ë¬¸ì„œ ì •ê·œí™” ë° ê²€ì¦
        normalized_docs = []
        validation_errors = []
        validation_warnings = []
        
        for i, doc in enumerate(documents):
            if doc is None:
                continue
            
            errors, warnings = self.validator.validate_document(doc, i)
            validation_errors.extend(errors)
            validation_warnings.extend(warnings)
            
            normalized_doc = self.normalizer.normalize_document(doc)
            normalized_docs.append(normalized_doc)
        
        print(f"DEBUG: After normalization: {len(normalized_docs)} documents")
        
        # ì •ê·œí™” í›„ ìƒíƒœ í™•ì¸
        for i, doc in enumerate(normalized_docs[:3]):
            incident_id = doc.get('incident_id', 'N/A')
            error_date = doc.get('error_date', 'N/A')
            year = doc.get('year', 'N/A')
            month = doc.get('month', 'N/A')
            extracted_year = doc.get('extracted_year', 'N/A')
            extracted_month = doc.get('extracted_month', 'N/A')
            print(f"DEBUG: Normalized doc {i+1}: ID={incident_id}, error_date={error_date}, year={year}, month={month}, extracted_year={extracted_year}, extracted_month={extracted_month}")

        # ì¤‘ë³µ ì œê±° (ì˜µì…˜ì— ë”°ë¼)
        if self.remove_duplicates:
            print("DEBUG: Applying duplicate removal")
            unique_docs = {}
            for doc in normalized_docs:
                incident_id = doc.get('incident_id', '')
                if incident_id and incident_id not in unique_docs:
                    unique_docs[incident_id] = doc
                elif not incident_id:
                    temp_id = f"temp_{len(unique_docs)}"
                    unique_docs[temp_id] = doc
            
            clean_documents = list(unique_docs.values())
            print(f"DEBUG: After deduplication: {len(clean_documents)} unique documents")
        else:
            print("DEBUG: Skipping duplicate removal - keeping all documents")
            clean_documents = normalized_docs
            print(f"DEBUG: Keeping all documents: {len(clean_documents)} documents")
        
        # í•„í„° ì¡°ê±´ ì¶”ì¶œ
        filter_conditions = self._extract_filter_conditions(query)
        print(f"DEBUG: Extracted filter conditions: {filter_conditions}")
        
        # í†µê³„ì„± ì§ˆë¬¸ì˜ ê²½ìš° í•„í„°ë§ ìµœì†Œí™” - ì„œë¹„ìŠ¤ëª… í•„í„°ë§ë„ ë¹„í™œì„±í™”
        is_stats_query = any(keyword in query.lower() for keyword in ['ê±´ìˆ˜', 'í†µê³„', 'ì—°ë„ë³„', 'ì›”ë³„', 'í˜„í™©', 'ë¶„í¬', 'ì•Œë ¤ì¤˜', 'ëª‡ê±´', 'ê°œìˆ˜'])
        
        if is_stats_query:
            print("DEBUG: Statistics query detected - skipping ALL filtering to preserve all documents")
            print(f"DEBUG: Original filter conditions ignored: {filter_conditions}")
            # í†µê³„ì„± ì§ˆë¬¸ì—ì„œëŠ” ëª¨ë“  í•„í„°ë§ ë¹„í™œì„±í™”
            filtered_docs = clean_documents
        else:
            # ì¼ë°˜ ì§ˆë¬¸ì—ì„œë§Œ ë¬¸ì„œ í•„í„°ë§ ì ìš©
            filtered_docs = self._apply_filters(clean_documents, filter_conditions)
            print(f"DEBUG: After filtering: {len(filtered_docs)} documents")
        
        # ìµœì¢… í•„í„°ë§ëœ ë¬¸ì„œë“¤ í™•ì¸
        print(f"DEBUG: ========== FINAL FILTERED DOCUMENTS ==========")
        for i, doc in enumerate(filtered_docs):
            incident_id = doc.get('incident_id', 'N/A')
            error_date = doc.get('error_date', 'N/A')
            year = doc.get('year', 'N/A')
            month = doc.get('month', 'N/A')
            print(f"DEBUG: Final doc {i+1}: ID={incident_id}, error_date={error_date}, year={year}, month={month}")

        # ì¥ì• ì‹œê°„ ì¿¼ë¦¬ ì—¬ë¶€ í™•ì¸
        is_error_time_query = self._is_error_time_query(query)
        
        # í†µê³„ ê³„ì‚°
        stats = self._calculate_detailed_statistics(filtered_docs, filter_conditions, is_error_time_query)
        
        # ê²°ê³¼ ê²€ì¦
        result_errors, result_warnings = self.validator.validate_statistics_result(stats, len(filtered_docs))
        validation_errors.extend(result_errors)
        validation_warnings.extend(result_warnings)
        
        stats['validation'] = {
            'errors': validation_errors,
            'warnings': validation_warnings,
            'is_valid': len(validation_errors) == 0
        }
        
        print(f"DEBUG: ============ STATISTICS CALCULATION END ============")
        print(f"DEBUG: Final statistics: {stats}")
        
        return stats

class QueryProcessorLocal:
    """ê°œì„ ëœ ì¿¼ë¦¬ ì²˜ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤ + ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ ì¶”ê°€"""
    
    def __init__(self, azure_openai_client, search_client, model_name, config=None):
        self.azure_openai_client = azure_openai_client
        self.search_client = search_client
        self.model_name = model_name
        self.config = config if config else AppConfigLocal()
        self.search_manager = SearchManagerLocal(search_client, self.config)
        self.ui_components = UIComponentsLocal()
        self.reprompting_db_manager = RepromptingDBManager()
        self.chart_manager = ChartManager()
        # ì¤‘ë³µ ì œê±° ë¹„í™œì„±í™”ë¡œ ë³€ê²½
        self.statistics_calculator = ImprovedStatisticsCalculator(remove_duplicates=False)
        self.debug_mode = True  # ë””ë²„ê¹… ëª¨ë“œ í™œì„±í™”
        
        # ëª¨ë‹ˆí„°ë§ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì¶”ê°€
        if MONITORING_AVAILABLE:
            try:
                self.monitoring_manager = MonitoringManager()
                self.monitoring_enabled = True
                print("DEBUG: MonitoringManager initialized successfully")
            except Exception as e:
                print(f"DEBUG: Failed to initialize MonitoringManager: {e}")
                self.monitoring_manager = MonitoringManager()  # ë”ë¯¸ ì¸ìŠ¤í„´ìŠ¤
                self.monitoring_enabled = False
        else:
            self.monitoring_manager = MonitoringManager()  # ë”ë¯¸ ì¸ìŠ¤í„´ìŠ¤
            self.monitoring_enabled = False
            print("DEBUG: MonitoringManager not available, using dummy instance")
        
        self.langsmith_enabled = LANGSMITH_ENABLED
        self._setup_langsmith()
    
    def _get_client_ip(self):
        """í´ë¼ì´ì–¸íŠ¸ IP ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # Streamlitì—ì„œ IP ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°
            if hasattr(st, 'session_state') and hasattr(st.session_state, 'client_ip'):
                return st.session_state.client_ip
            
            # í—¤ë”ì—ì„œ IP ê°€ì ¸ì˜¤ê¸° ì‹œë„
            headers = st.context.headers if hasattr(st, 'context') and hasattr(st.context, 'headers') else {}
            
            # ë‹¤ì–‘í•œ í—¤ë”ì—ì„œ IP ì¶”ì¶œ ì‹œë„
            ip_headers = [
                'X-Forwarded-For',
                'X-Real-IP', 
                'X-Client-IP',
                'CF-Connecting-IP',  # Cloudflare
                'True-Client-IP'
            ]
            
            for header in ip_headers:
                if header in headers:
                    ip = headers[header].split(',')[0].strip()
                    if ip and ip != '127.0.0.1':
                        return ip
            
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return '127.0.0.1'
            
        except Exception as e:
            print(f"DEBUG: Error getting client IP: {e}")
            return '127.0.0.1'
    
    def _get_user_agent(self):
        """ì‚¬ìš©ì ì—ì´ì „íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if hasattr(st, 'context') and hasattr(st.context, 'headers'):
                return st.context.headers.get('User-Agent', 'Unknown')
            return 'Streamlit-App'
        except Exception as e:
            print(f"DEBUG: Error getting user agent: {e}")
            return 'Unknown'
    
    def _log_user_activity(self, question, query_type=None, response_time=None, 
                          document_count=None, success=True, error_message=None):
        """ì‚¬ìš©ì í™œë™ ë¡œê¹…"""
        if not self.monitoring_enabled:
            return
            
        try:
            ip_address = self._get_client_ip()
            user_agent = self._get_user_agent()
            
            self.monitoring_manager.log_user_activity(
                ip_address=ip_address,
                question=question,
                query_type=query_type,
                user_agent=user_agent,
                response_time=response_time,
                document_count=document_count,
                success=success,
                error_message=error_message
            )
            
            if self.debug_mode:
                print(f"DEBUG: Logged user activity - IP: {ip_address}, Query: {question[:50]}...")
                
        except Exception as e:
            print(f"DEBUG: Error logging user activity: {e}")
    
    def safe_trace_update(self, trace_obj, **kwargs):
        """LangSmith trace ê°ì²´ì˜ ì•ˆì „í•œ ì—…ë°ì´íŠ¸"""
        if not self.langsmith_enabled:
            return
            
        try:
            if hasattr(trace_obj, 'update'):
                trace_obj.update(**kwargs)
            elif hasattr(trace_obj, 'add_outputs'):
                if 'outputs' in kwargs:
                    trace_obj.add_outputs(kwargs['outputs'])
                if 'metadata' in kwargs:
                    trace_obj.add_metadata(kwargs['metadata'])
        except Exception as e:
            pass
    
    def _setup_langsmith(self):
        if not self.langsmith_enabled:
            return
            
        try:
            langsmith_status = self.config.get_langsmith_status()
            if langsmith_status['enabled'] and LANGSMITH_AVAILABLE:
                success = self.config.setup_langsmith()
                if success:
                    self.azure_openai_client = wrap_openai(self.azure_openai_client)
        except Exception as e:
            pass

    def calculate_unified_statistics(self, documents, query, query_type="default"):
        """ê°œì„ ëœ í†µí•© í†µê³„ ê³„ì‚°"""
        if not documents:
            return self.statistics_calculator._empty_statistics()
        
        print(f"DEBUG: Using improved statistics calculator (no duplicates removal) for {len(documents)} documents")
        return self.statistics_calculator.calculate_comprehensive_statistics(documents, query, query_type)

    def extract_service_name_from_query_enhanced(self, query):
        """í–¥ìƒëœ ì„œë¹„ìŠ¤ëª… ì¶”ì¶œ"""
        if not query:
            return None
        
        service_patterns = [
            r'([A-Za-z][A-Za-z0-9_\-/\+\(\)\s]*[A-Za-z0-9_\-/\+\)])\s+(?:ì¥ì• |í˜„ìƒ|ë³µêµ¬|ì„œë¹„ìŠ¤|ì˜¤ë¥˜|ë¬¸ì œ|ë¶ˆê°€)',
            r'ì„œë¹„ìŠ¤.*?([A-Za-z][A-Za-z0-9_\-/\+\(\)\s]*[A-Za-z0-9_\-/\+\)])',
            r'^([A-Za-z][A-Za-z0-9_\-/\+\(\)\s]*[A-Za-z0-9_\-/\+\)])\s+',
            r'["\']([A-Za-z][A-Za-z0-9_\-/\+\(\)\s]*[A-Za-z0-9_\-/\+\)])["\']',
            r'\(([A-Za-z][A-Za-z0-9_\-/\+\s]*[A-Za-z0-9_\-/\+])\)',
            r'\b([A-Za-z][A-Za-z0-9_\-/\+\(\)]{2,}(?:\s+[A-Za-z0-9_\-/\+\(\)]+)*)\b',
            r'([ê°€-í£]{2,10})\s+(?:ì„œë¹„ìŠ¤|ì‹œìŠ¤í…œ|ì¥ì• |í˜„ìƒ|ë³µêµ¬|ì˜¤ë¥˜|ë¬¸ì œ|ë¶ˆê°€)',
            r'ì„œë¹„ìŠ¤.*?([ê°€-í£]{2,10})',
            r'^([ê°€-í£]{2,10})\s+',
            r'([A-Za-z]+[ê°€-í£]+|[ê°€-í£]+[A-Za-z]+)(?:\s+(?:ì„œë¹„ìŠ¤|ì‹œìŠ¤í…œ|ì¥ì• |í˜„ìƒ|ë³µêµ¬|ì˜¤ë¥˜|ë¬¸ì œ|ë¶ˆê°€))?',
        ]
        
        for pattern in service_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for match in matches:
                service_name = match.strip()
                if self.is_valid_service_name_enhanced(service_name):
                    return service_name
        
        return None
    
    def is_valid_service_name_enhanced(self, service_name):
        """í–¥ìƒëœ ì„œë¹„ìŠ¤ëª… ìœ íš¨ì„± ê²€ì¦"""
        if len(service_name) < 2:
            return False
        
        if not (service_name[0].isalpha() or ord('ê°€') <= ord(service_name[0]) <= ord('í£')):
            return False
        
        excluded_words = [
            'service', 'system', 'server', 'client', 'application', 'app',
            'website', 'web', 'platform', 'portal', 'interface', 'api',
            'database', 'data', 'file', 'log', 'error', 'issue', 'problem',
            'http', 'https', 'www', 'com', 'org', 'net',
            'ì¥ì• ', 'í˜„ìƒ', 'ë³µêµ¬', 'í†µê³„', 'ë°œìƒ', 'ì„œë¹„ìŠ¤', 'ì‹œìŠ¤í…œ'
        ]
        
        clean_name = re.sub(r'[\(\)/\+_\-\s]', '', service_name).lower()
        if clean_name in excluded_words:
            return False
        
        return True

    def extract_keywords_from_query_enhanced(self, query):
        """í–¥ìƒëœ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        tech_keywords = []
        
        tech_patterns = [
            r'\b(ë¡œê·¸ì¸|login)\b',
            r'\b(ì¹´ì¹´ì˜¤|kakao|naver|ë„¤ì´ë²„|google|êµ¬ê¸€)\b',
            r'\b(ì ‘ì†|ì—°ê²°|connection)\b',
            r'\b(ì¸ì¦|auth|authentication)\b',
            r'\b(API|api)\b',
            r'\b(ë°ì´í„°ë² ì´ìŠ¤|database|DB|db)\b',
            r'\b(ì„œë²„|server)\b',
            r'\b(ë„¤íŠ¸ì›Œí¬|network)\b',
            r'\b(ë©”ëª¨ë¦¬|memory)\b',
            r'\b(CPU|cpu)\b',
            r'\b(ë””ìŠ¤í¬|disk)\b',
            r'\b(ë³´ì•ˆ|security)\b',
            r'\b(ë¶ˆê°€|ì‹¤íŒ¨|error|fail)\b',
            r'\b(ëŠë¦¼|ì§€ì—°|slow|delay)\b',
            r'\b(ì¤‘ë‹¨|stop|halt)\b'
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            tech_keywords.extend(matches)
        
        return list(set(tech_keywords))

    def build_enhanced_search_query_with_flexible_matching(self, user_query, service_name=None, keywords=None):
        """ì„œë¹„ìŠ¤ëª… ìœ ì—° ë§¤ì¹­ì„ ìœ„í•œ í–¥ìƒëœ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±"""
        search_terms = []
        
        search_terms.append(user_query)
        
        if service_name:
            search_terms.append(service_name)
            
            for keyword in (keywords or []):
                search_terms.append(f"{service_name} {keyword}")
        
        if keywords:
            search_terms.extend(keywords)
        
        unique_terms = list(set(search_terms))
        search_query = " OR ".join(f'"{term}"' for term in unique_terms[:10])
        
        return search_query

    def perform_enhanced_search_with_flexible_service_matching(self, user_query, query_type="default"):
        """ì„œë¹„ìŠ¤ëª… ìœ ì—° ë§¤ì¹­ì´ ì ìš©ëœ í–¥ìƒëœ ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            service_name = self.extract_service_name_from_query_enhanced(user_query)
            keywords = self.extract_keywords_from_query_enhanced(user_query)
            
            if self.debug_mode:
                st.info(f"ğŸ” DEBUG: í–¥ìƒëœ ì„œë¹„ìŠ¤ëª…='{service_name}', í‚¤ì›Œë“œ={keywords}")
            
            thresholds = self.config.get_dynamic_thresholds(query_type, user_query)
            enhanced_query = self.build_enhanced_search_query_with_flexible_matching(user_query, service_name, keywords)
            
            if self.debug_mode:
                st.info(f"ğŸ” DEBUG: í–¥ìƒëœ ê²€ìƒ‰ ì¿¼ë¦¬='{enhanced_query}'")
            
            search_results = self.search_client.search(
                search_text=enhanced_query,
                top=thresholds.get('max_results', 30),
                include_total_count=True,
                search_mode='any',
                query_type='semantic',
                semantic_configuration_name='default',
                search_fields=['incident_id', 'service_name', 'symptom', 'effect', 'root_cause', 'incident_repair'],
                select=['incident_id', 'service_name', 'symptom', 'effect', 'root_cause', 'incident_repair', 'incident_plan', 'error_date', 'error_time', 'incident_grade', 'owner_depart', 'daynight', 'week']
            )
            
            filtered_results = []
            search_threshold = max(0.10, thresholds.get('search_threshold', 0.15) - 0.05)
            
            for result in search_results:
                score = getattr(result, '@search.score', 0)
                
                service_bonus = 0
                if service_name and hasattr(result, 'service_name'):
                    result_service = result.service_name or ''
                    
                    if service_name.upper() in result_service.upper():
                        service_bonus = 0.15
                    elif any(keyword.upper() in (getattr(result, field, '') or '').upper() 
                           for field in ['symptom', 'effect', 'root_cause'] 
                           for keyword in ([service_name] + keywords)):
                        service_bonus = 0.08
                    elif keywords and any(keyword.upper() in (getattr(result, field, '') or '').upper()
                                        for field in ['service_name', 'symptom', 'effect', 'root_cause', 'incident_repair']
                                        for keyword in keywords):
                        service_bonus = 0.05
                
                final_score = score + service_bonus
                
                if final_score >= search_threshold:
                    result_dict = {
                        'incident_id': getattr(result, 'incident_id', ''),
                        'service_name': getattr(result, 'service_name', ''),
                        'symptom': getattr(result, 'symptom', ''),
                        'effect': getattr(result, 'effect', ''),
                        'root_cause': getattr(result, 'root_cause', ''),
                        'incident_repair': getattr(result, 'incident_repair', ''),
                        'incident_plan': getattr(result, 'incident_plan', ''),
                        'error_date': getattr(result, 'error_date', ''),
                        'error_time': getattr(result, 'error_time', ''),
                        'incident_grade': getattr(result, 'incident_grade', ''),
                        'owner_depart': getattr(result, 'owner_depart', ''),
                        'daynight': getattr(result, 'daynight', ''),
                        'week': getattr(result, 'week', ''),
                        'search_score': final_score,
                        'original_score': score,
                        'service_bonus': service_bonus,
                        'service_match_type': 'exact' if service_bonus >= 0.15 else 'partial' if service_bonus >= 0.05 else 'keyword'
                    }
                    filtered_results.append(result_dict)
            
            filtered_results.sort(key=lambda x: x['search_score'], reverse=True)
            
            if self.debug_mode:
                st.info(f"ğŸ” DEBUG: ìœ ì—° ë§¤ì¹­ ê²€ìƒ‰ ê²°ê³¼ {len(filtered_results)}ê±´ (ì„ê³„ê°’: {search_threshold})")
                if filtered_results:
                    st.write("ìƒìœ„ 5ê°œ ê²°ê³¼:")
                    for i, result in enumerate(filtered_results[:5], 1):
                        st.write(f"{i}. {result['service_name']} - ì ìˆ˜: {result['search_score']:.3f} (ë³´ë„ˆìŠ¤: +{result['service_bonus']:.3f})")
            
            return filtered_results
            
        except Exception as e:
            if self.debug_mode:
                st.error(f"ğŸ” DEBUG: ìœ ì—° ë§¤ì¹­ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            else:
                st.error("ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return []

    @traceable(name="check_reprompting_question")
    def check_and_transform_query_with_reprompting(self, user_query):
        """ì‚¬ìš©ì ì§ˆë¬¸ì„ Reprompting DBì—ì„œ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ë³€í™˜"""
        if not user_query:
            return {
                'transformed': False,
                'original_query': user_query,
                'transformed_query': user_query,
                'match_type': 'none'
            }
            
        start_time = time.time()
        
        with trace(name="reprompting_check", inputs={"user_query": user_query}) as trace_context:
            try:
                with trace(name="exact_match_check", inputs={"query": user_query}) as exact_trace:
                    exact_result = self.reprompting_db_manager.check_reprompting_question(user_query)
                    self.safe_trace_update(exact_trace, outputs={"exact_match_found": exact_result['exists']})
                
                if exact_result['exists']:
                    result = {
                        'transformed': True,
                        'original_query': user_query,
                        'transformed_query': exact_result['custom_prompt'],
                        'question_type': exact_result['question_type'],
                        'wrong_answer_summary': exact_result['wrong_answer_summary'],
                        'match_type': 'exact'
                    }
                    
                    if not self.debug_mode:
                        st.success("âœ… ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
                    
                    self.safe_trace_update(trace_context,
                        outputs=result,
                        metadata={
                            "match_type": "exact",
                            "processing_time": time.time() - start_time,
                            "question_type": exact_result['question_type']
                        }
                    )
                    return result
                
                with trace(name="similar_match_search", inputs={"query": user_query, "threshold": 0.7}) as similar_trace:
                    similar_questions = self.reprompting_db_manager.find_similar_questions(
                        user_query, 
                        similarity_threshold=0.7,
                        limit=3
                    )
                    self.safe_trace_update(similar_trace, outputs={"similar_questions_count": len(similar_questions)})
                
                if similar_questions:
                    best_match = similar_questions[0]
                    
                    db_question = best_match['question']
                    custom_prompt_for_part = best_match['custom_prompt']
                    
                    try:
                        transformed_query = re.sub(re.escape(db_question), custom_prompt_for_part, user_query, flags=re.IGNORECASE)
                    except:
                        transformed_query = user_query.replace(db_question, custom_prompt_for_part)
                    
                    is_transformed = transformed_query != user_query
                    
                    result = {
                        'transformed': is_transformed,
                        'original_query': user_query,
                        'transformed_query': transformed_query,
                        'question_type': best_match['question_type'],
                        'wrong_answer_summary': best_match['wrong_answer_summary'],
                        'similarity': best_match['similarity'],
                        'similar_question': best_match['question'],
                        'match_type': 'similar'
                    }
                    
                    if is_transformed and not self.debug_mode:
                        st.info(f"ğŸ“‹ ìœ ì‚¬ ì§ˆë¬¸ íŒ¨í„´ì„ ê°ì§€í•˜ì—¬ ì§ˆë¬¸ì„ ìµœì í™”í–ˆìŠµë‹ˆë‹¤. (ìœ ì‚¬ë„: {best_match['similarity']:.1%})")

                    self.safe_trace_update(trace_context,
                        outputs=result,
                        metadata={
                            "match_type": "similar",
                            "similarity_score": best_match['similarity'],
                            "processing_time": time.time() - start_time,
                            "question_type": best_match['question_type']
                        }
                    )
                    return result
                
                result = {
                    'transformed': False,
                    'original_query': user_query,
                    'transformed_query': user_query,
                    'match_type': 'none'
                }
                
                self.safe_trace_update(trace_context,
                    outputs=result,
                    metadata={
                        "match_type": "none",
                        "processing_time": time.time() - start_time
                    }
                )
                return result
                
            except Exception as e:
                result = {
                    'transformed': False,
                    'original_query': user_query,
                    'transformed_query': user_query,
                    'match_type': 'error',
                    'error': str(e)
                }
                
                self.safe_trace_update(trace_context,
                    outputs=result,
                    metadata={
                        "match_type": "error",
                        "error": str(e),
                        "processing_time": time.time() - start_time
                    }
                )
                return result
    
    def extract_time_conditions(self, query):
        """ì¿¼ë¦¬ì—ì„œ ì‹œê°„ëŒ€/ìš”ì¼ ì¡°ê±´ ì¶”ì¶œ"""
        if not query:
            return {
                'daynight': None,
                'week': None,
                'is_time_query': False
            }
            
        time_conditions = {
            'daynight': None,
            'week': None,
            'is_time_query': False
        }
        
        daynight_patterns = [
            r'\b(ì•¼ê°„|ë°¤|ìƒˆë²½|ì‹¬ì•¼|ì•¼ì‹œê°„)\b',
            r'\b(ì£¼ê°„|ë‚®|ì˜¤ì „|ì˜¤í›„|ì£¼ì‹œê°„|ì¼ê³¼ì‹œê°„)\b'
        ]
        
        for pattern in daynight_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                time_conditions['is_time_query'] = True
                for match in matches:
                    if match in ['ì•¼ê°„', 'ë°¤', 'ìƒˆë²½', 'ì‹¬ì•¼', 'ì•¼ì‹œê°„']:
                        time_conditions['daynight'] = 'ì•¼ê°„'
                    elif match in ['ì£¼ê°„', 'ë‚®', 'ì˜¤ì „', 'ì˜¤í›„', 'ì£¼ì‹œê°„', 'ì¼ê³¼ì‹œê°„']:
                        time_conditions['daynight'] = 'ì£¼ê°„'
        
        week_patterns = [
            r'\b(ì›”ìš”ì¼|ì›”)\b',
            r'\b(í™”ìš”ì¼|í™”)\b', 
            r'\b(ìˆ˜ìš”ì¼|ìˆ˜)\b',
            r'\b(ëª©ìš”ì¼|ëª©)\b',
            r'\b(ê¸ˆìš”ì¼|ê¸ˆ)\b',
            r'\b(í† ìš”ì¼|í† )\b',
            r'\b(ì¼ìš”ì¼|ì¼)\b',
            r'\b(í‰ì¼|ì£¼ì¤‘)\b',
            r'\b(ì£¼ë§|í† ì¼)\b'
        ]
        
        for pattern in week_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                time_conditions['is_time_query'] = True
                for match in matches:
                    if match in ['ì›”ìš”ì¼', 'ì›”']:
                        time_conditions['week'] = 'ì›”'
                    elif match in ['í™”ìš”ì¼', 'í™”']:
                        time_conditions['week'] = 'í™”'
                    elif match in ['ìˆ˜ìš”ì¼', 'ìˆ˜']:
                        time_conditions['week'] = 'ìˆ˜'
                    elif match in ['ëª©ìš”ì¼', 'ëª©']:
                        time_conditions['week'] = 'ëª©'
                    elif match in ['ê¸ˆìš”ì¼', 'ê¸ˆ']:
                        time_conditions['week'] = 'ê¸ˆ'
                    elif match in ['í† ìš”ì¼', 'í† ']:
                        time_conditions['week'] = 'í† '
                    elif match in ['ì¼ìš”ì¼', 'ì¼']:
                        time_conditions['week'] = 'ì¼'
                    elif match in ['í‰ì¼', 'ì£¼ì¤‘']:
                        time_conditions['week'] = 'í‰ì¼'
                    elif match in ['ì£¼ë§', 'í† ì¼']:
                        time_conditions['week'] = 'ì£¼ë§'
        
        return time_conditions
    
    def extract_department_conditions(self, query):
        """ì¿¼ë¦¬ì—ì„œ ë¶€ì„œ ê´€ë ¨ ì¡°ê±´ ì¶”ì¶œ"""
        if not query:
            return {
                'owner_depart': None,
                'is_department_query': False
            }
            
        department_conditions = {
            'owner_depart': None,
            'is_department_query': False
        }
        
        department_keywords = [
            'ë‹´ë‹¹ë¶€ì„œ', 'ì¡°ì¹˜ë¶€ì„œ', 'ì²˜ë¦¬ë¶€ì„œ', 'ì±…ì„ë¶€ì„œ', 'ê´€ë¦¬ë¶€ì„œ',
            'ë¶€ì„œ', 'íŒ€', 'ì¡°ì§', 'ë‹´ë‹¹', 'ì²˜ë¦¬', 'ì¡°ì¹˜', 'ê´€ë¦¬'
        ]
        
        if any(keyword in query for keyword in department_keywords):
            department_conditions['is_department_query'] = True
        
        department_patterns = [
            r'\b(ê°œë°œ|ìš´ì˜|ê¸°ìˆ |ì‹œìŠ¤í…œ|ë„¤íŠ¸ì›Œí¬|ë³´ì•ˆ|DB|ë°ì´í„°ë² ì´ìŠ¤|ì¸í”„ë¼|í´ë¼ìš°ë“œ)(?:ë¶€ì„œ|íŒ€|íŒŒíŠ¸)?\b',
            r'\b(ê³ ê°|ì„œë¹„ìŠ¤|ìƒë‹´|ì§€ì›|í—¬í”„ë°ìŠ¤í¬)(?:ë¶€ì„œ|íŒ€|íŒŒíŠ¸)?\b',
            r'\b(IT|ì •ë³´ì‹œìŠ¤í…œ|ì •ë³´ê¸°ìˆ |ì „ì‚°)(?:ë¶€ì„œ|íŒ€|íŒŒíŠ¸)?\b',
            r'\b([ê°€-í£]+)(?:ë¶€ì„œ|íŒ€|íŒŒíŠ¸)\b'
        ]
        
        for pattern in department_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            if matches:
                department_conditions['owner_depart'] = matches[0]
                break
        
        return department_conditions
    
    @traceable(name="classify_query_type")
    def classify_query_type_with_llm(self, query):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ íƒ€ì…ì„ ìë™ìœ¼ë¡œ ë¶„ë¥˜ - statistics íƒ€ì… ì¶”ê°€"""
        if not query:
            return 'default'
            
        with trace(name="llm_query_classification", inputs={"query": query, "model": self.model_name}) as trace_context:
            try:
                classification_prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.

**ë¶„ë¥˜ ê¸°ì¤€:**
1. **repair**: ì„œë¹„ìŠ¤ëª…ê³¼ ì¥ì• í˜„ìƒì´ ëª¨ë‘ í¬í•¨ëœ ë³µêµ¬ë°©ë²• ë¬¸ì˜
   - ì˜ˆ: "ERP ì ‘ì†ë¶ˆê°€ ë³µêµ¬ë°©ë²•", "API_Link ì‘ë‹µì§€ì—° í•´ê²°ë°©ë²•"
   
2. **cause**: ì¥ì• ì›ì¸ ë¶„ì„ì´ë‚˜ ì›ì¸ íŒŒì•…ì„ ìš”ì²­í•˜ëŠ” ë¬¸ì˜
   - ì˜ˆ: "ERP ì ‘ì†ë¶ˆê°€ ì›ì¸ì´ ë­ì•¼?", "API ì‘ë‹µì§€ì—° ì¥ì• ì›ì¸", "ì™œ ì¥ì• ê°€ ë°œìƒí–ˆì–´?"
   
3. **similar**: ì„œë¹„ìŠ¤ëª… ì—†ì´ ì¥ì• í˜„ìƒë§Œìœ¼ë¡œ ìœ ì‚¬ì‚¬ë¡€ ë¬¸ì˜
   - ì˜ˆ: "ì ‘ì†ë¶ˆê°€ í˜„ìƒ ìœ ì‚¬ì‚¬ë¡€", "ì‘ë‹µì§€ì—° ë™ì¼í˜„ìƒ ë³µêµ¬ë°©ë²•"
   
4. **inquiry**: íŠ¹ì • ì¡°ê±´(ì‹œê°„ëŒ€, ìš”ì¼, ë…„ë„, ì›”, ì„œë¹„ìŠ¤, ë¶€ì„œ, ë“±ê¸‰ ë“±)ì— ëŒ€í•œ ì¥ì•  ë‚´ì—­ ì¡°íšŒ
   - ì˜ˆ: "2025ë…„ ì•¼ê°„ì— ë°œìƒí•œ ì¥ì•  ì•Œë ¤ì¤˜", "2020ë…„ í† ìš”ì¼ì— ë°œìƒí•œ ì¥ì•  ì•Œë ¤ì¤˜"
   - ì˜ˆ: "2025ë…„ ì£¼ë§ì— ë°œìƒí•œ ì¥ì• ê°€ ë­ì•¼?", "ì›”ìš”ì¼ì— ë°œìƒí•œ ERP ì¥ì•  ë‚´ì—­"
   - íŠ¹ì§•: ì‹œê°„/ì¡°ê±´ + "ë°œìƒí•œ" + "ì¥ì• " + ("ì•Œë ¤ì¤˜"/"ë‚´ì—­"/"ë­ì•¼"/"ëª©ë¡"/"ì–´ë–¤ê²Œ" ë“±)
   
5. **statistics**: í†µê³„ ì „ìš© ì§ˆë¬¸ (ê±´ìˆ˜, í†µê³„, í˜„í™©, ë¶„í¬, ì›”ë³„/ì—°ë„ë³„ ì§‘ê³„ ë“±)
   - ì˜ˆ: "2025ë…„ 1~6ì›” ì¥ì•  ê±´ìˆ˜", "ì—°ë„ë³„ ì¥ì•  í†µê³„", "ì„œë¹„ìŠ¤ë³„ ì¥ì•  í˜„í™©"
   - ì˜ˆ: "ì›”ë³„ ì¥ì•  ë¶„í¬", "ë¶€ì„œë³„ ì¥ì•  ê±´ìˆ˜", "ë“±ê¸‰ë³„ ì¥ì•  í†µê³„"
   - ì˜ˆ: "2025ë…„ ì¥ì• ì‹œê°„ í†µê³„", "ì›ì¸ìœ í˜•ë³„ ì›”ë³„ í˜„í™©"
   - íŠ¹ì§•: "ê±´ìˆ˜", "í†µê³„", "í˜„í™©", "ë¶„í¬", "ì›”ë³„", "ì—°ë„ë³„", "ëª‡ê±´", "ê°œìˆ˜" ë“±ì˜ í‚¤ì›Œë“œ í¬í•¨
   
6. **default**: ê·¸ ì™¸ì˜ ëª¨ë“  ê²½ìš° (ì¼ë°˜ì ì¸ í†µê³„, ê±´ìˆ˜, í˜„í™© ë¬¸ì˜, ë‹¨ìˆœí•œ ì¥ì• ë“±ê¸‰ ì¡°íšŒ ë“±)
   - ì˜ˆ: "ë…„ë„ë³„ ê±´ìˆ˜", "ì¥ì•  í†µê³„", "ì„œë¹„ìŠ¤ í˜„í™©", "ì¥ì• ë“±ê¸‰ ëª‡ê±´", "ERP ì¥ì• ê°€ ëª‡ê±´"

**ì¤‘ìš” êµ¬ë¶„ í¬ì¸íŠ¸:**
- **inquiry vs statistics**: 
  - inquiry: íŠ¹ì • ì¡°ê±´ì˜ "ì¥ì•  ë‚´ì—­/ëª©ë¡" ìš”ì²­ ("2025ë…„ ì•¼ê°„ì— ë°œìƒí•œ ì¥ì•  ì•Œë ¤ì¤˜")
  - statistics: "ê±´ìˆ˜/í†µê³„/í˜„í™©" ì§‘ê³„ ìš”ì²­ ("2025ë…„ ì•¼ê°„ ì¥ì• ê°€ ëª‡ê±´?")
- **statistics vs default**:
  - statistics: ëª…í™•í•œ í†µê³„ ì§‘ê³„ ì˜ë„ê°€ ìˆëŠ” ì§ˆë¬¸ (ì›”ë³„, ì—°ë„ë³„, ì„œë¹„ìŠ¤ë³„ ë“±ì˜ ê·¸ë£¹í™”ëœ í†µê³„)
  - default: ë‹¨ìˆœ ê±´ìˆ˜ ì§ˆë¬¸ì´ë‚˜ ì¼ë°˜ì ì¸ í˜„í™© ì§ˆë¬¸

**ì‚¬ìš©ì ì§ˆë¬¸:** {query}

**ì‘ë‹µ í˜•ì‹:** repair, cause, similar, inquiry, statistics, default ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

                response = self.azure_openai_client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ IT ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ì •í™•íˆ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."},
                        {"role": "user", "content": classification_prompt}
                    ],
                    temperature=0.1,
                    max_tokens=50
                )
                
                query_type = response.choices[0].message.content.strip().lower()
                
                # statistics íƒ€ì… ìœ íš¨ì„± ê²€ì¦
                if query_type not in ['repair', 'cause', 'similar', 'inquiry', 'statistics', 'default']:
                    query_type = 'default'
                
                self.safe_trace_update(trace_context,
                    outputs={"query_type": query_type},
                    metadata={
                        "model_used": self.model_name,
                        "temperature": 0.1,
                        "max_tokens": 50
                    }
                )
                
                return query_type
                
            except Exception as e:
                self.safe_trace_update(trace_context,
                    outputs={"query_type": "default"},
                    metadata={"error": str(e), "fallback_used": True}
                )
                return 'default'

    @traceable(name="validate_document_relevance")
    def validate_document_relevance_with_llm(self, query, documents):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„±ì„ ì¬ê²€ì¦"""
        if not query or not documents:
            return []
            
        with trace(name="llm_document_validation", inputs={"query": query, "document_count": len(documents)}) as trace_context:
            try:
                if not documents:
                    self.safe_trace_update(trace_context, outputs={"validated_documents": []})
                    return []
                
                validation_prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ë‹¤ìŒ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì‹¤ì œë¡œ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë§Œ ì„ ë³„í•´ì£¼ì„¸ìš”.
ê° ë¬¸ì„œì— ëŒ€í•´ 0-100ì  ì‚¬ì´ì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , 70ì  ì´ìƒì¸ ë¬¸ì„œë§Œ ì„ íƒí•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
1. ì„œë¹„ìŠ¤ëª… ì¼ì¹˜ë„ (ì‚¬ìš©ìê°€ íŠ¹ì • ì„œë¹„ìŠ¤ë¥¼ ì–¸ê¸‰í•œ ê²½ìš°)
2. ì¥ì• í˜„ìƒ/ì¦ìƒ ì¼ì¹˜ë„  
3. ì‚¬ìš©ìê°€ ìš”êµ¬í•œ ì •ë³´ ìœ í˜•ê³¼ì˜ ì¼ì¹˜ë„
4. ì „ì²´ì ì¸ ë§¥ë½ ì¼ì¹˜ë„

"""

                for i, doc in enumerate(documents):
                    if doc is None:
                        continue
                    doc_info = f"""
ë¬¸ì„œ {i+1}:
- ì„œë¹„ìŠ¤ëª…: {doc.get('service_name', '')}
- ì¥ì• í˜„ìƒ: {doc.get('symptom', '')}
- ì˜í–¥ë„: {doc.get('effect', '')}
- ì¥ì• ì›ì¸: {doc.get('root_cause', '')[:100]}...
- ë³µêµ¬ë°©ë²•: {doc.get('incident_repair', '')[:100]}...
"""
                    validation_prompt += doc_info

                validation_prompt += """

ì‘ë‹µ í˜•ì‹ (JSON):
{
    "validated_documents": [
        {
            "document_index": 1,
            "relevance_score": 85,
            "reason": "ì„œë¹„ìŠ¤ëª…ê³¼ ì¥ì• í˜„ìƒì´ ì •í™•íˆ ì¼ì¹˜í•¨"
        },
        {
            "document_index": 3,
            "relevance_score": 72,
            "reason": "ì¥ì• í˜„ìƒì€ ìœ ì‚¬í•˜ì§€ë§Œ ì„œë¹„ìŠ¤ëª…ì´ ë‹¤ë¦„"
        }
    ]
}

70ì  ì´ìƒì¸ ë¬¸ì„œë§Œ í¬í•¨í•˜ì„¸ìš”.
"""

                response = self.azure_openai_client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ ì •í™•í•˜ê²Œ í‰ê°€í•´ì£¼ì„¸ìš”."},
                        {"role": "user", "content": validation_prompt}
                    ],
                    temperature=0.1,
                    max_tokens=800
                )
                
                response_content = response.choices[0].message.content.strip()
                
                try:
                    import json
                    json_start = response_content.find('{')
                    json_end = response_content.rfind('}') + 1
                    if json_start >= 0 and json_end > json_start:
                        json_content = response_content[json_start:json_end]
                        validation_result = json.loads(json_content)
                        
                        validated_docs = []
                        for validated_doc in validation_result.get('validated_documents', []):
                            doc_index = validated_doc.get('document_index', 1) - 1
                            if 0 <= doc_index < len(documents):
                                original_doc = documents[doc_index].copy()
                                original_doc['relevance_score'] = validated_doc.get('relevance_score', 0)
                                original_doc['validation_reason'] = validated_doc.get('reason', 'ê²€ì¦ë¨')
                                validated_docs.append(original_doc)
                        
                        validated_docs.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
                        
                        self.safe_trace_update(trace_context,
                            outputs={
                                "validated_count": len(validated_docs),
                                "original_count": len(documents),
                                "filtered_out": len(documents) - len(validated_docs)
                            },
                            metadata={
                                "validation_threshold": 70,
                                "model_used": self.model_name
                            }
                        )
                        
                        return validated_docs
                        
                except (json.JSONDecodeError, KeyError) as e:
                    self.safe_trace_update(trace_context,
                        outputs={"validated_count": min(5, len(documents))},
                        metadata={"parsing_error": str(e), "fallback_used": True}
                    )
                    return documents[:5]
                    
            except Exception as e:
                self.safe_trace_update(trace_context,
                    outputs={"validated_count": min(5, len(documents))},
                    metadata={"validation_error": str(e), "fallback_used": True}
                )
                return documents[:5]
            
            return documents

    def validate_service_specific_documents(self, documents, target_service_name):
        """ì§€ì •ëœ ì„œë¹„ìŠ¤ëª…ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë§Œ í•„í„°ë§"""
        if not target_service_name or not documents:
            return documents
        
        is_common, _ = self.search_manager.is_common_term_service(target_service_name)
        
        validated_docs = []
        filter_stats = {
            'total': len(documents),
            'exact_matches': 0,
            'partial_matches': 0,
            'excluded': 0
        }
        
        for doc in documents:
            if doc is None:
                continue
                
            doc_service_name = doc.get('service_name', '').strip()
            
            if is_common:
                if doc_service_name.lower() == target_service_name.lower():
                    filter_stats['exact_matches'] += 1
                    validated_docs.append(doc)
                else:
                    filter_stats['excluded'] += 1
            else:
                if doc_service_name.lower() == target_service_name.lower():
                    filter_stats['exact_matches'] += 1
                    validated_docs.append(doc)
                elif target_service_name.lower() in doc_service_name.lower() or doc_service_name.lower() in target_service_name.lower():
                    filter_stats['partial_matches'] += 1
                    validated_docs.append(doc)
                else:
                    filter_stats['excluded'] += 1
        
        return validated_docs

    def _generate_chart_title(self, query, chart_type):
        """ì°¨íŠ¸ ì œëª© ìƒì„±"""
        title_map = {
            'yearly': 'ì—°ë„ë³„ ì¥ì•  ë°œìƒ í˜„í™©',
            'monthly': 'ì›”ë³„ ì¥ì•  ë°œìƒ í˜„í™©',
            'time_period': 'ì‹œê°„ëŒ€ë³„ ì¥ì•  ë°œìƒ ë¶„í¬',
            'weekday': 'ìš”ì¼ë³„ ì¥ì•  ë°œìƒ ë¶„í¬',
            'department': 'ë¶€ì„œë³„ ì¥ì•  ì²˜ë¦¬ í˜„í™©',
            'service': 'ì„œë¹„ìŠ¤ë³„ ì¥ì•  ë°œìƒ í˜„í™©',
            'grade': 'ì¥ì• ë“±ê¸‰ë³„ ë°œìƒ ë¹„ìœ¨',
            'cause_type': 'ì¥ì• ì›ì¸ ìœ í˜•ë³„ ë¶„í¬',
            'general': 'ì¥ì•  ë°œìƒ í†µê³„'
        }
        
        base_title = title_map.get(chart_type, 'ì¥ì•  í†µê³„')
        
        import re
        year_match = re.search(r'\b(202[0-9]|201[0-9])\b', query)
        if year_match:
            year = year_match.group(1)
            base_title = f"{year}ë…„ {base_title}"
        
        if query:
            error_time_keywords = ['ì¥ì• ì‹œê°„', 'ì¥ì•  ì‹œê°„', 'error_time', 'ì‹œê°„ í†µê³„', 'ì‹œê°„ í•©ê³„', 'ì‹œê°„ í•©ì‚°']
            is_error_time_query = any(keyword in query.lower() for keyword in error_time_keywords)
            if is_error_time_query:
                base_title = base_title.replace('ë°œìƒ', 'ì‹œê°„')
        
        if 'ì•¼ê°„' in query:
            base_title += ' (ì•¼ê°„)'
        elif 'ì£¼ê°„' in query:
            base_title += ' (ì£¼ê°„)'
        
        if any(day in query for day in ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼']):
            base_title += ' (í‰ì¼)'
        elif 'ì£¼ë§' in query:
            base_title += ' (ì£¼ë§)'
            
        return base_title

    def detect_statistics_type(self, query):
        """ì§ˆë¬¸ì—ì„œ ìš”ì²­ëœ í†µê³„ ìœ í˜• ê°ì§€"""
        if not query:
            return {'type': 'all', 'keywords': []}
        
        query_lower = query.lower()
        
        stats_patterns = {
            'yearly': ['ë…„ë„ë³„', 'ì—°ë„ë³„', 'ë…„ë³„', 'ì—°ë³„', 'ë…„ë„', 'ì—°ë„', 'ë…„', 'ì—°'],
            'monthly': ['ì›”ë³„', 'ì›”'],
            'time_period': ['ì‹œê°„ëŒ€ë³„', 'ì£¼ê°„', 'ì•¼ê°„', 'ë‚®', 'ë°¤'],
            'weekday': ['ìš”ì¼ë³„', 'ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼', 'í‰ì¼', 'ì£¼ë§'],
            'department': ['ë¶€ì„œë³„', 'ë¶€ì„œ', 'íŒ€ë³„', 'íŒ€', 'ë‹´ë‹¹ë¶€ì„œ', 'ì²˜ë¦¬ë¶€ì„œ'],
            'service': ['ì„œë¹„ìŠ¤ë³„', 'ì„œë¹„ìŠ¤'],
            'grade': ['ë“±ê¸‰ë³„', 'ë“±ê¸‰', 'ì¥ì• ë“±ê¸‰', '1ë“±ê¸‰', '2ë“±ê¸‰', '3ë“±ê¸‰', '4ë“±ê¸‰']
        }
        
        detected_types = []
        for stats_type, keywords in stats_patterns.items():
            if any(keyword in query_lower for keyword in keywords):
                detected_types.append(stats_type)
        
        if not detected_types:
            return {'type': 'all', 'keywords': []}
        
        return {'type': 'specific', 'types': detected_types, 'keywords': []}

    def _extract_incident_id_sort_key(self, incident_id):
        """ì¥ì•  IDì—ì„œ ì •ë ¬ìš© í‚¤ ì¶”ì¶œ"""
        if not incident_id:
            return 999999999999999
        
        try:
            if incident_id.startswith('INM') and len(incident_id) > 3:
                number_part = incident_id[3:]
                result = int(number_part)
                print(f"DEBUG: ID {incident_id} -> key {result}")
                return result
            else:
                result = hash(incident_id) % 999999999999999
                print(f"DEBUG: ID {incident_id} (non-INM) -> key {result}")
                return result
                
        except (ValueError, TypeError) as e:
            result = hash(str(incident_id)) % 999999999999999
            print(f"DEBUG: ID {incident_id} (parse error) -> key {result}, error: {e}")
            return result

    def _test_incident_id_sorting(self, documents):
        """ì¥ì•  ID ì •ë ¬ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
        if not documents:
            return
        
        print("DEBUG: Testing incident ID sorting:")
        id_keys = []
        for doc in documents[:10]:
            incident_id = doc.get('incident_id', 'N/A')
            if incident_id != 'N/A':
                sort_key = self._extract_incident_id_sort_key(incident_id)
                id_keys.append((incident_id, sort_key))
        
        id_keys.sort(key=lambda x: x[1])
        
        print("DEBUG: Incident IDs sorted by key:")
        for i, (incident_id, sort_key) in enumerate(id_keys):
            print(f"  {i+1}. {incident_id} (key: {sort_key})")

    def _apply_default_sorting(self, documents):
        """ê¸°ë³¸ ì •ë ¬ ì ìš©"""
        if not documents:
            return documents
        
        try:
            self._test_incident_id_sorting(documents)
            
            def default_sort_key(doc):
                error_date = doc.get('error_date', '1900-01-01')
                if not error_date:
                    error_date = '1900-01-01'
                
                error_time = doc.get('error_time', 0)
                try:
                    error_time_val = int(error_time) if error_time is not None else 0
                except (ValueError, TypeError):
                    error_time_val = 0
                
                incident_id = doc.get('incident_id', 'INM99999999999')
                incident_sort_key = self._extract_incident_id_sort_key(incident_id)
                
                return (
                    error_date,
                    error_time_val,
                    -incident_sort_key
                )
            
            documents.sort(key=default_sort_key, reverse=True)
            
            print("DEBUG: Applied default sorting - Final result:")
            for i, doc in enumerate(documents[:7]):
                incident_id = doc.get('incident_id', 'N/A')
                sort_key = self._extract_incident_id_sort_key(incident_id) if incident_id != 'N/A' else 'N/A'
                print(f"  {i+1}. ID: {incident_id} (key: {sort_key}), Date: {doc.get('error_date')}, Time: {doc.get('error_time')}ë¶„")
            
            dates = [doc.get('error_date') for doc in documents[:7]]
            times = [doc.get('error_time') for doc in documents[:7]]
            print(f"DEBUG: Dates: {dates}")
            print(f"DEBUG: Times: {times}")
            
            if len(set(dates)) <= 2 and len(set(times)) <= 2:
                print("DEBUG: Dates and times are similar, re-sorting by incident ID only")
                documents.sort(key=lambda doc: self._extract_incident_id_sort_key(doc.get('incident_id', 'INM99999999999')))
                
                print("DEBUG: After incident ID only sorting:")
                for i, doc in enumerate(documents[:7]):
                    incident_id = doc.get('incident_id', 'N/A')
                    sort_key = self._extract_incident_id_sort_key(incident_id) if incident_id != 'N/A' else 'N/A'
                    print(f"  {i+1}. ID: {incident_id} (key: {sort_key})")
                
        except Exception as e:
            print(f"DEBUG: Default sorting error: {e}")
            try:
                print("DEBUG: Fallback to incident ID only sorting")
                documents.sort(key=lambda doc: self._extract_incident_id_sort_key(doc.get('incident_id', 'INM99999999999')))
                
                print("DEBUG: Fallback sorting result:")
                for i, doc in enumerate(documents[:7]):
                    incident_id = doc.get('incident_id', 'N/A')
                    sort_key = self._extract_incident_id_sort_key(incident_id) if incident_id != 'N/A' else 'N/A'
                    print(f"  {i+1}. ID: {incident_id} (key: {sort_key})")
                    
            except Exception as fallback_error:
                print(f"DEBUG: Fallback sorting also failed: {fallback_error}")
                pass

    def detect_sorting_requirements(self, query):
        """ì¿¼ë¦¬ì—ì„œ ì •ë ¬ ìš”êµ¬ì‚¬í•­ ê°ì§€"""
        sort_info = {
            'requires_custom_sort': False,
            'sort_field': None,
            'sort_direction': 'desc',
            'sort_type': None,
            'limit': None,
            'secondary_sort': 'default'
        }
        
        if not query:
            return sort_info
        
        query_lower = query.lower()
        
        error_time_patterns = [
            r'ì¥ì• ì‹œê°„.*(?:ê°€ì¥.*?ê¸´|ê¸´.*?ìˆœ|ì˜¤ë˜.*?ê±¸ë¦°|ìµœëŒ€|í°.*?ìˆœ|ë§ì€.*?ìˆœ)',
            r'(?:ê°€ì¥.*?ê¸´|ê¸´.*?ìˆœ|ì˜¤ë˜.*?ê±¸ë¦°|ìµœëŒ€|í°.*?ìˆœ|ë§ì€.*?ìˆœ).*ì¥ì• ì‹œê°„',
            r'ì¥ì• ì‹œê°„.*(?:ë‚´ë¦¼ì°¨ìˆœ|í°.*?ìˆœì„œ|ë†’ì€.*?ìˆœì„œ|ë§ì€.*?ìˆœì„œ)',
            r'error_time.*(?:desc|ë‚´ë¦¼ì°¨ìˆœ|í°.*?ìˆœì„œ)',
            r'(?:ìµœì¥|ìµœëŒ€|ê°€ì¥.*?ì˜¤ë˜).*ì¥ì• ',
            r'ì¥ì• .*(?:ìµœì¥|ìµœëŒ€|ê°€ì¥.*?ì˜¤ë˜)',
            r'top.*\d+.*ì¥ì• ì‹œê°„',
            r'ìƒìœ„.*\d+.*ì¥ì• ì‹œê°„',
            r'ì¥ì• ì‹œê°„.*(?:ìˆœì„œ|ì •ë ¬)',
            r'(?:ìˆœì„œ|ì •ë ¬).*ì¥ì• ì‹œê°„'
        ]
        
        date_patterns = [
            r'ë°œìƒì¼.*ìˆœì„œ',
            r'ë‚ ì§œ.*ìˆœì„œ',
            r'ì‹œê°„.*ìˆœì„œ.*ë°œìƒ',
            r'ìµœê·¼.*ìˆœì„œ',
            r'ê³¼ê±°.*ìˆœì„œ',
            r'error_date.*(?:desc|asc|ë‚´ë¦¼ì°¨ìˆœ|ì˜¤ë¦„ì°¨ìˆœ)',
            r'ìµœì‹ .*ì¥ì• ',
            r'ìµœê·¼.*ì¥ì• ',
            r'ì˜ˆì „.*ì¥ì• ',
            r'ê³¼ê±°.*ì¥ì• '
        ]
        
        top_patterns = [
            r'top\s*(\d+)',
            r'Top\s*(\d+)',            
            r'ìƒìœ„\s*(\d+)',
            r'ì²«\s*(\d+)',
            r'(\d+)ê°œ.*?ìˆœì„œ',
            r'(\d+)ê±´.*?ìˆœì„œ',
            r'(\d+)ê°œ.*?ì •ë ¬',
            r'(\d+)ê±´.*?ì •ë ¬'
        ]
        
        for pattern in error_time_patterns:
            if re.search(pattern, query_lower):
                sort_info['requires_custom_sort'] = True
                sort_info['sort_field'] = 'error_time'
                sort_info['sort_type'] = 'error_time'
                sort_info['sort_direction'] = 'desc'
                break
        
        if not sort_info['requires_custom_sort']:
            for pattern in date_patterns:
                match = re.search(pattern, query_lower)
                if match:
                    sort_info['requires_custom_sort'] = True
                    sort_info['sort_field'] = 'error_date'
                    sort_info['sort_type'] = 'error_date'
                    if any(keyword in query_lower for keyword in ['ìµœê·¼', 'ìµœì‹ ']):
                        sort_info['sort_direction'] = 'desc'
                    elif any(keyword in query_lower for keyword in ['ê³¼ê±°', 'ì˜ˆì „']):
                        sort_info['sort_direction'] = 'asc'
                    else:
                        sort_info['sort_direction'] = 'desc'
                    break
        
        for pattern in top_patterns:
            match = re.search(pattern, query_lower)
            if match:
                try:
                    limit = int(match.group(1))
                    sort_info['limit'] = min(limit, 50)
                    if not sort_info['requires_custom_sort']:
                        sort_info['requires_custom_sort'] = True
                        sort_info['sort_field'] = 'error_time'
                        sort_info['sort_type'] = 'error_time'
                        sort_info['sort_direction'] = 'desc'
                except ValueError:
                    pass
                break
        
        if 'ì‹œê°„ìˆœì„œ' in query_lower and not sort_info['requires_custom_sort']:
            sort_info['requires_custom_sort'] = True
            sort_info['sort_field'] = 'error_date'
            sort_info['sort_type'] = 'error_date'
            sort_info['sort_direction'] = 'desc'
        
        return sort_info

    def apply_custom_sorting(self, documents, sort_info):
        """ì •ë ¬ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ë¬¸ì„œ ì •ë ¬ ì ìš©"""
        if not documents:
            return documents
        
        try:
            if sort_info['requires_custom_sort']:
                
                if sort_info['sort_type'] == 'error_time':
                    def error_time_sort_key(doc):
                        error_time = doc.get('error_time', 0)
                        if error_time is None:
                            error_time_val = 0
                        else:
                            try:
                                error_time_val = int(error_time)
                            except (ValueError, TypeError):
                                error_time_val = 0
                        
                        error_date = doc.get('error_date', '1900-01-01')
                        incident_id = doc.get('incident_id', 'INM99999999999')
                        incident_sort_key = self._extract_incident_id_sort_key(incident_id)
                        
                        if sort_info['sort_direction'] == 'desc':
                            return (-error_time_val, error_date, incident_sort_key)
                        else:
                            return (error_time_val, error_date, incident_sort_key)
                    
                    documents.sort(key=error_time_sort_key)
                    
                elif sort_info['sort_type'] == 'error_date':
                    def error_date_sort_key(doc):
                        error_date = doc.get('error_date', '1900-01-01')
                        if not error_date:
                            error_date = '1900-01-01'
                        
                        error_time = doc.get('error_time', 0)
                        try:
                            error_time_val = int(error_time) if error_time is not None else 0
                        except (ValueError, TypeError):
                            error_time_val = 0
                        
                        incident_id = doc.get('incident_id', 'INM99999999999')
                        incident_sort_key = self._extract_incident_id_sort_key(incident_id)
                        
                        if sort_info['sort_direction'] == 'desc':
                            return (error_date, -error_time_val, incident_sort_key)
                        else:
                            return (error_date, error_time_val, incident_sort_key)
                    
                    documents.sort(key=error_date_sort_key, reverse=(sort_info['sort_direction'] == 'desc'))
                
                if sort_info['limit']:
                    documents = documents[:sort_info['limit']]
            
            else:
                self._apply_default_sorting(documents)
            
            return documents
            
        except Exception as e:
            print(f"DEBUG: Sorting error: {e}")
            self._apply_default_sorting(documents)
            return documents

    def _validate_sorting_result(self, documents, sort_info):
        """ì •ë ¬ ê²°ê³¼ ê²€ì¦ ë° ë¡œê¹…"""
        if not documents or len(documents) < 2:
            return True
        
        try:
            print(f"DEBUG: Sorting validation - Type: {sort_info.get('sort_type', 'default')}")
            
            if sort_info.get('sort_type') == 'error_time':
                for i in range(len(documents) - 1):
                    try:
                        current_time = int(documents[i].get('error_time', 0))
                        next_time = int(documents[i + 1].get('error_time', 0))
                        
                        if sort_info['sort_direction'] == 'desc' and current_time < next_time:
                            print(f"DEBUG: Error time sorting issue at index {i}: {current_time} < {next_time}")
                            return False
                        elif sort_info['sort_direction'] == 'asc' and current_time > next_time:
                            print(f"DEBUG: Error time sorting issue at index {i}: {current_time} > {next_time}")
                            return False
                    except (ValueError, TypeError):
                        continue
            
            elif sort_info.get('sort_type') == 'error_date':
                for i in range(len(documents) - 1):
                    current_date = documents[i].get('error_date', '1900-01-01')
                    next_date = documents[i + 1].get('error_date', '1900-01-01')
                    
                    if sort_info['sort_direction'] == 'desc' and current_date < next_date:
                        print(f"DEBUG: Error date sorting issue at index {i}: {current_date} < {next_date}")
                        return False
                    elif sort_info['sort_direction'] == 'asc' and current_date > next_date:
                        print(f"DEBUG: Error date sorting issue at index {i}: {current_date} > {next_date}")
                        return False
            
            else:
                print("DEBUG: Validating default sorting (Incident ID ascending):")
                for i in range(len(documents) - 1):
                    current_id = documents[i].get('incident_id', 'INM99999999999')
                    next_id = documents[i + 1].get('incident_id', 'INM99999999999')
                    current_id_key = self._extract_incident_id_sort_key(current_id)
                    next_id_key = self._extract_incident_id_sort_key(next_id)
                    
                    current_date = documents[i].get('error_date', '1900-01-01')
                    next_date = documents[i + 1].get('error_date', '1900-01-01')
                    current_time = documents[i].get('error_time', 0)
                    next_time = documents[i + 1].get('error_time', 0)
                    
                    if current_date == next_date and current_time == next_time:
                        if current_id_key > next_id_key:
                            print(f"DEBUG: Incident ID sorting issue at index {i}: {current_id}({current_id_key}) > {next_id}({next_id_key})")
                            return False
            
            print("DEBUG: Final sorting result:")
            for i, doc in enumerate(documents[:7]):
                incident_id = doc.get('incident_id', 'N/A')
                sort_key = self._extract_incident_id_sort_key(incident_id) if incident_id != 'N/A' else 'N/A'
                print(f"  {i+1}. ID: {incident_id} (key: {sort_key}), "
                      f"Date: {doc.get('error_date', 'N/A')}, "
                      f"Time: {doc.get('error_time', 'N/A')}ë¶„")
            
            return True
            
        except Exception as e:
            print(f"DEBUG: Sorting validation error: {e}")
            return False

    def _apply_improved_sorting_in_rag_response(self, processing_documents, sort_info):
        """RAG ì‘ë‹µ ìƒì„±ì—ì„œ ê°œì„ ëœ ì •ë ¬ ì ìš©"""
        if sort_info['requires_custom_sort']:
            with trace(name="custom_sorting", inputs=sort_info) as sort_trace:
                original_order = [doc.get('incident_id', '') for doc in processing_documents[:5]]
                
                processing_documents = self.apply_custom_sorting(processing_documents, sort_info)
                
                is_valid = self._validate_sorting_result(processing_documents, sort_info)
                if not is_valid:
                    print("DEBUG: Sorting validation failed, applying default sorting")
                    self._apply_default_sorting(processing_documents)
                
                new_order = [doc.get('incident_id', '') for doc in processing_documents[:5]]
                
                self.safe_trace_update(sort_trace, outputs={
                    "sort_type": sort_info['sort_type'],
                    "sort_direction": sort_info['sort_direction'],
                    "limit": sort_info['limit'],
                    "original_order": original_order,
                    "new_order": new_order,
                    "validation_passed": is_valid
                })
        else:
            print("DEBUG: Applying default sorting")
            self._apply_default_sorting(processing_documents)
            
            sample_order = [f"{doc.get('incident_id', 'N/A')}({doc.get('error_date', 'N/A')},{doc.get('error_time', 0)}ë¶„)" 
                          for doc in processing_documents[:3]]
            print(f"DEBUG: Default sorting applied - Sample: {sample_order}")
        
        return processing_documents

    def filter_negative_keywords(self, documents, query_type, query_text):
        """ë¶€ì ì ˆí•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œ ì œê±°"""
        
        if not documents or documents is None:
            return []
        
        negative_keywords = {
            'repair': {
                'strong': ['í†µê³„', 'ê±´ìˆ˜', 'í˜„í™©', 'ë¶„ì„', 'ëª‡ê±´', 'ê°œìˆ˜', 'ì´', 'ì „ì²´'],
                'weak': ['ì—°ë„ë³„', 'ì›”ë³„', 'ì‹œê°„ëŒ€ë³„', 'ìš”ì¼ë³„']
            },
            'cause': {
                'strong': ['ë³µêµ¬ë°©ë²•', 'í•´ê²°ë°©ë²•', 'ì¡°ì¹˜ë°©ë²•', 'ëŒ€ì‘ë°©ë²•'],
                'weak': ['í†µê³„', 'ê±´ìˆ˜', 'í˜„í™©', 'ë¶„ì„']
            },
            'similar': {
                'strong': ['ê±´ìˆ˜', 'í†µê³„', 'í˜„í™©', 'ë¶„ì„', 'ê°œìˆ˜', 'ì´'],
                'weak': ['ì—°ë„ë³„', 'ì›”ë³„', 'ì‹œê°„ëŒ€ë³„']
            },
            'default': {
                'strong': [],
                'weak': []
            }
        }
        
        keywords = negative_keywords.get(query_type, {'strong': [], 'weak': []})
        
        filtered_docs = []
        filter_stats = {'total': len(documents), 'strong_filtered': 0, 'weak_filtered': 0}
        
        for doc in documents:
            if doc is None:
                continue
                
            doc_text = f"{doc.get('symptom', '')} {doc.get('effect', '')} {doc.get('incident_repair', '')}".lower()
            
            strong_negative = any(keyword in doc_text for keyword in keywords['strong'])
            if strong_negative:
                filter_stats['strong_filtered'] += 1
                continue
            
            weak_negative_count = sum(1 for keyword in keywords['weak'] if keyword in doc_text)
            if weak_negative_count > 0:
                filter_stats['weak_filtered'] += 1
                original_score = doc.get('final_score', 0)
                penalty = weak_negative_count * 0.1
                doc['final_score'] = max(original_score - penalty, 0)
                doc['negative_penalty'] = penalty
            
            filtered_docs.append(doc)
        
        return filtered_docs

    def calculate_confidence_score(self, query, documents, query_type):
        """ë‹¤ì¤‘ ìš”ì†Œ ê¸°ë°˜ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        
        if not documents or documents is None:
            return 0.0
        
        confidence_factors = {
            'document_count': 0.2,
            'score_consistency': 0.25,
            'service_match_quality': 0.3,
            'query_clarity': 0.15,
            'temporal_alignment': 0.1
        }
        
        confidence_score = 0.0
        
        doc_count = len(documents)
        optimal_range = {'repair': (3, 8), 'cause': (3, 8), 'similar': (5, 15), 'statistics': (5, 50), 'default': (5, 20)}
        min_docs, max_docs = optimal_range.get(query_type, (3, 15))
        
        if min_docs <= doc_count <= max_docs:
            document_score = 1.0
        elif doc_count < min_docs:
            document_score = doc_count / min_docs
        else:
            document_score = max(0.3, max_docs / doc_count)
        
        confidence_score += document_score * confidence_factors['document_count']
        
        if documents:
            scores = [doc.get('final_score', 0) for doc in documents[:5]]
            if len(scores) > 1:
                score_std = self._calculate_std(scores)
                consistency_score = max(0, 1 - (score_std / max(scores) if max(scores) > 0 else 1))
            else:
                consistency_score = 1.0
        else:
            consistency_score = 0.0
        
        confidence_score += consistency_score * confidence_factors['score_consistency']
        
        if documents:
            exact_matches = sum(1 for doc in documents if doc.get('service_match_type') == 'exact')
            match_quality = exact_matches / len(documents)
        else:
            match_quality = 0.0
        
        confidence_score += match_quality * confidence_factors['service_match_quality']
        
        clarity_keywords = {
            'repair': ['ë³µêµ¬ë°©ë²•', 'í•´ê²°ë°©ë²•', 'ì¡°ì¹˜ë°©ë²•'],
            'cause': ['ì›ì¸', 'ì´ìœ ', 'ì™œ'],
            'similar': ['ìœ ì‚¬', 'ë¹„ìŠ·', 'ë™ì¼'],
            'statistics': ['ê±´ìˆ˜', 'í†µê³„', 'í˜„í™©', 'ë¶„í¬', 'ì›”ë³„', 'ì—°ë„ë³„'],
            'default': ['ê±´ìˆ˜', 'í†µê³„', 'í˜„í™©', 'ëª‡', 'ë“±ê¸‰']
        }
        
        if not query:
            clarity_score = 0.0
        else:
            query_lower = query.lower()
            relevant_keywords = clarity_keywords.get(query_type, [])
            clarity_score = min(1.0, sum(1 for keyword in relevant_keywords if keyword in query_lower) / max(len(relevant_keywords), 1))
        
        confidence_score += clarity_score * confidence_factors['query_clarity']
        
        time_keywords = ['ì•¼ê°„', 'ì£¼ê°„', 'ìš”ì¼', 'ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
        has_time_condition = any(keyword in query.lower() for keyword in time_keywords) if query else False
        
        if has_time_condition and documents:
            time_matching_docs = sum(1 for doc in documents 
                                   if doc.get('daynight') or doc.get('week'))
            temporal_score = time_matching_docs / len(documents)
        else:
            temporal_score = 1.0 if not has_time_condition else 0.5
        
        confidence_score += temporal_score * confidence_factors['temporal_alignment']
        
        return min(confidence_score, 1.0)
    
    def _calculate_std(self, values):
        """í‘œì¤€í¸ì°¨ ê³„ì‚°"""
        if not values:
            return 0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5

    def remove_text_charts_from_response(self, response_text):
        """ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì°¨íŠ¸ ì œê±°"""
        if not response_text:
            return response_text
            
        import re
        
        # í…ìŠ¤íŠ¸ ì°¨íŠ¸ íŒ¨í„´ë“¤
        text_chart_patterns = [
            # "ê° ì›”ë³„ ì¥ì• ê±´ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì°¨íŠ¸ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:" í˜•íƒœ
            r'ê°\s*ì›”ë³„.*?ì°¨íŠ¸ë¡œ\s*ë‚˜íƒ€ë‚¼\s*ìˆ˜\s*ìˆìŠµë‹ˆë‹¤:.*?(?=\n\n|\n[^ì›”"\d]|$)',
            # "ë‹¤ìŒê³¼ ê°™ì´ ê·¸ë˜í”„ë¡œ í‘œì‹œë©ë‹ˆë‹¤:" í˜•íƒœ  
            r'ë‹¤ìŒê³¼\s*ê°™ì´.*?ê·¸ë˜í”„ë¡œ\s*í‘œì‹œë©ë‹ˆë‹¤:.*?(?=\n\n|\n[^ì›”"\d]|$)',
            # ì›”ë³„ ë°ì´í„° + â–ˆ ë¬¸ì íŒ¨í„´
            r'\d+ì›”:\s*[â–ˆâ–“â–’â–‘â–¬\*\-\|]+.*?(?=\n\n|\n[^ì›”"\d]|$)',
            # "ì´ í†µê³„ëŠ” ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ" í˜•íƒœ
            r'ì´\s*í†µê³„ëŠ”\s*ì œê³µëœ\s*ë¬¸ì„œì˜.*?ê²°ê³¼ì…ë‹ˆë‹¤\.?',
            # ì—°ì†ëœ â–ˆ ë¬¸ìê°€ í¬í•¨ëœ ë¼ì¸ë“¤
            r'\n.*[â–ˆâ–“â–’â–‘â–¬]{2,}.*\n',
            # ì°¨íŠ¸ ì„¤ëª… ë¬¸êµ¬ë“¤
            r'ì°¨íŠ¸ë¡œ\s*ë‚˜íƒ€ë‚´[ë©´ë©°].*?:.*?(?=\n\n|\n[^ì›”"\d]|$)',
            r'ê·¸ë˜í”„ë¡œ\s*í‘œì‹œí•˜[ë©´ë©°].*?:.*?(?=\n\n|\n[^ì›”"\d]|$)',
            # ë°” ì°¨íŠ¸ë‚˜ í…ìŠ¤íŠ¸ ì‹œê°í™” ë¸”ë¡
            r'```[^`]*[â–ˆâ–“â–’â–‘â–¬\*\-\|]{2,}[^`]*```',
            # "ì°¨íŠ¸ë¡œ ë‚˜íƒ€ë‚´ë©´", "ê·¸ë˜í”„ë¡œ í‘œì‹œí•˜ë©´" ë“±ì˜ ë¬¸êµ¬ì™€ ì´ì–´ì§€ëŠ” í…ìŠ¤íŠ¸ ì°¨íŠ¸
            r'(ì°¨íŠ¸ë¡œ|ê·¸ë˜í”„ë¡œ).*?(ë‚˜íƒ€ë‚´|í‘œì‹œ|ë³´ì—¬).*?[:ï¼š]\s*\n.*?[â–ˆâ–“â–’â–‘â–¬\*\-\|]+.*?(?=\n\n|$)',
            # ì—°ë„ë³„, ì›”ë³„ ë°ì´í„°ì™€ í•¨ê»˜ ë‚˜ì˜¤ëŠ” í…ìŠ¤íŠ¸ ì°¨íŠ¸
            r'\d+[ë…„ì›”ì¼].*?[â–ˆâ–“â–’â–‘â–¬\*\-\|]{3,}.*?(?=\n\n|$)',
        ]
        
        cleaned_response = response_text
        
        for pattern in text_chart_patterns:
            cleaned_response = re.sub(pattern, '', cleaned_response, flags=re.MULTILINE | re.DOTALL)
        
        # ì¤‘ë³µëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
        cleaned_response = re.sub(r'\n{3,}', '\n\n', cleaned_response)
        cleaned_response = cleaned_response.strip()
        
        return cleaned_response

    @traceable(name="generate_rag_response")
    def generate_rag_response_with_adaptive_processing(self, query, documents, query_type="default", time_conditions=None, department_conditions=None, reprompting_info=None):
        """ê°œì„ ëœ RAG ì‘ë‹µ ìƒì„± - ì¤‘ë³µ ì œê±° ë¹„í™œì„±í™” + statistics íƒ€ì… ì§€ì›"""
        
        if documents is None:
            documents = []
        
        if not documents:
            return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì–´ì„œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ ì¡°ê±´ì„ ì‹œë„í•´ë³´ì„¸ìš”."
        
        with trace(
            name="adaptive_rag_processing", 
            inputs={
                "query": query,
                "document_count": len(documents) if documents else 0,
                "query_type": query_type,
                "has_time_conditions": bool(time_conditions and time_conditions.get('is_time_query')),
                "has_department_conditions": bool(department_conditions and department_conditions.get('is_department_query')),
                "reprompting_applied": bool(reprompting_info and reprompting_info.get('transformed'))
            }
        ) as trace_context:
            try:
                start_time = time.time()

                # í†µê³„ ì¼ê´€ì„± ê²€ì¦ì„ ìœ„í•œ ì¶”ê°€ ë¡œê¹…
                if any(keyword in query.lower() for keyword in ['ê±´ìˆ˜', 'í†µê³„', 'í˜„í™©', 'ì›”']):
                    print(f"DEBUG: Statistics consistency check for query: '{query}'")
                    print(f"DEBUG: Input documents count: {len(documents)}")
                    
                    # ì›”ë³„ ë¶„í¬ ë¯¸ë¦¬ í™•ì¸
                    month_check = {}
                    for doc in documents:
                        month = doc.get('month', '')
                        error_date = doc.get('error_date', '')
                        if month:
                            month_check[month] = month_check.get(month, 0) + 1
                        elif error_date and len(error_date) >= 7:
                            try:
                                month_from_date = error_date.split('-')[1]
                                if month_from_date.isdigit():
                                    month_num = int(month_from_date)
                                    month_check[str(month_num)] = month_check.get(str(month_num), 0) + 1
                            except:
                                pass
                    print(f"DEBUG: Pre-calculation month distribution: {month_check}")

                print(f"DEBUG: Starting improved statistics calculation (no duplicate removal)")
                unified_stats = self.calculate_unified_statistics(documents, query, query_type)
                print(f"DEBUG: Improved stats calculated: {unified_stats}")

                # í†µê³„ ì¼ê´€ì„± ìµœì¢… ê²€ì¦
                if unified_stats.get('monthly_stats'):
                    monthly_total = sum(unified_stats['monthly_stats'].values())
                    total_count = unified_stats.get('total_count', 0)
                    print(f"DEBUG: Final consistency check - Monthly total: {monthly_total}, Document total: {total_count}")
                    
                    if monthly_total != total_count and not unified_stats.get('is_error_time_query', False):
                        print(f"WARNING: Statistics inconsistency detected!")
                        print(f"  - Monthly sum: {monthly_total}")
                        print(f"  - Document count: {total_count}")
                        print(f"  - Monthly breakdown: {unified_stats['monthly_stats']}")


                if not unified_stats.get('validation', {}).get('is_valid', True):
                    validation_errors = unified_stats.get('validation', {}).get('errors', [])
                    if validation_errors and self.debug_mode:
                        st.warning(f"í†µê³„ ê³„ì‚° ê²€ì¦ ì˜¤ë¥˜: {validation_errors}")
                
                print(f"DEBUG: Starting chart detection for query: {query}")
                
                chart_suitable, chart_type, chart_data = self.chart_manager.detect_chart_suitable_query(query, documents)
                chart_fig = None
                chart_info = None
                
                print(f"DEBUG: Chart detection result: suitable={chart_suitable}, type={chart_type}")
                
                if chart_suitable:
                    if chart_type and chart_data:
                        print(f"DEBUG: Using extracted chart data: {chart_data}")
                    else:
                        print("DEBUG: Chart manager failed, using improved unified stats")
                        
                        if 'monthly' in unified_stats and unified_stats['monthly_stats']:
                            chart_data = unified_stats['monthly_stats']
                            chart_type = 'line'
                            print(f"DEBUG: Using monthly stats: {chart_data}")
                        elif 'yearly' in unified_stats and unified_stats['yearly_stats']:
                            chart_data = unified_stats['yearly_stats']  
                            chart_type = 'line'
                            print(f"DEBUG: Using yearly stats: {chart_data}")
                        elif 'service' in unified_stats and unified_stats['service_stats']:
                            chart_data = unified_stats['service_stats']
                            chart_type = 'horizontal_bar'
                            print(f"DEBUG: Using service stats: {chart_data}")
                        elif 'grade' in unified_stats and unified_stats['grade_stats']:
                            chart_data = unified_stats['grade_stats']
                            chart_type = 'pie'
                            print(f"DEBUG: Using grade stats: {chart_data}")
                        else:
                            chart_data = {'ì „ì²´ ì¥ì• ': unified_stats['total_count']}
                            chart_type = 'bar'
                            print(f"DEBUG: Using total count: {chart_data}")
                    
                    if chart_data:
                        chart_title = self._generate_chart_title(query, chart_type)
                        print(f"DEBUG: Generating chart with title: {chart_title}")
                        
                        try:
                            chart_fig = self.chart_manager.create_chart(chart_type, chart_data, chart_title)
                            
                            if chart_fig:
                                chart_info = {
                                    'chart': chart_fig,
                                    'chart_type': chart_type,
                                    'chart_data': chart_data,
                                    'chart_title': chart_title,
                                    'query': query,
                                    'is_error_time_query': unified_stats.get('is_error_time_query', False)
                                }
                                print("DEBUG: Chart created successfully")
                            else:
                                print("DEBUG: Chart creation returned None")
                                
                        except Exception as chart_error:
                            print(f"DEBUG: Chart creation error: {chart_error}")
                            chart_info = None
                
                sort_info = self.detect_sorting_requirements(query)
                documents = self.filter_negative_keywords(documents, query_type, query)
                
                if time_conditions and time_conditions.get('is_time_query'):
                    with trace(name="time_filtering", inputs=time_conditions) as time_trace:
                        original_count = len(documents)
                        documents = self.search_manager.filter_documents_by_time_conditions(documents, time_conditions)
                        self.safe_trace_update(time_trace, outputs={
                            "original_count": original_count,
                            "filtered_count": len(documents)
                        })
                        
                        if not documents:
                            time_desc = []
                            if time_conditions.get('daynight'):
                                time_desc.append(f"{time_conditions['daynight']}")
                            if time_conditions.get('week'):
                                time_desc.append(f"{time_conditions['week']}")
                            
                            result = f"{''.join(time_desc)} ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì¥ì•  ë‚´ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ ì¡°ê±´ì„ ì‹œë„í•´ë³´ì„¸ìš”."
                            self.safe_trace_update(trace_context,
                                outputs={"response": result, "no_results_reason": "time_filter"}
                            )
                            return result
                
                
                if department_conditions and department_conditions.get('is_department_query'):
                    with trace(name="department_filtering", inputs=department_conditions) as dept_trace:
                        original_count = len(documents)
                        documents = self.search_manager.filter_documents_by_department_conditions(documents, department_conditions)
                        self.safe_trace_update(dept_trace, outputs={
                            "original_count": original_count,
                            "filtered_count": len(documents)
                        })
                        
                        if not documents:
                            dept_desc = department_conditions.get('owner_depart', 'í•´ë‹¹ ë¶€ì„œ')
                            result = f"{dept_desc} ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì¥ì•  ë‚´ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ ì¡°ê±´ì„ ì‹œë„í•´ë³´ì„¸ìš”."
                            self.safe_trace_update(trace_context,
                                outputs={"response": result, "no_results_reason": "department_filter"}
                            )
                            return result
                
                confidence_score = self.calculate_confidence_score(query, documents, query_type)
                use_llm_validation = query_type in ['repair', 'cause']
                
                if use_llm_validation:
                    with trace(name="llm_validation_step") as validation_trace:
                        validated_documents = self.validate_document_relevance_with_llm(query, documents)
                        self.safe_trace_update(validation_trace, outputs={
                            "original_count": len(documents),
                            "validated_count": len(validated_documents)
                        })
                    
                    if not validated_documents:
                        result = "ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë‚®ì•„ ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë‚˜ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
                        self.safe_trace_update(trace_context,
                            outputs={"response": result, "no_results_reason": "low_relevance"}
                        )
                        return result
                    
                    processing_documents = validated_documents
                else:
                    processing_documents = documents

                processing_documents = self._apply_improved_sorting_in_rag_response(processing_documents, sort_info)
                
                # ì¤‘ë³µ ì œê±° ë¡œì§ ì œê±° - ëª¨ë“  ë¬¸ì„œ ìœ ì§€
                print(f"DEBUG: Keeping all documents without deduplication: {len(processing_documents)} documents")
                total_count = len(processing_documents)
                
                yearly_stats = unified_stats.get('yearly_stats', {})
                monthly_stats = unified_stats.get('monthly_stats', {})
                time_stats = unified_stats.get('time_stats', {'daynight': {}, 'week': {}})
                department_stats = unified_stats.get('department_stats', {})
                service_stats = unified_stats.get('service_stats', {})
                grade_stats = unified_stats.get('grade_stats', {})
                is_error_time_query = unified_stats.get('is_error_time_query', False)
                calculation_details = unified_stats.get('calculation_details', {})
                
                yearly_total = sum(yearly_stats.values())
                monthly_total = sum(monthly_stats.values())
                
                context_parts = []
                
                stats_info = f"""
=== ì •í™•í•œ ì§‘ê³„ ì •ë³´ ===
ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_count}ê±´ (ì œê³µëœ ëª¨ë“  ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œ í†µê³„ ê³„ì‚°)"""
                
                if yearly_stats:
                    stats_info += f"\nì—°ë„ë³„ ë¶„í¬: {dict(sorted(yearly_stats.items()))}"
                    stats_info += f"\nì—°ë„ë³„ í•©ê³„: {yearly_total}{'ë¶„' if is_error_time_query else 'ê±´'}"
                
                if monthly_stats:
                    stats_info += f"\nì›”ë³„ ë¶„í¬: {monthly_stats}"
                    stats_info += f"\nì›”ë³„ í•©ê³„: {monthly_total}{'ë¶„' if is_error_time_query else 'ê±´'}"
                    if is_error_time_query:
                        stats_info += f"\në°ì´í„° ìœ í˜•: ì¥ì• ì‹œê°„ í•©ì‚°(ë¶„ ë‹¨ìœ„)"
                        if calculation_details:
                            stats_info += f"\nì´ ì¥ì• ì‹œê°„: {calculation_details.get('total_error_time_minutes', 0)}ë¶„ ({calculation_details.get('total_error_time_hours', 0)}ì‹œê°„)"
                            stats_info += f"\ní‰ê·  ì¥ì• ì‹œê°„: {calculation_details.get('average_error_time', 0)}ë¶„"
                            stats_info += f"\nìµœëŒ€ ì¥ì• ì‹œê°„: {calculation_details.get('max_error_time', 0)}ë¶„"
                    else:
                        stats_info += f"\në°ì´í„° ìœ í˜•: ë°œìƒ ê±´ìˆ˜"
                
                if service_stats:
                    stats_info += f"\nì„œë¹„ìŠ¤ë³„ ë¶„í¬: {dict(sorted(service_stats.items(), key=lambda x: x[1], reverse=True))}"
                
                if grade_stats:
                    stats_info += f"\nì¥ì• ë“±ê¸‰ë³„ ë¶„í¬: {dict(sorted(grade_stats.items(), key=lambda x: int(x[0][0]) if x[0] and x[0][0].isdigit() else 999))}"
                
                if time_stats['daynight']:
                    stats_info += f"\nì‹œê°„ëŒ€ë³„ ë¶„í¬: {time_stats['daynight']}"
                
                if time_stats['week']:
                    stats_info += f"\nìš”ì¼ë³„ ë¶„í¬: {time_stats['week']}"
                
                if department_stats:
                    stats_info += f"\në¶€ì„œë³„ ë¶„í¬: {department_stats}"
                
                if sort_info['requires_custom_sort']:
                    sort_desc = ""
                    if sort_info['sort_type'] == 'error_time':
                        sort_desc = f"ì¥ì• ì‹œê°„ ê¸°ì¤€ {'ë‚´ë¦¼ì°¨ìˆœ(ê¸´ ìˆœì„œ)' if sort_info['sort_direction'] == 'desc' else 'ì˜¤ë¦„ì°¨ìˆœ(ì§§ì€ ìˆœì„œ)'} ì •ë ¬"
                    elif sort_info['sort_type'] == 'error_date':
                        sort_desc = f"ë°œìƒì¼ì ê¸°ì¤€ {'ë‚´ë¦¼ì°¨ìˆœ(ìµœê·¼ ìˆœ)' if sort_info['sort_direction'] == 'desc' else 'ì˜¤ë¦„ì°¨ìˆœ(ê³¼ê±° ìˆœ)'} ì •ë ¬"
                    
                    if sort_info['limit']:
                        sort_desc += f", ìƒìœ„ {sort_info['limit']}ê°œ ì œí•œ"
                    
                    stats_info += f"\nì •ë ¬ ì •ë³´: {sort_desc}"
                else:
                    stats_info += f"\nì •ë ¬ ì •ë³´: ê¸°ë³¸ ì •ë ¬(ë°œìƒì¼ì ìµœì‹ ìˆœ â†’ ì¥ì• ì‹œê°„ í°ìˆœ â†’ ì¥ì• ID ìˆœ)"
                
                if self.debug_mode and unified_stats.get('validation'):
                    validation = unified_stats['validation']
                    if validation.get('warnings'):
                        stats_info += f"\nê²€ì¦ ê²½ê³ : {len(validation['warnings'])}ê°œ"
                
                stats_info += "\n=========================="
                
                context_parts.append(stats_info)
                
                for i, doc in enumerate(processing_documents):
                    final_score = doc.get('final_score', 0) if doc.get('final_score') is not None else 0.0
                    quality_tier = doc.get('quality_tier', 'Standard')
                    filter_reason = doc.get('filter_reason', 'ê¸°ë³¸ ì„ ë³„')
                    service_match_type = doc.get('service_match_type', 'unknown')
                    relevance_score = doc.get('relevance_score', 0) if use_llm_validation else "N/A"
                    validation_reason = doc.get('validation_reason', 'ê²€ì¦ë¨') if use_llm_validation else "í¬ê´„ì  ì²˜ë¦¬"
                    negative_penalty = doc.get('negative_penalty', 0)
                    
                    validation_info = f" - ê´€ë ¨ì„±: {relevance_score}ì  ({validation_reason})" if use_llm_validation else " - í¬ê´„ì  ê²€ìƒ‰"
                    penalty_info = f" - ë„¤ê±°í‹°ë¸Œ ê°ì : {negative_penalty:.1f}" if negative_penalty > 0 else ""
                    
                    time_info = ""
                    if doc.get('daynight'):
                        time_info += f" - ì‹œê°„ëŒ€: {doc.get('daynight')}"
                    if doc.get('week'):
                        time_info += f" - ìš”ì¼: {doc.get('week')}"
                    
                    department_info = ""
                    if doc.get('owner_depart'):
                        department_info += f" - ë‹´ë‹¹ë¶€ì„œ: {doc.get('owner_depart')}"
                    
                    grade_info = ""
                    if doc.get('incident_grade'):
                        grade_info += f" - ì¥ì• ë“±ê¸‰: {doc.get('incident_grade')}"
                    
                    sort_info_text = ""
                    if sort_info['requires_custom_sort']:
                        if sort_info['sort_type'] == 'error_time':
                            error_time = doc.get('error_time', 0)
                            sort_info_text = f" - ì¥ì• ì‹œê°„: {error_time}ë¶„ (ì •ë ¬ê¸°ì¤€)"
                        elif sort_info['sort_type'] == 'error_date':
                            error_date = doc.get('error_date', '')
                            sort_info_text = f" - ë°œìƒì¼ì: {error_date} (ì •ë ¬ê¸°ì¤€)"
                    
                    incident_repair = doc.get('incident_repair', '').strip()
                    incident_plan = doc.get('incident_plan', '').strip()
                    
                    if incident_repair and incident_plan:
                        if incident_plan in incident_repair:
                            incident_repair = incident_repair.replace(incident_plan, '').strip()
                        
                    context_part = f"""ë¬¸ì„œ {i+1} [{quality_tier}ê¸‰ - {filter_reason} - {service_match_type} ë§¤ì¹­{validation_info}{penalty_info}{time_info}{department_info}{grade_info}{sort_info_text}]:
ì¥ì•  ID: {doc['incident_id']}
ì„œë¹„ìŠ¤ëª…: {doc['service_name']}
ì¥ì• ì‹œê°„: {doc['error_time']}
ì˜í–¥ë„: {doc['effect']}
í˜„ìƒ: {doc['symptom']}
ë³µêµ¬ê³µì§€: {doc['repair_notice']}
ë°œìƒì¼ì: {doc['error_date']}
ìš”ì¼: {doc['week']}
ì‹œê°„ëŒ€: {doc['daynight']}
ì¥ì• ì›ì¸: {doc['root_cause']}
ë³µêµ¬ë°©ë²•(incident_repair): {incident_repair}
ê°œì„ ê³„íš(incident_plan): {incident_plan}
ì›ì¸ìœ í˜•: {doc['cause_type']}
ì²˜ë¦¬ìœ í˜•: {doc['done_type']}
ì¥ì• ë“±ê¸‰: {doc['incident_grade']}
ë‹´ë‹¹ë¶€ì„œ: {doc['owner_depart']}
ì—°ë„: {doc['year']}
ì›”: {doc['month']}
í’ˆì§ˆì ìˆ˜: {final_score:.2f}
"""
                    if use_llm_validation:
                        context_part += f"ê´€ë ¨ì„±ì ìˆ˜: {relevance_score}ì  \n"
                    
                    context_parts.append(context_part)
                
                context = "\n\n".join(context_parts)
                
                system_prompt = SystemPrompts.get_prompt(query_type)
                final_query = reprompting_info.get('transformed_query', query) if reprompting_info and reprompting_info.get('transformed') else query

                sorting_instruction = ""
                if sort_info.get('requires_custom_sort'):
                    sort_type_kr = ""
                    if sort_info.get('sort_type') == 'error_time':
                        sort_type_kr = "ì¥ì• ì‹œê°„ì´ ê¸´ ìˆœì„œ"
                    elif sort_info.get('sort_type') == 'error_date':
                        sort_type_kr = "ìµœì‹  ë°œìƒì¼ì ìˆœì„œ"
                    
                    if sort_type_kr:
                        sorting_instruction = f"""
**ì¤‘ìš”! ì •ë ¬ ìˆœì„œ ì¤€ìˆ˜:**
- ì•„ë˜ ë¬¸ì„œë“¤ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ '{sort_type_kr}'ìœ¼ë¡œ ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- **ë°˜ë“œì‹œ ì´ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.** ìˆœì„œë¥¼ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”.
"""

                time_stats_instruction = ""
                if is_error_time_query:
                    time_stats_instruction = f"""
**ì¤‘ìš”! ì¥ì• ì‹œê°„ í†µê³„ ì²˜ë¦¬:**
- ì´ ì§ˆë¬¸ì€ ì¥ì• ì‹œê°„ í•©ì‚° í†µê³„ì— ê´€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.
- ì›”ë³„ í†µê³„: {monthly_stats} (ë‹¨ìœ„: ë¶„)
- ë‹µë³€ ì‹œ "ê±´"ì´ ì•„ë‹Œ "ë¶„" ë‹¨ìœ„ë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.
- ì°¨íŠ¸ê°€ í•¨ê»˜ ì œê³µë˜ëŠ” ê²½ìš° ì°¨íŠ¸ì˜ ë‹¨ìœ„ì™€ ì¼ì¹˜ì‹œì¼œì£¼ì„¸ìš”.
- ì´ ì¥ì• ì‹œê°„: {calculation_details.get('total_error_time_minutes', 0)}ë¶„ ({calculation_details.get('total_error_time_hours', 0)}ì‹œê°„)
- í‰ê·  ì¥ì• ì‹œê°„: {calculation_details.get('average_error_time', 0)}ë¶„
- í‰ê·  ì¥ì• ì‹œê°„: {calculation_details.get('average_error_time', 0)}ë¶„
"""

                chart_instruction = ""
                if chart_suitable and chart_data:
                    chart_instruction = """
**ì¤‘ìš”! ì°¨íŠ¸ ì‹œê°í™” ì²˜ë¦¬:**
- ì´ ì§ˆë¬¸ì—ëŠ” ìë™ìœ¼ë¡œ ì‹œê°í™” ì°¨íŠ¸ê°€ ì œê³µë©ë‹ˆë‹¤
- ë‹µë³€ì—ì„œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì°¨íŠ¸(â–ˆ, *, -, | ë“±ì˜ ë¬¸ìë¥¼ ì‚¬ìš©í•œ ì‹œê°í™”)ë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”
- "ì°¨íŠ¸ë¡œ ë‚˜íƒ€ë‚´ë©´", "ê·¸ë˜í”„ë¡œ í‘œì‹œí•˜ë©´" ë“±ì˜ ë¬¸êµ¬ ì‚¬ìš© ê¸ˆì§€
- ìˆ«ì ë°ì´í„°ì™€ ë¶„ì„ ê²°ê³¼ë§Œ ì œê³µí•˜ê³ , ë³„ë„ì˜ í…ìŠ¤íŠ¸ ì‹œê°í™”ëŠ” ìƒëµí•˜ì„¸ìš”
- ì°¨íŠ¸ëŠ” ì‹œìŠ¤í…œì—ì„œ ìë™ìœ¼ë¡œ ìƒì„±ë˜ì–´ ë³„ë„ë¡œ í‘œì‹œë©ë‹ˆë‹¤
"""

                user_prompt = f"""
ë‹¤ìŒ ì¥ì•  ì´ë ¥ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
{sorting_instruction}
{time_stats_instruction}
{chart_instruction}
**ì¤‘ìš”! ë³µêµ¬ë°©ë²• ê´€ë ¨ ë‹µë³€ ì‹œ í•„ìˆ˜ì‚¬í•­:**
- ë³µêµ¬ë°©ë²• ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ incident_repair í•„ë“œì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- incident_plan(ê°œì„ ê³„íš)ì€ ë³µêµ¬ë°©ë²•ì— í¬í•¨í•˜ì§€ ë§ê³  ë³„ë„ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì œê³µí•˜ì„¸ìš”
- ë³µêµ¬ë°©ë²•ê³¼ ê°œì„ ê³„íšì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”

**ì¤‘ìš”! ì •í™•í•œ ì§‘ê³„ ê²€ì¦ í•„ìˆ˜ì‚¬í•­:**
- ì‹¤ì œ ì œê³µëœ ë¬¸ì„œ ìˆ˜: {total_count}ê±´ (ì¤‘ë³µ ì œê±° ì™„ë£Œ)
- ì—°ë„ë³„ ê±´ìˆ˜: {dict(sorted(yearly_stats.items()))} (ì—°ë„ë³„ í•©ê³„: {yearly_total}{'ë¶„' if is_error_time_query else 'ê±´'})
- ì›”ë³„ ê±´ìˆ˜: {monthly_stats} (ì›”ë³„ í•©ê³„: {monthly_total}{'ë¶„' if is_error_time_query else 'ê±´'})
- ì„œë¹„ìŠ¤ë³„ ë¶„í¬: {dict(sorted(service_stats.items(), key=lambda x: x[1], reverse=True)) if service_stats else 'ì •ë³´ì—†ìŒ'}
- ì¥ì• ë“±ê¸‰ë³„ ë¶„í¬: {dict(sorted(grade_stats.items(), key=lambda x: int(x[0][0]) if x[0] and x[0][0].isdigit() else 999)) if grade_stats else 'ì •ë³´ì—†ìŒ'}
- **ë‹µë³€ ì‹œ ë°˜ë“œì‹œ ì‹¤ì œ ë¬¸ì„œ ìˆ˜({total_count}ê±´)ì™€ ì¼ì¹˜í•´ì•¼ í•¨**
- **í‘œì‹œí•˜ëŠ” ë‚´ì—­ ìˆ˜ì™€ ì´ ê±´ìˆ˜ê°€ ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•¨**
- **ë¶ˆì¼ì¹˜ ì‹œ ë°˜ë“œì‹œ ì¬ê³„ì‚°í•˜ì—¬ ì •í™•í•œ ìˆ˜ì¹˜ë¡œ ë‹µë³€í•  ê²ƒ**
{"- **í†µê³„ ë‹¨ìœ„: ì¥ì• ì‹œê°„ í•©ì‚°(ë¶„)**" if is_error_time_query else "- **í†µê³„ ë‹¨ìœ„: ë°œìƒ ê±´ìˆ˜**"}

{context}

ì§ˆë¬¸: {final_query}

ë‹µë³€:"""

                if query_type == 'inquiry':
                    max_tokens_initial = 2500
                    temperature = 0.0
                elif query_type == 'cause':
                    max_tokens_initial = 3000
                    temperature = 0.0
                else:
                    max_tokens_initial = 1500
                    temperature = 0.0

                with trace(name="final_response_generation", inputs={"model": self.model_name, "query": final_query, "query_type": query_type}) as final_trace:
                    response = self.azure_openai_client.chat.completions.create(
                        model=self.model_name,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        temperature=temperature,
                        max_tokens=max_tokens_initial
                    )
                    
                    final_answer = response.choices[0].message.content
                    
                    self.safe_trace_update(final_trace, outputs={
                        "response_length": len(final_answer),
                        "tokens_used": response.usage.total_tokens if hasattr(response, 'usage') else None,
                        "query_type": query_type,
                        "max_tokens_used": max_tokens_initial,
                        "custom_sort_applied": sort_info['requires_custom_sort'],
                        "chart_generated": chart_suitable and chart_fig is not None,
                        "is_error_time_query": is_error_time_query,
                        "improved_statistics_used": True
                    })
                
                processing_time = time.time() - start_time
                
                self.safe_trace_update(trace_context,
                    outputs={
                        "response": final_answer,
                        "document_count_processed": total_count,
                        "processing_time": processing_time,
                        "query_type": query_type,
                        "use_llm_validation": use_llm_validation,
                        "improvements_applied": True,
                        "max_tokens_initial": max_tokens_initial,
                        "custom_sort_applied": sort_info['requires_custom_sort'],
                        "chart_generated": chart_suitable and chart_fig is not None,
                        "is_error_time_query": is_error_time_query,
                        "statistics_validation_passed": unified_stats.get('validation', {}).get('is_valid', True)
                    },
                    metadata={
                        "yearly_stats": yearly_stats,
                        "monthly_stats": monthly_stats,
                        "service_stats": service_stats,
                        "time_stats": time_stats,
                        "department_stats": department_stats,
                        "grade_stats": grade_stats,
                        "reprompting_applied": bool(reprompting_info and reprompting_info.get('transformed')),
                        "improvements_applied": ["improved_statistics_calculator", "data_normalization", "validation_system", "negative_keyword_filtering", "confidence_scoring", "enhanced_prompting", "inquiry_optimization", "custom_sorting", "repair_plan_separation", "chart_generation", "error_time_support"],
                        "sort_info": sort_info,
                        "chart_info": chart_info,
                        "unified_stats": unified_stats,
                        "calculation_details": calculation_details
                    }
                )
                
                if chart_info:
                    print("DEBUG: Returning response with chart info")
                    return final_answer, chart_info
                else:
                    print("DEBUG: Returning response without chart")
                    return final_answer
            
            except Exception as e:
                error_msg = f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}"
                st.error(error_msg)
                
                self.safe_trace_update(trace_context,
                    outputs={"error": error_msg},
                    metadata={"error_type": type(e).__name__}
                )
                
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _display_response_with_marker_conversion(self, response, chart_info=None):
        """í˜„ì¬ ë‹µë³€ì˜ ë§ˆì»¤ë¥¼ HTMLë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ - í…ìŠ¤íŠ¸ ì°¨íŠ¸ ì œê±°"""
        if not response:
            st.write("ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        if isinstance(response, tuple):
            response_text, chart_info = response
        else:
            response_text = response
        
        # ì°¨íŠ¸ê°€ ì œê³µë  ë•Œ í…ìŠ¤íŠ¸ ì°¨íŠ¸ ì œê±°
        if chart_info and chart_info.get('chart'):
            response_text = self.remove_text_charts_from_response(response_text)
            
        converted_content = response_text
        html_converted = False
        
        if '[REPAIR_BOX_START]' in converted_content and '[REPAIR_BOX_END]' in converted_content:
            converted_content, has_repair_html = self.ui_components.convert_repair_box_to_html(converted_content)
            if has_repair_html:
                html_converted = True
        
        if '[CAUSE_BOX_START]' in converted_content and '[CAUSE_BOX_END]' in converted_content:
            converted_content, has_cause_html = self.ui_components.convert_cause_box_to_html(converted_content)
            if has_cause_html:
                html_converted = True
        
        if html_converted or ('<div style=' in response_text and ('ì¥ì• ì›ì¸' in response_text or 'ë³µêµ¬ë°©ë²•' in response_text)):
            st.markdown(converted_content, unsafe_allow_html=True)
        else:
            st.write(converted_content)
        
        print(f"DEBUG: Chart display check - chart_info exists: {chart_info is not None}")
        if chart_info and chart_info.get('chart'):
            print("DEBUG: Displaying chart")
            st.markdown("---")
            
            try:
                self.chart_manager.display_chart_with_data(
                    chart_info['chart'],
                    chart_info['chart_data'],
                    chart_info['chart_type'],
                    chart_info.get('query', '')
                )
                
                if self.debug_mode:
                    st.success(f"âœ… {chart_info['chart_type']} ì°¨íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                print(f"DEBUG: Chart display error: {str(e)}")
                st.error(f"ì°¨íŠ¸ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                
                if self.debug_mode:
                    st.write("ì°¨íŠ¸ ë°ì´í„°:", chart_info.get('chart_data', {}))
                    st.write("ì°¨íŠ¸ íƒ€ì…:", chart_info.get('chart_type', 'unknown'))
        
        elif chart_info is None and self.debug_mode:
            print("DEBUG: No chart info available")
            st.info("ğŸ“Š ì´ ì§ˆë¬¸ì—ëŠ” ì°¨íŠ¸ ìƒì„±ì´ ì ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    @traceable(name="process_user_query")
    def process_query(self, query, query_type=None):
        """ê°œì„ ëœ ë©”ì¸ ì¿¼ë¦¬ ì²˜ë¦¬ - í†µê³„ ì¼ê´€ì„± ê²€ì¦ ê°•í™”"""
        
        if not query:
            st.error("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        with st.chat_message("assistant"):
            start_time = time.time()
            
            try:
                print(f"DEBUG: Processing query: {query}")
                
                # ì¿¼ë¦¬ íŒŒì‹± ê²°ê³¼ ìƒì„¸ ë¡œê¹… ì¶”ê°€
                reprompting_info = self.check_and_transform_query_with_reprompting(query)
                processing_query = reprompting_info.get('transformed_query', query)
                
                # ì‹œê°„ ì¡°ê±´ ì¶”ì¶œ ë° ë¡œê¹… ê°•í™”
                time_conditions = self.extract_time_conditions(processing_query)
                print(f"DEBUG: Time conditions extracted: {time_conditions}")
                
                # ì›” ê´€ë ¨ ì¿¼ë¦¬ì¸ì§€ íŠ¹ë³„íˆ í™•ì¸
                if 'ì›”' in processing_query:
                    print(f"DEBUG: Month-related query detected: '{processing_query}'")
                    # ê°œë³„ ì›” ë‚˜ì—´ íŒ¨í„´ í™•ì¸
                    individual_months = re.findall(r'\b(\d{1,2})ì›”\b', processing_query)
                    if len(individual_months) > 1:
                        print(f"DEBUG: Individual months detected: {individual_months}")
                        print(f"DEBUG: This should be treated as month range: {min(individual_months)}~{max(individual_months)}")
                
                department_conditions = self.extract_department_conditions(processing_query)
                
                if query_type is None:
                    with st.spinner("ğŸ” ì§ˆë¬¸ ë¶„ì„ ì¤‘..."):
                        query_type = self.classify_query_type_with_llm(processing_query)
                
                print(f"DEBUG: Query type classified as: {query_type}")
                
                target_service_name = self.search_manager.extract_service_name_from_query(processing_query)
                
                with st.spinner("ğŸ“„ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
                    documents = self.search_manager.semantic_search_with_adaptive_filtering(
                        processing_query, target_service_name, query_type
                    )
                    
                    print(f"DEBUG: Found {len(documents) if documents else 0} documents")
                    
                    # í†µê³„ì„± ì§ˆë¬¸ì˜ ê²½ìš° ì¶”ê°€ ê²€ì¦ ë¡œê¹…
                    if any(keyword in processing_query.lower() for keyword in ['ê±´ìˆ˜', 'í†µê³„', 'í˜„í™©', 'ë¶„í¬']):
                        print(f"DEBUG: Statistics query detected - ensuring consistency")
                        if documents:
                            # ì›”ë³„ ë¶„í¬ í™•ì¸
                            month_dist = {}
                            for doc in documents:
                                month = doc.get('month', '')
                                if month:
                                    month_dist[month] = month_dist.get(month, 0) + 1
                            print(f"DEBUG: Month distribution in results: {month_dist}")
                    
                    if documents is None:
                        documents = []
                    
                    if documents and len(documents) > 0:
                        with st.expander("ğŸ“„ ë§¤ì¹­ëœ ë¬¸ì„œ ìƒì„¸ ë³´ê¸°"):
                            self.ui_components.display_documents_with_quality_info(documents)
                        
                        with st.spinner("ğŸ¤– AI ë‹µë³€ ìƒì„± ì¤‘..."):
                            response = self.generate_rag_response_with_adaptive_processing(
                                query, documents, query_type, time_conditions, department_conditions, reprompting_info
                            )
                            
                            print(f"DEBUG: Generated response type: {type(response)}")
                            
                            if response is None:
                                response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            
                            if isinstance(response, tuple):
                                response_text, chart_info = response
                                print(f"DEBUG: Response includes chart info: {chart_info is not None}")
                                self._display_response_with_marker_conversion(response_text, chart_info)
                                st.session_state.messages.append({"role": "assistant", "content": response_text})
                            else:
                                print("DEBUG: Regular text response")
                                self._display_response_with_marker_conversion(response)
                                st.session_state.messages.append({"role": "assistant", "content": response})
                            
                    else:
                        with st.spinner("ğŸ“„ ì¶”ê°€ ê²€ìƒ‰ ì¤‘..."):
                            fallback_documents = self.search_manager.search_documents_fallback(processing_query, target_service_name)
                            
                            if fallback_documents and len(fallback_documents) > 0:
                                response = self.generate_rag_response_with_adaptive_processing(
                                    query, fallback_documents, query_type, time_conditions, department_conditions, reprompting_info
                                )
                                
                                if response is None:
                                    response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                    
                                if isinstance(response, tuple):
                                    response_text, chart_info = response
                                    self._display_response_with_marker_conversion(response_text, chart_info)
                                    st.session_state.messages.append({"role": "assistant", "content": response_text})
                                else:
                                    self._display_response_with_marker_conversion(response)
                                    st.session_state.messages.append({"role": "assistant", "content": response})
                            else:
                                self._show_no_results_message(target_service_name, query_type, time_conditions)
            
            except Exception as e:
                error_msg = f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
                
                if self.debug_mode:
                    import traceback
                    st.error("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
                    st.code(traceback.format_exc())

    
    def _show_no_results_message(self, target_service_name, query_type, time_conditions=None):
        """ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê°œì„  ë°©ì•ˆ ì œì‹œ"""
        time_condition_desc = ""
        if time_conditions and time_conditions.get('is_time_query'):
            time_desc = []
            if time_conditions.get('daynight'):
                time_desc.append(f"ì‹œê°„ëŒ€: {time_conditions['daynight']}")
            if time_conditions.get('week'):
                time_desc.append(f"ìš”ì¼: {time_conditions['week']}")
            time_condition_desc = f" ({', '.join(time_desc)} ì¡°ê±´)"
        
        error_msg = f"""
        '{target_service_name or 'í•´ë‹¹ ì¡°ê±´'}{time_condition_desc}'ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
        
        **ğŸ”§ ê°œì„  ë°©ì•ˆ:**
        - ì„œë¹„ìŠ¤ëª…ì˜ ì¼ë¶€ë§Œ ì…ë ¥í•´ë³´ì„¸ìš” (ì˜ˆ: 'API' ëŒ€ì‹  'API_Link')
        - ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”
        - ì „ì²´ ê²€ìƒ‰ì„ ì›í•˜ì‹œë©´ ì„œë¹„ìŠ¤ëª…ì„ ì œì™¸í•˜ê³  ê²€ìƒ‰í•´ì£¼ì„¸ìš”
        - ë” ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”
        
        **ì‹œê°„ ì¡°ê±´ ê´€ë ¨ ê°œì„  ë°©ì•ˆ:**
        - ì‹œê°„ëŒ€ ì¡°ê±´ì„ ì œê±°í•´ë³´ì„¸ìš” (ì£¼ê°„/ì•¼ê°„)
        - ìš”ì¼ ì¡°ê±´ì„ ì œê±°í•´ë³´ì„¸ìš”
        - ë” ë„“ì€ ì‹œê°„ ë²”ìœ„ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”
        
        **ì¥ì• ë“±ê¸‰ ê´€ë ¨ ê°œì„  ë°©ì•ˆ:**
        - ë“±ê¸‰ ì¡°ê±´ì„ ì œê±°í•˜ê³  ì „ì²´ ë“±ê¸‰ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”
        - 'ì¥ì• ë“±ê¸‰' í‚¤ì›Œë“œë§Œìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”
        - íŠ¹ì • ë“±ê¸‰ ëŒ€ì‹  'ë“±ê¸‰' í‚¤ì›Œë“œë§Œ ì‚¬ìš©í•´ë³´ì„¸ìš”
        
        **ğŸ’¡ {query_type.upper()} ì¿¼ë¦¬ ìµœì í™” íŒ:**
        """
        
        if query_type == 'repair':
            error_msg += """
        - ì„œë¹„ìŠ¤ëª…ê³¼ ì¥ì• í˜„ìƒì„ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”
        - êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ì¦ìƒì„ ëª…ì‹œí•˜ì„¸ìš”
        - 'incident_repair í•„ë“œ ê¸°ì¤€ìœ¼ë¡œë§Œ ë³µêµ¬ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤'
        """
        elif query_type == 'cause':
            error_msg += """
        - 'ì›ì¸', 'ì´ìœ ', 'cause' ë“±ì˜ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”
        - ì¥ì•  í˜„ìƒì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
        """
        elif query_type == 'similar':
            error_msg += """
        - 'ìœ ì‚¬', 'ë¹„ìŠ·í•œ', 'similar' í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì„¸ìš”
        - í•µì‹¬ ì¥ì•  í˜„ìƒë§Œ ê°„ê²°í•˜ê²Œ ê¸°ìˆ í•˜ì„¸ìš”
        """
        else:
            error_msg += """
        - í†µê³„ë‚˜ í˜„í™© ì¡°íšŒ ì‹œ ê¸°ê°„ì„ ëª…ì‹œí•˜ì„¸ìš”
        - êµ¬ì²´ì ì¸ ì„œë¹„ìŠ¤ëª…ì´ë‚˜ ì¡°ê±´ì„ í¬í•¨í•˜ì„¸ìš”
        - 'ê±´ìˆ˜', 'í†µê³„', 'í˜„í™©' ë“±ì˜ í‚¤ì›Œë“œë¥¼ í™œìš©í•˜ì„¸ìš”
        - ì‹œê°„ëŒ€ë³„(ì£¼ê°„/ì•¼ê°„) ë˜ëŠ” ìš”ì¼ë³„ ì§‘ê³„ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤
        
        **ë³µêµ¬ë°©ë²• ê´€ë ¨ ì°¸ê³ :**
        - ë³µêµ¬ë°©ë²• ì§ˆë¬¸ ì‹œ incident_repair í•„ë“œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤
        - ê°œì„ ê³„íš(incident_plan)ì€ ë³„ë„ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì œê³µë©ë‹ˆë‹¤
        """
        
        st.write(error_msg)
        st.session_state.messages.append({"role": "assistant", "content": error_msg})