import json
import re
import datetime
import os
from typing import Dict, List, Tuple, Any
import streamlit as st

class DataIntegrityMonitor:
    """RAG ë°ì´í„° ë¬´ê²°ì„± ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config=None):
        self.config = config
        self.violation_logs = []
        self.critical_fields = ['root_cause', 'incident_repair', 'incident_plan', 'symptom', 'effect']
        self.technical_patterns = self._initialize_technical_patterns()
        self.violation_count = {'high': 0, 'medium': 0, 'low': 0}
        
        if self.config and self.config.save_violation_logs:
            log_dir = os.path.dirname(self.config.violation_log_path)
            if log_dir and not os.path.exists(log_dir):
                os.makedirs(log_dir, exist_ok=True)
    
    def _initialize_technical_patterns(self):
        return {
            'system_components': [
                r'[ê°€-í£]*(?:ì„œë²„|ì‹œìŠ¤í…œ|ì„œë¹„ìŠ¤|ë°ì´í„°ë² ì´ìŠ¤|ë„¤íŠ¸ì›Œí¬|API|í—¤ë”|ë¡œì§|í”„ë¡œì„¸ìŠ¤)',
                r'[A-Za-z]+(?:\s+[A-Za-z]+)*(?:Server|System|Service|Database|Network|API|Header|Logic|Process)',
                r'(?:WAS|DB|DNS|SSL|VPN|OTP|SMS|HTTP|HTTPS|TCP|IP)'
            ],
            'error_terms': [
                r'(?:ëˆ„ë½|ì¤‘ë‹¨|ì‹¤íŒ¨|ì˜¤ë¥˜|ì—ëŸ¬|ì¥ì• |ë¬¸ì œ|ì´ìŠˆ|ë²„ê·¸)',
                r'(?:ë¶ˆê°€|ì•ˆë¨|ì•ˆë˜|ë˜ì§€ì•Š|í• ìˆ˜ì—†|ë¶ˆëŠ¥)',
                r'(?:ì§€ì—°|íƒ€ì„ì•„ì›ƒ|timeout|delay|slow)'
            ],
            'action_terms': [
                r'(?:ì ìš©|ì„¤ì •|êµ¬ì„±|ë°°í¬|ì„¤ì¹˜|ì—…ë°ì´íŠ¸|íŒ¨ì¹˜|ìˆ˜ì •|ë³€ê²½)',
                r'(?:ì¬ì‹œì‘|ë¦¬ë¶€íŒ…|ì¬ì‹œë„|ë¡¤ë°±|ë³µêµ¬|ë³µì›)',
                r'(?:ì ê²€|í™•ì¸|ê²€ì¦|ëª¨ë‹ˆí„°ë§|ì²´í¬)'
            ],
            'specific_values': [
                r'\d+(?:MB|GB|TB|KB|ë¶„|ì‹œê°„|ì´ˆ|%|ê°œ|ê±´)',
                r'\d{1,3}(?:\.\d{1,3}){3}',
                r'\d{4}-\d{2}-\d{2}',
                r'[A-Za-z0-9]+\.[A-Za-z0-9]+(?:\.[A-Za-z0-9]+)*'
            ]
        }
    
    def validate_llm_output(self, original_documents: List[Dict], llm_output: str) -> Dict[str, Any]:
        if not original_documents or not llm_output:
            return {'is_valid': True, 'violations': [], 'warning_count': 0}
        
        all_violations = []
        
        for idx, doc in enumerate(original_documents[:3]):
            doc_violations = self._validate_document_fields(doc, llm_output, idx)
            all_violations.extend(doc_violations)
        
        global_violations = self._validate_global_technical_terms(original_documents, llm_output)
        all_violations.extend(global_violations)
        
        high_violations = [v for v in all_violations if v['severity'] == 'HIGH']
        medium_violations = [v for v in all_violations if v['severity'] == 'MEDIUM']
        low_violations = [v for v in all_violations if v['severity'] == 'LOW']
        
        is_valid = len(high_violations) == 0 and len(medium_violations) <= 1
        
        validation_result = {
            'is_valid': is_valid,
            'violations': all_violations,
            'violation_count': len(all_violations),
            'severity_breakdown': {
                'high': len(high_violations),
                'medium': len(medium_violations),
                'low': len(low_violations)
            },
            'critical_field_violations': len([v for v in all_violations if v.get('field') in ['root_cause', 'incident_repair']]),
            'technical_term_retention_rate': self._calculate_technical_term_retention(original_documents, llm_output),
            'overall_score': self._calculate_integrity_score(all_violations, len(original_documents))
        }
        
        if all_violations and self.config and self.config.log_integrity_violations:
            self._log_violations(validation_result)
        
        return validation_result
    
    def _validate_document_fields(self, doc: Dict, llm_output: str, doc_index: int) -> List[Dict]:
        violations = []
        
        for field in self.critical_fields:
            original_value = doc.get(field, '').strip()
            if not original_value or len(original_value) < 10:
                continue
            
            violation = self._check_field_preservation(field, original_value, llm_output, doc_index, doc.get('incident_id', ''))
            if violation:
                violations.append(violation)
        
        return violations
    
    def _check_field_preservation(self, field_name: str, original_value: str, llm_output: str, doc_index: int, incident_id: str) -> Dict:
        tech_terms = self._extract_technical_terms(original_value)
        if not tech_terms:
            return None
        
        preserved_terms = []
        missing_terms = []
        
        for term in tech_terms:
            if self._is_term_preserved_in_output(term, llm_output):
                preserved_terms.append(term)
            else:
                missing_terms.append(term)
        
        preservation_rate = len(preserved_terms) / len(tech_terms) if tech_terms else 1.0
        
        if field_name in ['root_cause', 'incident_repair']:
            if preservation_rate < 0.7:
                severity = 'HIGH'
            elif preservation_rate < 0.8:
                severity = 'MEDIUM'
            elif preservation_rate < 0.9:
                severity = 'LOW'
            else:
                return None
        else:
            if preservation_rate < 0.5:
                severity = 'HIGH'
            elif preservation_rate < 0.7:
                severity = 'MEDIUM'
            elif preservation_rate < 0.8:
                severity = 'LOW'
            else:
                return None
        
        return {
            'type': 'FIELD_MODIFICATION',
            'field': field_name,
            'document_index': doc_index,
            'incident_id': incident_id,
            'severity': severity,
            'preservation_rate': preservation_rate,
            'original_length': len(original_value),
            'missing_terms': missing_terms,
            'preserved_terms': preserved_terms,
            'total_terms': len(tech_terms),
            'description': f"{field_name} í•„ë“œì˜ ê¸°ìˆ  ìš©ì–´ {len(missing_terms)}ê°œ ëˆ„ë½ (ë³´ì¡´ìœ¨: {preservation_rate:.1%})"
        }
    
    def _extract_technical_terms(self, text: str) -> List[str]:
        if not text:
            return []
        
        terms = set()
        
        for category, patterns in self.technical_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, str) and len(match.strip()) >= 2:
                        terms.add(match.strip())
        
        english_terms = re.findall(r'\b[A-Z][A-Za-z]{2,}\b', text)
        terms.update([term for term in english_terms if len(term) >= 3])
        
        korean_terms = re.findall(r'[ê°€-í£]{3,}', text)
        terms.update([term for term in korean_terms if len(term) >= 3 and not re.match(r'^[ê°€-í£]{1,2}(ì´|ê°€|ì„|ë¥¼|ì˜|ì—|ì„œ|ë¡œ|ìœ¼ë¡œ|ì—ì„œ|ë¶€í„°|ê¹Œì§€)$', term)])
        
        numeric_terms = re.findall(r'\d+[ê°€-í£A-Za-z%]+', text)
        terms.update(numeric_terms)
        
        return list(terms)[:15]
    
    def _is_term_preserved_in_output(self, term: str, llm_output: str) -> bool:
        if not term or not llm_output:
            return False
        
        if term in llm_output:
            return True
        
        if term.lower() in llm_output.lower():
            return True
        
        if len(term) >= 3:
            if re.search(re.escape(term), llm_output, re.IGNORECASE):
                return True
        
        synonyms = self._get_term_synonyms(term)
        for synonym in synonyms:
            if synonym in llm_output:
                return True
        
        return False
    
    def _get_term_synonyms(self, term: str) -> List[str]:
        synonym_map = {
            'ëˆ„ë½': ['ë¹ ì§', 'ì—†ìŒ', 'ë¯¸í¬í•¨'],
            'ì¤‘ë‹¨': ['ì •ì§€', 'ì¤‘ì§€', 'ì¢…ë£Œ'],
            'ì‹¤íŒ¨': ['ì˜¤ë¥˜', 'ì—ëŸ¬', 'ì¥ì• '],
            'ì„œë²„': ['ì‹œìŠ¤í…œ', 'í˜¸ìŠ¤íŠ¸'],
            'ë°ì´í„°ë² ì´ìŠ¤': ['DB', 'ë””ë¹„'],
            'ë„¤íŠ¸ì›Œí¬': ['ë§', 'í†µì‹ '],
            'ë¡œê·¸ì¸': ['ì ‘ì†', 'ì¸ì¦'],
            'ê°€ì…': ['ë“±ë¡', 'ì‹ ì²­'],
            'ê²°ì œ': ['êµ¬ë§¤', 'ì£¼ë¬¸'],
            'ë°œì†¡': ['ì „ì†¡', 'ì†¡ì‹ ']
        }
        
        return synonym_map.get(term.lower(), [])
    
    def _validate_global_technical_terms(self, documents: List[Dict], llm_output: str) -> List[Dict]:
        violations = []
        
        all_critical_terms = set()
        for doc in documents[:3]:
            for field in ['root_cause', 'incident_repair']:
                value = doc.get(field, '')
                if value:
                    terms = self._extract_technical_terms(value)
                    critical_terms = [t for t in terms if self._is_critical_technical_term(t)]
                    all_critical_terms.update(critical_terms)
        
        if not all_critical_terms:
            return violations
        
        preserved_critical_terms = []
        missing_critical_terms = []
        
        for term in all_critical_terms:
            if self._is_term_preserved_in_output(term, llm_output):
                preserved_critical_terms.append(term)
            else:
                missing_critical_terms.append(term)
        
        global_preservation_rate = len(preserved_critical_terms) / len(all_critical_terms)
        
        if global_preservation_rate < 0.6:
            violations.append({
                'type': 'GLOBAL_TECHNICAL_TERM_LOSS',
                'severity': 'HIGH',
                'preservation_rate': global_preservation_rate,
                'missing_terms': missing_critical_terms,
                'preserved_terms': preserved_critical_terms,
                'total_critical_terms': len(all_critical_terms),
                'description': f"ê¸€ë¡œë²Œ í•µì‹¬ ê¸°ìˆ  ìš©ì–´ {len(missing_critical_terms)}ê°œ ëˆ„ë½ (ë³´ì¡´ìœ¨: {global_preservation_rate:.1%})"
            })
        elif global_preservation_rate < 0.8:
            violations.append({
                'type': 'GLOBAL_TECHNICAL_TERM_LOSS',
                'severity': 'MEDIUM',
                'preservation_rate': global_preservation_rate,
                'missing_terms': missing_critical_terms,
                'preserved_terms': preserved_critical_terms,
                'total_critical_terms': len(all_critical_terms),
                'description': f"ê¸€ë¡œë²Œ í•µì‹¬ ê¸°ìˆ  ìš©ì–´ ì¼ë¶€ ëˆ„ë½ (ë³´ì¡´ìœ¨: {global_preservation_rate:.1%})"
            })
        
        return violations
    
    def _is_critical_technical_term(self, term: str) -> bool:
        critical_patterns = [
            r'.*(?:ì„œë²„|ì‹œìŠ¤í…œ|ì„œë¹„ìŠ¤|ë°ì´í„°ë² ì´ìŠ¤|ë„¤íŠ¸ì›Œí¬|API|í—¤ë”|ë¡œì§).*',
            r'.*(?:ëˆ„ë½|ì¤‘ë‹¨|ì‹¤íŒ¨|ì˜¤ë¥˜|ì—ëŸ¬|ì¥ì• ).*',
            r'[A-Z]{2,}',
            r'\d+(?:MB|GB|ë¶„|%)',
            r'.*(?:ì„¤ì •|êµ¬ì„±|ì ìš©|í”„ë¡œì„¸ìŠ¤).*'
        ]
        
        return any(re.match(pattern, term, re.IGNORECASE) for pattern in critical_patterns)
    
    def _calculate_technical_term_retention(self, documents: List[Dict], llm_output: str) -> float:
        all_terms = set()
        
        for doc in documents[:3]:
            for field in self.critical_fields:
                value = doc.get(field, '')
                if value:
                    terms = self._extract_technical_terms(value)
                    all_terms.update(terms)
        
        if not all_terms:
            return 1.0
        
        preserved_count = sum(1 for term in all_terms if self._is_term_preserved_in_output(term, llm_output))
        return preserved_count / len(all_terms)
    
    def _calculate_integrity_score(self, violations: List[Dict], document_count: int) -> float:
        if not violations:
            return 100.0
        
        penalty = 0
        for violation in violations:
            if violation['severity'] == 'HIGH':
                penalty += 25
            elif violation['severity'] == 'MEDIUM':
                penalty += 10
            elif violation['severity'] == 'LOW':
                penalty += 5
        
        adjusted_penalty = penalty / max(document_count, 1)
        
        return max(0.0, 100.0 - adjusted_penalty)
    
    def _log_violations(self, validation_result: Dict):
        log_entry = {
            'timestamp': datetime.datetime.now().isoformat(),
            'violation_type': 'RAG_DATA_INTEGRITY_CHECK',
            'result': validation_result,
            'session_info': {
                'user_agent': 'Streamlit-ChatBot',
                'ip_address': getattr(st.session_state, 'client_ip', 'unknown')
            }
        }
        
        self.violation_logs.append(log_entry)
        
        severity_breakdown = validation_result.get('severity_breakdown', {})
        self.violation_count['high'] += severity_breakdown.get('high', 0)
        self.violation_count['medium'] += severity_breakdown.get('medium', 0)
        self.violation_count['low'] += severity_breakdown.get('low', 0)
        
        if self.config and self.config.save_violation_logs and self.config.violation_log_path:
            try:
                existing_logs = []
                if os.path.exists(self.config.violation_log_path):
                    with open(self.config.violation_log_path, 'r', encoding='utf-8') as f:
                        existing_logs = json.load(f)
                
                existing_logs.append(log_entry)
                
                if len(existing_logs) > 1000:
                    existing_logs = existing_logs[-1000:]
                
                with open(self.config.violation_log_path, 'w', encoding='utf-8') as f:
                    json.dump(existing_logs, f, ensure_ascii=False, indent=2)
                    
            except Exception as e:
                print(f"WARNING: Failed to save violation log: {e}")
        
        if self.config and self.config.debug_data_integrity:
            print(f"ğŸš¨ DATA INTEGRITY VIOLATION: {validation_result['violation_count']} violations detected")
            for violation in validation_result['violations']:
                print(f"   - {violation['severity']}: {violation['description']}")
    
    def get_violation_summary(self) -> str:
        if not self.violation_logs:
            return "âœ… ë°ì´í„° ë¬´ê²°ì„± ìœ„ë°˜ ì—†ìŒ"
        
        total_violations = sum(self.violation_count.values())
        
        summary = f"""
ğŸ“Š ë°ì´í„° ë¬´ê²°ì„± ìœ„ë°˜ ìš”ì•½:
- ì´ ì„¸ì…˜ ìˆ˜: {len(self.violation_logs)}
- ì´ ìœ„ë°˜ ê±´ìˆ˜: {total_violations}
- ğŸ”´ ë†’ì€ ì‹¬ê°ë„: {self.violation_count['high']}ê±´
- ğŸŸ¡ ì¤‘ê°„ ì‹¬ê°ë„: {self.violation_count['medium']}ê±´  
- ğŸŸ¢ ë‚®ì€ ì‹¬ê°ë„: {self.violation_count['low']}ê±´

ìµœê·¼ ìœ„ë°˜ ì‹œê°: {self.violation_logs[-1]['timestamp'] if self.violation_logs else 'N/A'}
        """.strip()
        
        return summary
    
    def get_field_violation_statistics(self) -> Dict[str, int]:
        field_violations = {}
        
        for log_entry in self.violation_logs:
            violations = log_entry.get('result', {}).get('violations', [])
            for violation in violations:
                field = violation.get('field', 'unknown')
                if field not in field_violations:
                    field_violations[field] = 0
                field_violations[field] += 1
        
        return field_violations
    
    def generate_integrity_report(self) -> str:
        if not self.violation_logs:
            return "ğŸ“Š ë°ì´í„° ë¬´ê²°ì„± ë¦¬í¬íŠ¸: ìœ„ë°˜ ì‚¬í•­ ì—†ìŒ"
        
        field_stats = self.get_field_violation_statistics()
        
        report = f"""
ğŸ“Š ë°ì´í„° ë¬´ê²°ì„± ìƒì„¸ ë¦¬í¬íŠ¸
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ˆ ì „ì²´ í†µê³„:
- ê²€ì¦ ì„¸ì…˜ ìˆ˜: {len(self.violation_logs)}
- ì´ ìœ„ë°˜ ê±´ìˆ˜: {sum(self.violation_count.values())}

ğŸ¯ ì‹¬ê°ë„ë³„ ë¶„í¬:
- HIGH (ğŸ”´): {self.violation_count['high']}ê±´ ({self.violation_count['high']/max(sum(self.violation_count.values()),1)*100:.1f}%)
- MEDIUM (ğŸŸ¡): {self.violation_count['medium']}ê±´ ({self.violation_count['medium']/max(sum(self.violation_count.values()),1)*100:.1f}%)
- LOW (ğŸŸ¢): {self.violation_count['low']}ê±´ ({self.violation_count['low']/max(sum(self.violation_count.values()),1)*100:.1f}%)

ğŸ“‹ í•„ë“œë³„ ìœ„ë°˜ í†µê³„:
"""
        
        for field, count in sorted(field_stats.items(), key=lambda x: x[1], reverse=True):
            report += f"- {field}: {count}ê±´\n"
        
        report += f"\nğŸ•’ ìµœê·¼ ìœ„ë°˜ ì‚¬ë¡€ (ìµœëŒ€ 5ê°œ):\n"
        recent_logs = self.violation_logs[-5:]
        
        for i, log_entry in enumerate(recent_logs, 1):
            timestamp = log_entry['timestamp']
            violations = log_entry.get('result', {}).get('violations', [])
            high_violations = [v for v in violations if v['severity'] == 'HIGH']
            
            report += f"{i}. {timestamp}: {len(violations)}ê±´ ìœ„ë°˜"
            if high_violations:
                report += f" (HIGH: {len(high_violations)}ê±´)"
            report += "\n"
        
        return report.strip()
    
    def reset_statistics(self):
        self.violation_logs.clear()
        self.violation_count = {'high': 0, 'medium': 0, 'low': 0}
        print("ğŸ”„ ë°ì´í„° ë¬´ê²°ì„± í†µê³„ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")